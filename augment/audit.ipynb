{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76012ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mav204/Documents/minor-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ROOT = '/home/mav204/Documents/minor-project'\n",
    "os.chdir(ROOT)\n",
    "print(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07509130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd drive/MyDrive/minor-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82413b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from misc.dataset_modifier import save_json, get_json\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(sample):\n",
    "    return f\"\"\"\n",
    "You are a strict grader.\n",
    "\n",
    "Return JSON only:\n",
    "{{\n",
    "  \"predicted_label\": \"correct|partial|incorrect|uncertain\",\n",
    "  \"confidence\": 0.0-1.0\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Base your judgment primarily on the Reference Answer vs Provided Answer.\n",
    "- Use the normalized_score only as supporting evidence.\n",
    "- If content clearly contradicts the reference, ignore the score.\n",
    "- correct: matches all key points\n",
    "- partial: mostly correct, missing minor points\n",
    "- incorrect: wrong or contradicts reference\n",
    "- uncertain: ambiguous or insufficient info\n",
    "\n",
    "Question: {sample['question']}\n",
    "Reference Answer: {sample['reference_answer']}\n",
    "Provided Answer: {sample['provided_answer']}\n",
    "Original Label: {sample['verification_feedback']}\n",
    "Normalized Score: {sample['normalized_score']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_sample(sample):\n",
    "    prompt = build_prompt(sample)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,   # IMPORTANT\n",
    "            temperature=0.0,\n",
    "            top_p=1.0\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_json(\"./data/augmented/aug.json\")\n",
    "\n",
    "results = []\n",
    "for s in samples:\n",
    "    out = audit_sample(s)\n",
    "    results.append({\n",
    "        \"id\": s.get(\"id\"),\n",
    "        \"raw_output\": out\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14323eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "    return json.loads(match.group()) if match else None\n",
    "\n",
    "cleaned = []\n",
    "for r in results:\n",
    "    parsed = extract_json(r[\"raw_output\"])\n",
    "    if parsed:\n",
    "        cleaned.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6898f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(cleaned, './data/metadata/audit.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

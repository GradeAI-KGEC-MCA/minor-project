[
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "\u03bb = 9pkts/s\n\u00b5 = 10pkts/s\n\u03c1 = 9/10 = 0,9 = 90% Utilization\nE[n] = (0,9/0,1) \u2013 ((11*0,9^11) / (1-0,9^11)) = 3,969 = 4 Expected number of customers in the system.",
        "answer_feedback": "The response is incorrect because the question requirement is to find out the expected number of seconds where the system has less than 10 packets waiting in the queue while the response states the number of expected packets in the system, which is incorrect as well.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "TCP has a Error Control. So the users can be sure, that all packages have been transmitted in the right order.\nTCP has an included flow control, to assure, that the two clients don't get an overflow of packages.\nMulitplexing: In UDP you only have on port at the receiver, where to send the data. For TCP you have to, one at each side.\nConnections are established and torn down in TCP, with the three-way-handshake. For UDP there is no guarantee the connection is established or closed.",
        "answer_feedback": "The response states general differences between UDP and TCP while the question requirement is to identify TCP and UDP header differences.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "In TCP the ports are at both ends.\nIn TCP it has error control, flow control, congestion avoidance while in UDP has only checksum.",
        "answer_feedback": "The question requirement is to identify the difference between UDP and TCP headers while the response states general differences between UDP and TCP.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "Yes. Because Internet traffic can be also modeled as a sequence of arrivals of discrete entities, such as packets, cells, etc. Mathematically, this leads to the usage of two equivalent representations: counting processes and interarrival time processes.",
        "answer_feedback": "The correct answer is \"No\". In real internet, the arrival of the packet at a node is affected by previous arrivals.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds the routing entries to forward packets to their destination. The table is initially empty and will be filled with the information of routes during the backwards learning phase. The table entries will be scanned and updated when receiving frames, thus it will adapt to changes in topology, which is a benefit.",
        "answer_feedback": "The response does not state which entries are present that are used for forwarding the packets. The backward learning process does not explain which packet information is inspected and used for building the table. The correct benefit is that there is less traffic because of selective forwarding, not just topological change adaption.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Baudot time multiplex system.\n3 users have perfect clocks, so all channels can be processed in a fixed grid within a certain time. Each channel is assigned a fixed time window (time slot).The time windows can be synchronized and of the same length or asynchronous and depending on requirements. This is Time Division Multiplex.",
        "answer_feedback": "The question asks for the type of encoding rather the channel access type.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose is to reduce that overall network usage, and not produce unneeded traffic. You only send out the gotten packet if it came from a router you would route through to the sender, otherwise the packet gets droped as it can not be the optimal path.",
        "answer_feedback": "While both algorithms reduce traffic, the main purpose is to minimize duplicate packets during broadcasting. Also, it's unclear to which algorithm the given description is explaining.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "asynchronous: byte- and block-oriented\nsynchronous: character-, count- and bit-oriented",
        "answer_feedback": "The differences mentioned in the response are incorrect. The correct difference is: In asynchronous transmission, every character unit is surrounded by a start bit and a stop bit. In synchronous transmission, several characters are pooled into a frame.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Both sides must send data to use piggybacking to be able to attach ACKs to data frames otherwise the sender will assume a frame loss.",
        "answer_feedback": "The response is incorrect as there will not always be data to send, in such cases a dedicated timer timeout is used to signal the absence of sufficient data and trigger the sending of separate acknowledgment.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "The system reaches equilibrium when the arriving packet rate is the same as the serving packet rate.\nThe system will never reach equilibrium because the arriving packet rate is less than the serving packet rate, thus the packets in the queue will always be less than 10 packets.",
        "answer_feedback": "The response states that the system will never reach equilibrium, but it is stated in the question as an assumption already. Also, the given rates are not constant and can vary through time, so an average needs to be calculated for the given time. Instead of 60 seconds, the buffer is less than full for 56.9512 seconds on average.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "Yes the assumption that time between packet arrivals are independent holds true for real internet traffic. Packets transferred between one node to another suffer from various delays such as propagation delay, processing delay, queuing delay, transmission delay etc. Each of these are independent of each other. Hence the arrival interval of packets which is a function of these delays is also independent.",
        "answer_feedback": "The response is incorrect because the real traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is  a Transmission technique which is used to increase the Transmission rate of data Frames in the data link layer.\nAdvantage:the number of collision chances is reduced.\nDisadvantage:Frame Bursting does not address the primary goal of reducing the header Overhead.",
        "answer_feedback": "The response answer is incorrect as frame bursting is used to concatenate and transmit multiple frames in one single transmission to reduce the overhead of small frame transmission. Frame bursting offers higher efficiency in comparison to the carrier extension, but it increases end to end delay as the receiver may have to wait for frames.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "xxx.xxx.xxx.0\n\nxxx.xxx.xxx.255",
        "answer_feedback": "Missing: LoopbackNetwork and Broadcast are x.0.0.0 and x.255.255.255 with x between 0 and 127",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Transport Layer, in which it writes an error detection and correction also increases energy efficiency.  Network Layer, in which it adapts routing protocols and multicast routing.",
        "answer_feedback": "The response doesn\u2019t answer any challenges of mobile routing.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Since the system reached an equilibrium the probabilities do not change anymore ( dp_n(t)/dt = 0 ). So it can be assumed that the queue is emptied by one package each second on average (10 served - 9 arrived).\n\nNow assuming that the queue is full at obvservation start, it will be empty after 10 seconds, from which 9 seconds the queue has less packets then 10.",
        "answer_feedback": "The response is incorrect because the stated number of expected seconds is incorrect as the correct number of seconds is 56.95 seconds.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "I think the network topology is perfect. Because there so many paths between A and G, A-B-C-F-G, A-E-I-F-G etc. If some paths is fail, the data could through another path to send. But the problem is the right part is a little simple. It's not reliable and steady. It will be better, if there are more connection between G and the other nodes in right part, for example G-I, or G-j.",
        "answer_feedback": "The question is not about the network topology but about the routing strategy!",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "from 0.0.0.0 to 127.255.255.255",
        "answer_feedback": "Not all addresses in Class A are reserved",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Character oriented encoding is used.",
        "answer_feedback": "The response is not related to the theme of the encoding types.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "p = 9/10 = 0.9\nThe expected Number of Customers in the System are p/1-p = 0.9/0.1 = 9. I expect that the queue always has less than 10 packets waiting in it.",
        "answer_feedback": "The stated justification is incorrect as the given rates are not constant and can vary through time, so an average needs to be calculated for the given time. Therefore, the stated time (60 seconds) is also incorrect.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "127.255.255.255 (Broadcast)\n1.0.0.0 to 126.0.0.0 (Network ID)",
        "answer_feedback": "Please watch your notation: 1.0.0.0 - 126.0.0.0 does not mean, only addresses with .0.0.0, but every address in this range, for example 13.8.255.4, tooBroadcast is x.255.255.255 with x between 0 and 127Missing: Loopback",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Need to know the initial sequence number, aswell as the next sequence number and acknowledgement.",
        "answer_feedback": "The response is incorrect as the above points are not specific to piggybacking but hold for the sliding window protocol in general.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    }
]
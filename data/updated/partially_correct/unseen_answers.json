[
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I'll choose CSMA/CD, because it costs efficient, it can detect the collision,but it has short frame.",
        "answer_feedback": "Why is the collision detection an advantage? Why is the short frame an disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I recommend the token ring, because (1) only one system can send at a time (the one with the token), so the channel load is reduced and (2) the token ring procedure is collision-free and works decentralized. \nPotential weakness: the token can be lost if the system which is currently holding the token unexpectedly disconnects from the network.",
        "answer_feedback": "Actually, the token ring needs a central monitor (See slide 73, LAN slide set).",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Token ring\nAdvantages: still expandable, high usage, due to token ring: sending only possible when sender has ring; prevention of collisions, no random waiting time\nDisadvantage: more expansive than other MAC procedures",
        "answer_feedback": "Why is high usage an advantage?\u00a0Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.79
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Token Ring,\n+, Good throughput - even during increased utilization\n+, expandable\n\n-, Delays because of waiting for token",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend to use token ring for the following reasons:\n- As they expect the channel load to be high it is no advised to use CSMA/CD or a variant of it as that will cause a lot of collsions. Whereas token still performs reasonable well during increased utilization\n- The other requirement is that it should support 20 systems and should be expandable later. Token ring can support a maximum of 250 stations and can be extended with coax or optic fiber later on (for increased transmission rate). \n\nOne potential weakness is:\n- As their funding is tight CSMA could still be considered as it is a lot more cost efficient than token ring.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD:\nReasons:\n1. there are 20 systems sharing the channel, so it should be Random Access.\n2. stations know whether the channel is in use or not before trying to use it, so it should be With carrier sense\n\nPotential weakness:\n1. CSMA/CD has no maximum waiting time.",
        "answer_feedback": "Why exactly should it be Random Access, what advantages has it for sharing? The other CSMA procedures are also with carrier sense, why then choose CSMA/CD?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.43
    },
    {
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "We can use Bit encoding - it has a good utilization of the bandwidth, and it can work because the users have perfect clocks. \nanother technique we can use is Manchester encoding - is it not sensitive to  \u05deoise on the line and therefore can deal with more users using the line.",
        "answer_feedback": "Manchester encoding is not correct as we do not require self-clocking, also bandwidth utilization is less. (Note: as per the notification in the quiz slide, an additional\u00a0 incorrect answer will also be counted while providing grading )",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.5
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "First we need to calculate the utilization. In this scenario the utilization is quite high with 90%. However we want to know the time that the buffer has less than 10 entries. This means that not the case if the buffer is full. We can calculate probability for that using the above-mentioned utilization. The probability is about 5%. So it\u2019s likely that the system will most likely have the full minute less than 10 packets.",
        "answer_feedback": "The response is partially correct because calculating the probability when the buffer is full is correct. However, it lacks the step where the probability is multiplied with the time to get the expected number of seconds, that is 56.9512 seconds.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "\u03bb = 9     less than equal to packets arriving per second\n\u00b5 = 10   less than equal to packets served per second\nN = 10   less than equal to buffer size\n\n=> \u03c1 = 9/10\n\nThe probability that there less than 10 packets in the system is E(n less than 10).\n\nE(n less than 10) = 1- E(10) = 1 - Blocking probability\n= 1 - 0.457324\n= 0.954276\n\nSo in 60 seconds, there are less than 10 packets in the queue for E(n less than 10) * 60 = 57.2561 seconds.",
        "answer_feedback": "The steps stated are correct but the obtained blocking probability and the final time are not correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Ja, da es zu sogenanntem Oszillierendem Verhalten kommen kann wenn die aktuelle Auslastung einer Leitung als Metrik benutzt wird, wodurch permanent die Route gewechselt wird. Dies passiert dadurch das ein Packet \u00fcber Route X zu G geschickt wird, wodurch die Auslastung dieser Route steigt, was wiederum A dazu animiert eine andere Route zu w\u00e4hlen -> usw.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "When there are multiple transmiisions, with load may lead to an oscillation of the load, i.e., if A wants to send a message to H under the condition that CF is overload and EI is avaliable, it will choose EI to transmit message, when CF is avaliable, it will choose CF. Hence, routing tables may oscillate frequently.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "A k\u00f6nnte viele packets in einer kurzen Zeit senden und damit G (oder einen der Zwischenknoten) \u00fcberlasten. Flow control muss also beachtet werden.",
        "answer_feedback": "But this is not a problem specific for the routing strategy",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "It could be a problem that the packets are oscillating. So the packets will never arrive to G.",
        "answer_feedback": "Oscillating does not mean, that packets are never arriving in G!",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Using metrics like load or utilization can lead to 'oscillations' - which means that every time the load or utilization changes, the path taken will change, and the load and utilization will change while packets are being routed.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Yes this strategy could lead to problems as it could lead to oscillations. If there are two possible path (i.e link CF or link EI) the choice of which path to take could flip around as choosing one path increases the load on that path and in return making the other path more favorable increasing the load on this other path, so the decision on which path to take could swap around repeatedly.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, forward)\n\n\n\n\nHop 2:(B, E, forward) (B, C, drop) <= A->C is shorter\n\n(C, B, drop) <= A->B is shorter\n\n(C, E, drop) <= A->B->E is shorter\n(C, F, forward)\n\n(C, D, drop) <= A->D is shorter\n\n(D, C, drop) <= A->C is shorter\n\n(D, F, drop)  <= A->C->F is shorter \nHop 3: \n\n(E, C, drop) <= A->C shorter\n\n(E, F, drop) <= A -> C-> F is shorter\n\n(E, G, forward)\n\n(F, D, drop) => A->D is shorter\n\n(F, E, drop) => A -> B-> E is shorter\n\n(F, G, drop) => A -> B -> E -> G is shorter \nHop 4:\n\n(G, F, drop) => A->C->F is shorter\n\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward), (A, C, forward), (A, D, forward)\n\nHop 2:\n(B, E, forward)\n(C ,F, forward)\n\nHop 3:\n(E, G, forward)\n\nHop 4:\n(G, H, forward)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning\u00a0(A,D, drop),\u00a0(C, F, drop) and (G, H, drop)\u00a0.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2:\n(B, C, drop), C hat das Paket bereits empfangen.\n(B, E, forward)\n(C, B, drop), B hat das Paket bereits empfangen.\n(C, D, drop), D hat das Paket bereits empfangen.\n(C, E, drop), E bekommt Pakete von A normalerweise \u00fcber B.\n(C, F, forward)\n(D, C, drop), C hat das Paket bereits empfangen.\n(D, F, drop), F bekommt Pakete von A normalerweise \u00fcber C.\n\nHop 3:\n(E, C, drop), C hat das Paket bereits empfangen.\n(E, F, drop), F hat das Paket bereits empfangen.\n(E, G, forward)\n(F, D, drop), D hat das Paket bereits empfangen.\n(F, E, drop), E hat das Paket bereits empfangen.\n(F, G, drop), G bekommt Pakete von A normalerweise \u00fcber E.\n\nHop 4:\n(G, F, drop), F hat das Paket bereits empfangen.\n(G, H, drop) H hat keine Nachbar, an die das Paket weitergereicht werden kann.",
        "answer_feedback": "In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1:\u00a0\n(A, B, forward)\n(A, C, forward)\n(A, D, drop)\u00a0// D will receive the packet and won't forward it\u00a0\n\nHop 2:\u00a0\n(B, E, forward)\n(C, F, drop)\u00a0// F will receive the packet and won't forward it\u00a0\n\nHop 3:\u00a0\n(E, G, forward)\n\nHop 4:\u00a0\n(G, H, drop) // H will receive the packet and won't forward it",
        "answer_feedback": "The reason should explain why it is not forwarded, for example,\u00a0(A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A.\u00a0 For (G,H,drop), it has no other neighbor",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1: \n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, C, dropped) C would not send packets to A via B, costs 4, direct path to B costs 2(B, E, forward)(C, B, dropped) same reason as before(C, D, dropped) C would not send packets to A via D, costs 4, direct path to D costs 2(C, E, dropped) E would send packet via B(C, F, forward)(D, C, dropped) same reason as (C, D)(D, F, dropped) f would send packet via C, costs of 1 instead of 3\nHop 3:\n(E, F, dropped) e would send packet via B, costs of 1 instead of 2 (over F and C)(E, G, forward)(F, E, dropped) same reason as before(F, G, dropped) g would send packet via e, because costs of 1 instead of 2\nHop 4:\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,C,forward)(A,B,forward)(A,D,forward)\nHop 2:\n(B,E,forward)(C,F,forward)\nHop 3:\n(E,G,forward)\nHop 4:\n(G,H,drop) => dropped because the package arrived from the port with shortest path to S but there is no other port to forward the package to.",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.5 for reasoning (A,D, drop), (C, F, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1\n(A,B, forward)\n(A,C, forward)\n(A,D, forward)\n\nHop2\n(B,E,forward)\n(C,F,forward)\n\nHop3\n(E,G,forward)\n\nHop4\n(G,H,forward)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2\n(B, E, forward)\n(C, F, forward)\nHop 3\n(E, G, forward)\nHop 4\n(G, H, forward)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, E, forward)(B, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C,A)(C, B, drop) <= not the minimal route / part of minimal spanning tree. B would use (B, A) (C, E, drop) <= not the minimal route / part of minimal spanning tree. E would use (E, B, A)(C, F, forward)(C, D, drop) <= not the minimal route / part of minimal spanning tree. D would use (D, A)(D, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C, A)(D, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)\nHop 3:\n(E, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C, A)(E, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)(E, G, forward)(F, D, drop) <= not the minimal route / part of minimal spanning tree. D would use (D, A)(F, E, drop) <= not the minimal route / part of minimal spanning tree. E would use (E, B, A)(F, G, drop) <= not the minimal route / part of minimal spanning tree. G would use (G, E, B, A)\nHop 4:\n(G, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)(G, H, forward)\nHop 5:\nNo further transmissions because no more routes except to G available.",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start \nPhase 2: Congestion avoidance \n\nIn the beginning of Phase 1 the cwnd is increasing exponentially starting at cwnd=1 by doubling cwnd after every transmission until a threshhold ss_thresh is reached. From this point onwards, cwnd is increased linearly until congestion occurs. This initiates Phase 2, in which ss_thresh is set to the half of the value of cwnd at the moment when the congestion occured (ss_thresh_new = cwnd/2). Afterwards cwnd is reset to 1 and Phase 1 starts again with the new value of ss_thresh.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The congestion avoidance phase starts when the cwnd becomes equal to the threshold, not when the congestion starts.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow Start\n              In this phase, every time the segment is acknowledged, cnwd plus one until it reaches ss_thresh or packet loses, but when cnwd is not smaller than ss_thresh, cnwd will increase much slower.\n\nPhase 2: Congestion Avoidance \n            In this phase, the new ss_thresh is half of the cnwd, and cnwd will be reset to one, then slow start starts.",
        "answer_feedback": "The response correctly states the name of the two phases. The Slow start phase's explanation is partially correct because though it mentions that packet loss can occur before ss_thresh is reached, it does not provide details of the changes in the value of ss_thresh and the congestion window. Also, a linear increase of the congestion window happens in phase 2, not 1. The second phase does not state when the threshold and congestion window changes are done.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are called slow start and congestion avoidance. In the first phase grows the cwnd exponentially until it reaches ss_thresh. From this moment, the second phase begins and the cwnd grows only linear. If a timeout happens, cwnd is reset to one and ss_thresh is set to the half of the (former) cwnd. Then the two phases start again.",
        "answer_feedback": "The Slow start phase's explanation is partially correct as it does not mention what happens when a packet is lost before ss_thresh is reached. Here the slow start threshold also becomes half of the congestion window, and the congestion window becomes 1. The explanation of the congestion avoidance phase is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1 is called Slow Start. In this phase, the congestion window (cwnd) grows exponentially until the slow start threshold (ss_thresh) is reached, and then it grows linearly. Then, everytime congestion occurs, Phase 2 or congestion control starts. In this phase, ss_thresh is set to 50% of the current cwnd value. cwnd is then set to 1 and slow start starts again.",
        "answer_feedback": "The response is partially correct because the second phase is congestion avoidance and not congestion control. The slow start phase is missing details about how ss_thresh changes when a packet is lost. Also, in the congestion avoidance phase, it's unclear how the cwnd increases, till which condition it's done, and also when the ss_thresh is set to half of the current cwnd.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.38
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "In the first phase, the slow start, cwnd grows exponentially with base 2. \nWhen cwnd equals ss_thresh, the second phase, the congestion avoidance, starts, where cwnd now only grows linearly. \nWhen congestion occurs, ss_thresh is set to cwnd / 2 and cwnd is reset to 1 and the system is back in phase 1.",
        "answer_feedback": "The response is correct, except that it is not clear whether the congestion and the corresponding changes occur in phase 2 or both phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are a slow start and congestion avoidance. Lets assume the slow start threshold is X. After initialization, the \"slow start\" begins with checking if one segment arrives at the sender (receiving ACK) and increases the number of segments until the procedure throws an error. The new ss_thresh is half of the last segments that arrived successfully (cwnd/2). Then in phase two, the congestion window is reset to 1, and the process starts with a new ss_thresh (cwnd/2).",
        "answer_feedback": "The response states the two phases of congestion control correctly. However, it's unclear how cwnd is incremented. The response also lacks details about how and when ss_thresh changes in both phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "First, in the \"slow start\" phase, the sender sends one segment, then two, four, eight segments, etc. (always doubling the cwnd), until the ss_thresh is reached. Then, in \"congestion avoidance\" phase, the sender only increments the cwnd linearly (+1). If a congestion occurs, the ss_thresh is set to 50% of the current size of the cwnd, the cwnd is set to 1, and the sender starts again with slow start.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: After the initializtion, cwnd is increased by 1 each time when a segment is acknowledged. This continues until cwnd == ss_thresh or a packet gets lost. When cwnd >= ss_trhesh, TCP slows down the increase of cwnd. Especially, slow start increases the rate exponentially if each ACK generates 2 packets. 2. Congestion avoidance: Each time congestion occurs, ss_thresh is set to ss_tresh = cwnd / 2 and cwnd is reset to one s.t. cwnd = 1. After that, slow-start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. In the congestion avoidance phase, the extent of slow down of the congestion window rate is not precisely mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are Slow Start and Congestion Avoidance. At the slow start, the cwnd is initialised with 1 and then ex the slow start treshold (ss-thresh) is set on the advertised window size. While in the slow phase, the cwnd is counted up exponentially, but smaller than the ss_thresh. The congestion avoidance phase is reached, when the cwnd is as big as the ss_thresh. After that, the cwnd is increased linearily. Whenever there is a timeout, then the ss_thresh will be set on half the amount of the cwnd, and cwnd will be reset at 1 again and phase 1 starts again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The congestion control consists of two phases: slow start and congestion avoidance. After the initialization of cwnd and ss_thresh, the slow start tries to discover the proper sending rate as quickly as possible by incrementing the cwnd by 1 for each acknowledged package. This is continued until the ss_thresh is reached or a packet gets lost, then the congestion avoidance starts. Now each time a congestion occurs, the ss_thresh is set to cwnd/2, the cwnd is reset to 1 and the slow-start is entered again.",
        "answer_feedback": "In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.\u00a0 Congestion avoidance starts ONLY WHEN the threshold is reached , not when packet loss occurs.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.68
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are \"Slow start\" and \"Congestion Avoidance\". To make sure the network is not overloaded immediately, a TCP sender will start to send \"slowly\": First one segment, then as long as the segments get acknowleged double the rate each time, until the ss_thresh value is reached. This means after the first ACK is received, the sender will send two segments at once, then four, eight, etc until the ss_thresh value is reached or no ACK is received. If the ss_thresh value is reached (phase 2) the sender will increase the rate linearly by one each time. If a packet times out the ss_thresh value is set to 50% of the current rate and the cycle is repeated with phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached. This needs to be explicit for both the phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start (cwnd < ss_thresh)\nPhase 2: Congestion avoidance (cwnd >= ss_thresh)\nIn Phase 1, cwnd is initialized to 1, then it increases exponentially until it reaches to ss_thresh.\nIn Phase 2, cwnd increases one by one until it reaches the congestion, then new ss_thresh will be set to 2. Then cwnd is reset to 1 and phase starts.",
        "answer_feedback": "\"then new ss_thresh will be set to 2(half of current cwnd). Then cwnd is reset to 1 and phase starts(which phase starts??).\"\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The goal of the slow start phase is to quickly find a good sending rate. \nFor each ACK that is received, the cwnd is incremented, effectively doubling the cwnd within the round trip time.\nThe congestion avoidance phase starts as soon as either a packet loss occured or cwnd is greater or equals to ss_thresh and the cwnd is only incremented once each round trip time. \nWhen a timeout occures, ss_thresh is set to half of cwnd, cwnd is reset back to one and the slow start phase is entered again.",
        "answer_feedback": "Congestion avoidance phase\u00a0starts only when\u00a0 cwnd >= ss_thresh. Packet loss can occur in both phases, resulting in\u00a0ss_thresh = cwnd / 2 and\u00a0cwnd = 1",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1 - Slow Start:\nFor each received ACk, the cwnd is increased by one until the ss_thresh (Threshold) is reached.\nPhase 2 - Congestion Control:\nDuring congestion avoidance the cwnd increases linear by one per RTT. If a timeout occurs (congestion) the ss_tresh is set to half of the current window size (cwnd) and cwnd is set to 1. Then slow start will begin again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\nPhase 2: congestion avoidance\n\nPhase 1: start with a cwnd with one and double it every time a/the acknowledgment/s comes Until the ss-thresh(default: advertised window size) is reached or a congestion occurs.\n\nPhase 2: if a congestion didn't occur increase cwnd by one each time. until a congestions occurs. Then set ss_tresh to the half of the cwnd right now. and repeat phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases of congestion control are the \"Slow Start\" and the \"Congestion Avoidance\" phase. In the slow start phase, the cwnd is incremented by one whenever a segment is acknowledged until the the cwnd reaches the value of ss_thresh or until packet loss occurs. In the congestion avoidance phase, the ss_thresh value is set to cwnd / 2 whenever congestion occurs. Afterwards, cwnd is reset to one in that phase.",
        "answer_feedback": "In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1) Slow start\ncwnd verdoppelt sich nach jedem roundtrip (exponentiale Erh\u00f6hung)\nss-thresh bleibt w\u00e4hrend slow start gleich.\u00a0\n2) Congestion Avoidance\ncwnd wird nach jedem roundtrip um MSS/cwnd erh\u00f6ht (lineare Erh\u00f6hung)\nwenn congestion eintritt wird ss_tresh = cwnd/2 gesetzt und cwnd auf 1 gesetzt.",
        "answer_feedback": "\"ss-thresh remains the same during slow start.\u00a0\" not always as congestion can occur in this phase also.\u00a0After a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start(getting to equilibrium)\nPhase 2: Congestion Avoidance.\n\nIn phase 1, cwnd is < ss_thresh, and initialize cwnd=1,\u00a0 and then it increases expotentially until reach the ss_thresh, in phase 2, cwnd>=ss_thresh, it increases slowly one by one until reaches the congestion, then set the new ss_thresh=cwnd/2, and reset the cwnd=1 and continue start from phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1. Slow start -> discover proper sending rate\nWhen starting traffic on a new connecting or when experiencing an increase in traffic after congestion, the cwnd is initalized with one.\nWhenever a segment is acknowledged, the cwnd is incremented by one until eather ss_thresh is reached or packet loss is experienced.\nPhase 2: Congestion Avoidance\nAfter leaving the slow start phase (cwnd >(=) ss_thresh), cwnd may be incremented by 1 MSS every RTT to a maximum of SMSS.\nWhen a timeout occurs, meaning a congestion is experienced, ss_thresh is set to half the current cwnd. Cwnd is reset to one and slow-start is entered again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow start: in Slow Start phase, cwnd is increased by one from 1 each time a segment is acknowledged i.e. cwnd is increased exponentially, but untill cwnd reaches ss_thresh (cwnd = ss_thresh) or when there is a packet loss. The increament is lowed down when cwnd >= ss_thresh i.e. cwnd is increased successively. \nCongestion Avoidance. when congestion occurs, the size of ss_thresh is set to 50% of the current size of the congestion window i.e. ss_thresh = cwnd / 2 and cwnd is reset to 1. After that, Slow Start phase is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached. Also \"The increament is lowed down when cwnd >= ss_thresh i.e. cwnd is increased successively.\" happens in the congestion avoidance phase.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are called \"Slow start\" and \"Congestion avoidance\". When cwnd is smaller than ss_thresh the slow start phase is in action and cwnd is rapidly increased in a short amount of time by incrementing it by one each time a segment is acknowledged resulting in doubling the rate exponetially by doubling it every RTT. If cwnd is greater (or equal) than ss_thresh the congestion avoidance phase starts where as long as non-duplicate ACKs are received the cwnd may be increased by 1 MSS every RTT (AIMD). When a timeout occurs ss_thresh is set to cwnd/2 and cwnd is set to 1 and another slow start phase is entered resulting in alternating slow start and congestion avoidance phases.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "phase 1:slow start\u00a0\nphase2: congestion avoidance\nIn the slow start :Each time a segment is acknowledged,\u00a0 increment cwnd by one (cwnd++)\nContinue until\u00a0 reach ss_thresh or packet loss\nIn the phase 2:Each time congestion occurs: ss_thresh is set to 50% of the current size of the congestion window:\u00a0\n\u00a0ss_thresh = cwnd / 2\ncwnd = 1",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow Start\nThe congestion window is increased exponentially, for every acknowledge the congestion window is increased by 1. \nPhase 2: Congestion Avoidance\nThe congestion window is increased by 1 for every round trip. If a congestion happens it is reset to 1 and the ss_thresh is halfed",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "cwnd = 1 MMS, ss_thresh = window size\n\nPhase 1: Slow start\ncwnd < ss_thresh\n\nPhase 2: Congestion Avoidance\ncwnd >= ss_thresh",
        "answer_feedback": "Changes in both ss_thresh and cwnd in both the phases need to be explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start:\n\u00a0-1- initialize cwnd =1\n\n\u00a0-2-\u00a0Each time a segment is acknowledged, increment cwnd by one (cwnd++)\n\n\u00a0-3-\u00a0\u00a0Continue until\u00a0reach ss_thresh,\u00a0packet loss\n\nPhase 2: Congestion Avoidance:\u00a0\n\u00a0 Timeout = congestion\n\u00a0 Each time congestion occurs: ss_thresh is set to 50% of the current size of the\u00a0 \u00a0 \u00a0congestion window:\u00a0 ss_thresh = cwnd / 2 and\u00a0 cwnd is reset to one: cwnd = 1\u00a0 \u00a0 \u00a0and\u00a0 slow-start is entered",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow_start (cwnd <= ss_thresh)\u00a0 cwnd is doubled each RTT which is equal to an increase by one for every acknowledged segment.\u00a0 Phase continues until ss_thresh is reached or packet loss occured Congestion Avoidance (cwnd >= ss_thresh)Additive increase multiplicative decrease cwnd+1 per RTTIf timeout occurs ss_thresh = ss_thresh / 2, and cwnd = 1enter slow_start again",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The phases are slow start and congestion avoidance.\nIn the slow phase, the cwnd starts getting bigger in size, first slowly, then rapidly, until the ssthresh is reached. Once reached, the congestion control phase begins, where the cwnd slowly grows in size until a congestion occurs (timeout). In this case the slow start phase is entered again, the cwnd is reset and a new ssthresh is calculated (half of reached cwnd before timeout.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "2 phases: Slow start and congestion avoidance. In the slow start phase,\u00a0 the cwnd is doubled from 1 to 2, 4, 8, after each ACK is received,\u00a0 when cwnd >= ss_thresh,\u00a0addiitively to 9, 10, 11... until timeout(congestion) or packet loss. When congestion occurs, ss_thresh is set to 50% of the current size of cwnd, cwnd is set to 1. Repeat again with slow start.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\u00a0(cwnd < ss_thresh)\ncwnd wird mit 1 initialisiert. Senderate, sprich cwnd, wird solange langsam erh\u00f6ht, bis Aufstauung (congestion) entsteht.\nPhase 2: Congestion Avoidance\u00a0(cwnd >= ss_thresh)\nss-thresh wird auf cwnd/2 gesetzt, wobei cwnd der aktuellen Fenstergr\u00f6\u00dfe entspricht.Anschlie\u00dfend wird cwnd wieder auf 1 gesetzt und zur \"Slow start\"-Phase \u00fcbergegangen.",
        "answer_feedback": "In phase 1, cwnd increases exponentially.\u00a0In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The first phase (\"slow start\") doubles the cwnd after every RTT until ss_thresh is reached. After that, the second phase (\"congestion avoidance\") starts and furthermore only increases cwnd +1. Each time a congestion occurs, the ss_thresh is set to 50% of the current cwnd and cwnd is reset to 1, after which \"slow start\" phase is entered again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start(getting to equilibrium)\nPhase 2: Congestion Avoidence\u00a0\nIn both phases,when timeout occurs, ss_thresh is set to 50% of the current size of the congestion window, cwnd is reset to one, and slow-start is entered.And each time a segment is acknowleged , cwnd increase by one ,when cwnd=ss_thresh or packet loss , congestion avoidence is entered from slow-start .",
        "answer_feedback": "At what rate cwnd increases in the congestion avoidance phase is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "From less probable to most probable: B, C, A.\nEvent B is very unlikely, because it is requiring a strict sequence of heads and tails, which can be calculated by 0.6^3 * 0.4^3 = 1.4%.  Event C is more likely than C, because it just requires 3 heads, which is less strict than B and includes B, it can be calculated by BinCoef(6, 3) * 0.6^3 * 0.4^3 = 27.7%. A is more likely than C, because it is less strict and having at least three heads as well includes C, having exactly three heads. It can be calculated as the sum for all k=3 to 6: BinCoeff(6, k) 0.6^k * 0.4^{6-k} = 81.1%.",
        "answer_feedback": "The response correctly answers the events' order with appropriate justification, but the final result of the probability calculation of events C and A is incorrect. The correct values are 27.648% (rounding mistake) and 82.08%, respectively.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B\nEvent C\nEvent A\n\nEvent B: This is because the probability of seeing exactly in sequence HHHTTT is very low as there can be many different combinations such as HTHHTH, HTHTHT and others similar to that but for having the sequence HHHTTT there is only a single combination so the probability is very low.\n\nEvent C: Then the probability of seeing exactly three H's as the probability for showing up a head is 0.6 so there is more chances that a head shows up so the probability of head is more so exactly three H's probability is also low.\n\nEvent A: The probability of seeing head at least three is more because the probability of head is more 0.6 so the probability of a tail is 0.4 so we consider for this circumstance 0 T, 1T, 2T and 3T so summation of all these gives us the probability of at least three H's",
        "answer_feedback": "The response correctly states the sequence of the three given events. The justification for the events C and A is incorrect as the exact probability of H is not relevant (except when P(H) = 1 or 0) as C is a subset of A.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> C -> A\n\nB is the least propable, because there is only this one combination out of 64 possible combinations. The propability is 1,38% ( 0,6*0,6*0,6*0,3*0,3*0,3). C is more propable, because out of the 64 combinations, 20 combinations can satisfy this condition (formula would be 6!/3!*3!) and results in 27,63% propability. The most propable case is A, because it contains B and has the propabilities of having 4 Hs, 5Hs and 6Hs added onto it, which results in 82,08% propability.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification but the probability of event C is incorrect. The correct value is 27.648% (rounding mistake).",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Before sending data from Node A to Node B, A has to reserve a frame in the bus where A comes after B. Outer nodes are restricted to sending only in one direction, while nodes in the middle may make reservations in both directions and thus have a higher chance to get a reservation.",
        "answer_feedback": "The response is partially correct as the nodes located close to the generator have a higher chance of getting a reservation rather than the \"middle\" node. So unidirectional or bidirectional alone does not decide fairness.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "A UDP header has a length of 8 bytes whereas a TCP header has a length of 20 bytes. A UDP header has a field for the packet length, unlike a TCP header. A UDP header doesn\u2019t contain a sequence number, while a TCP header does. A UDP header neither contains an acknowledgement number but a TCP header has an extra field for that.",
        "answer_feedback": "The response correctly identifies and states the four differences between TCP and UDP headers except that the TCP header can be between 20 and 60 bytes long.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "It\u2019s not realistic, because in the Real-World the high traffic in the internet depends often on the day and the time of the day. For example, weekend or holiday and morning, afternoon or evening. So, it can be that in the morning there are many zeros in the time slots and the evening there much ones, because for example everyone is watching Netflix in the end of working day or is doing some other internet things. So there can be more than one on\u2019s in time interval delta t.",
        "answer_feedback": "One can use a function instead of a constant to model the arrival rate to reflect such large-scale behavioral patterns like having more traffic in the evening. The arrivals would not depend on previous arrivals then, only on the time of the day, which is known. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption can not hold for real internet traffic because the underlying assumption of independence is false. Over a higher timescale the behavior of the user is undergoing changes. For example a user checks his mails in the morning for which packets arrive but then he goes to work and in that time no packets arrive. Another example disproving the assumption of independence is the on/off bursty traffic while watching videos. For some time packets arrive continuously and then if the buffer is full no packets arrive until the buffer is empty again and needs to be refilled.",
        "answer_feedback": "The first example in the response is partially correct because the arrival process' parameters can be time-dependent. That can model such intra-day variations like people going to work. Knowing previous arrivals no longer has to capture this information for us, thus making the inter-arrival times independent in this regard. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, because sometimes many users want to access the server at the same time, while at other times, only few request the server. For example a livestream of a football match: everybody sends requests to the server at kickoff, but only few do after the game (to watch the highlights). That means that the arrivals are not independent. They can depend on other events.",
        "answer_feedback": "The arrival depends on other events in such cases but the dependency can also be observed in the normal scenario when no such event is happening. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, it is not true for real time internet traffic. The arrivals of packet is not independent on time interval as while loading or streaming something the next video is loaded automatically if the previous video is about to end.eg. youtube,netflix etc.. this proves that it is not independent of the time interval.",
        "answer_feedback": "The response correctly associates the probability of arrivals at a node with previous arrivals. However, the example given does not illustrate this well because the next video's auto-load can be turned off in the application setting. A better example would be on-demand video streaming in general, as the traffic is bursty depending on the segment loading.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption does not hold for real internet traffic. Real traffic is for example dependent on the daytime. Furthermore the application is relevant. Some might use bursty traffic.",
        "answer_feedback": "The response is partially correct because the arrival process' parameters can be time-dependent. In this way, the arrival rate wouldn't depend on the previous arrivals, but instead on the time of the day.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "At the start, the bridges table is empty, it uses flooding for an unknown destination. During the backward learning process, the bridge works in promiscuous mode as it receives any frame on any of its LANs, then the bridge receives frames with sources address Q on LAN L, Q can be reached over L and therefore create table entry accordingly.",
        "answer_feedback": "The response correctly describes how transparent bridges build their bridge table. However, the response does not provide information on how the table is used during the forwarding process and what benefits this brings.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table stores the information, which station it can reach over which LAN (output line). The bridge works in the promiscuous mode, which means that it receives every frame of each connected LAN and during the backwards learning phase when the bridge receives frames with a source address S on a LAN L it \"learns\" that S can be reached over L and creates a table entry accordingly. These entries are associated with timestamps and updated when new frames were received from the source (e.g. S). To forward a frame the bridge will look at the source and destination LANs and drop the frame if they're identical (and therefore prevent unnecessary traffic) but if they are different the bridge can look up in the table to which LAN the frame has to be rerouted. Only if the destination is unknown the network will be flooded with the frame. Because the bridge is not visible as such for the other components of the network, these other components are simplified and they don't have to deal with the forwarding process.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The invisible bridge contains a table which holds information about which address can be reached in which of the connected LANs around it. This table is initially empty, but then filled during the process of backward learning - when the bridge receives a packet from a LAN L with the sender address A, it can be concluded that A is part of the LAN L and therefore routable on this network. As the name clearly states, the bridge in the network is transparent as such, instead it is just addressed with the network receiver address by any senders in one LAN, so that it then can use its table to figure out in which destination-LAN the package should be sent. So one can conclude that the table prevents flooding from the transparent bridge and therefore unnecessary traffic. The other overall feature of the usage of a transparent bridge is the decreased complexity of transmission for all nodes in the combined LANs, because they can just sent packages to all nodes in all connected LANs without having to deal with the routing between the LANs by itselves.",
        "answer_feedback": "The response does not mention how flooding is done when there is no entry for a packet destination in the table, so flooding can not be prevented completely. Apart from that, the response is correct. It also additionally states the benefit of the transparent bridges in general, which was not required.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "All bridges inspect all the traffic and build up tables (bridge tables), these tables hold information to manage the traffic and each entry contains an address and the LAN that leads to that address.\n\nThe bridge table is initially empty and uses flooding for an unknown destination. \nDuring the backward learning phase (Learning process) the bridge works in promiscuous mode and receives any frame on any of its LANs. If the bridge receives frames with source address Q on LAN L and Q can be reached over L, then it will create table entry accordingly. \nThese tables are adapted to changes in topology. Each entry is associated with a timestamp (frame arrival time), and the timestamp of an entry (Z, LAN, TS) is updated when the frame received from Z. \nThe table scanned periodically and old entries purged if no update for some time, usually several minutes (e.g., because the system moved and reinserted at a different position, or flooding was used if the machine was quiet for some minutes).\n\nThe main benefit of bridge tables in the forwarding process is to increase reliability by connecting LANs via VARIOUS bridges in parallel.",
        "answer_feedback": "The response incorrectly mentions the benefit of using multiple transparent bridges but the question asked for the benefit of using bridging information in forwarding frames. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table contains mappings of station to the output line that has to be used to reach the station. The bridge uses promiscuous mode to observe the sent frames and if a frame from a specific source station is sent over a connected LAN the bridge knows that this station can be reached over that LAN and the bridge table is updated. In the forwarding process the destination is looked up in the bridge table and the frame is rerouted to the correct output LAN, if it differs from the current LAN. If the station is not found, flooding is used. A benefit of this setup is that the stations can transparently reach other stations in a different network like they were in the same.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "This bridge table has MAC addresses and ports of bridge in it. At the very beginning, the table is empty, then for example, bridge sees that a frame on port 1 coming from source address A, it knows that A must be reachable via port 1, then it makes an entry in its table.\n\nBridge receives a frame, then it looks up the corresponding destination on its table, if the destination is found, and source address and the destination is identical, the frame would be dropped, if not identical, the bridge will forward this frame to its destination. But if the destination is not found, it will flood.\n\nThis table increases the reliability.",
        "answer_feedback": "The response states reliability as the benefit but it is not mentioned how the table usage increases the reliability. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "HIDDEN TERMINAL Because there no cable connecting every Terminal together it can happen that two or more station can not reach each other and therefor hidden. This can be a problem if for example a terminal has to be quiet so it doesn\u2019t disturb the communication of a neighbor but it can\u2019t get the communication request signal from the communication partner because it is hidden. This can be solved by using a busy signal or by listening to acknowledgments from the neighbor. NEAR AND FAR TERMINALS A signal from a station gets weaker with distance by the inverse square law. This lead to the situation that nearer stations are overpowering stations which are further away and drowning there signal. As a result stations which would normally be able to communicate with each other can\u2019t do so anymore. This can be a severe problem and can only be handled with precise power control.",
        "answer_feedback": "The response correctly describes the near and far terminal problem. In the hidden terminal, two stations cannot reach each other because they are out of one other's detection range, not just because they are wireless nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. application layer - the discovery of services - you will need service awareness and need places for the services security\n\t - outside you can always be attacked, or the mobile routing could be disturbed.",
        "answer_feedback": "The response states application layer related challenges, but there is no clear relation to the wireless network routing challenges. The second point does not specify what is meant by outside. Even the optical fiber lines of wired networks are laid outside, even in seas.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "- The network structure is changing fast, so the routing tables must be adapted to these changes. The routing algorithm needs to converge fast.\n\t - Because all nodes share the same communication medium (the ether), the signaling overhead needs to be minimized, to reduce the load in the medium.",
        "answer_feedback": "In the second challenge, it should be made explicit that the usable frequency is limited.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "- Hidden Terminal Problem:\nAssume we have 2 senders s1,s2 and one receiver r build like this:\n\ns1 \u2192 r \u2190 s2\n\nThe radius of s1 can just sense the receiver and s2 can also just sense the receiver. \n\nS1 is sending something to r. But since S2 cannot sense s1 it assumes the receiver is free and starts sending to r too. Hence s1 is hidden to c the collison detection fails \u2192 Hidden Terminal Problem.\n\n- Exposed Terminal Problem:\n\nAssume we have 2 senders s1,s2 and two receiver r1, r2 build like this:\n\nr1 \u2190 s1 --- s2 \u2192 r2\n\nNow s1 sends to r1. s2 wants to send to r2 but it gets the signal from s1 that it is sending data at the moment. Since s1 is sending to r1 and s2 can not sense r1, it assumes r2 is busy, and hence waits unnecessarily.  --> Exposed Terminal Problem",
        "answer_feedback": "The response related to the hidden terminal is partially correct as s1 assumes the medium instead of the node to be free. The description of the exposed terminal problem is partially correct as well. S2 senses the medium is busy and waits, not because it assumes r2 busy. The wait is unnecessary as S2 wants to send data to R2, and R2 is out of the range of S1.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The Reverse Path Forwarding guarantees that the Packet used the best route when this packet arrived at the IS entry port. Reverse Path Broadcast is based on RPF  to suitable reduce of Overhead. RPF (for a packet arriving at an IS)  -Has this packet arrived at the IS entry port   over which the packets for this station/source are usually also sent?    Yes:  -Assumption:   Packet used the BEST route until now  -Action:     resend over all edges (not including the incoming one)     No:  -Assumption:   Packet did NOT use this route (it is NOT the best route)  -Action:     discard packet (most likely duplicate)  RPB: -Has this packet arrived at the IS entry port   over which the packets for this station/source are usually also sent?    Yes:  -Packet used the BEST route until now?    -YES: select the edge at which the packets arrived and from which they are then rerouted to source S (in reversed direction)  -NO: DO NOT send over all edges (without the incoming one),  i.e., not as in Reverse Path Forwarding (RPF)     No:  -discard packet (most likely duplicate)",
        "answer_feedback": "The response correctly explains RPF and RPB but it lacks the purpose. The purpose of both algorithms is to minimize the number of duplicate packets during broadcasting.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding: Used for ensuring loop-free forwarding of multicast packets in multicast routing and to help prevent IP address spoofing in unicast routing.  Checks if the packet arrived at the IS entry port over which the packets for this station/source are usually sent. If packet is assumed taking the best route: resend over all edges (not including the incoming one). If packet is assumed not taking the best route: discard packet.  Reverse Path Broadcast: Used to check if the set of shortest paths to a node forms a tree that spans the network.  If the packet arrives at the IS entry over which the packets for this station/source are usually sent: Checks if Packet used the BEST route until now: if yes, select the edge at which the packets arrived and from which they are then rerouted to source. If no, do not send over all edges (without the incoming one). If the packet is not for this station/source: discard packet.",
        "answer_feedback": "The purpose of the RPB algorithm is missing in the response. Additionally, the purpose is not limited to unicast and multicast but instead used widely in broadcast too.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.9
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "-prevent routing loops/ cycles in the network   RPF: -each node has a route to every other node -each node only forwards a broadcast packet received from the same port used to send packets back towards the sender -so the packet is forwarded only if it comes from the same route that would be used to reply to the source   RPB: -improvement of RPF If the packet arrived at the IS entry over which the packets for this station/source S are usually also sent and packet used the best route until, then select the edge at which the packets arrived and from which they are then rerouted to source S in reversed direction, if it\u2019s not the best route then not send over all edges without the incoming one. -if not then discard packet.",
        "answer_feedback": "The response is partially correct as the explanation of RPF incorrectly states that \"each node has a route to every other node\" and also does not state over which links the packet are forwarded in RPF. The purpose and explanation for RPB are correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "They are techniques to forward multicast packets in networks. They use information the IS has about the network structure (derived from normal unicast packets) to guess where to send the multicast packets. If a packet in RPF arrives over the \"usual\" path over which the sender sends, the IS will distribute(flood) the network with the packet. If the packet arrives not over the usual path, the packet will be dropped. In RPB, if the packet arrives over the \"usual\" path, the IS will send it over the path that unicast packets \"usually\" take and not flood the network. If the packet arrives not over the usual path, the packet will be dropped.",
        "answer_feedback": "The response is partially correct because it lacks the purpose of both algorithms which is to minimize the number of duplicate packets during broadcasting. In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.7
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding (RPF) and Reverse Path Broadcast (RPB) are algorithms that are used to distribute packets with more than one receiver in a network. Simple approaches are individual sending to every destination or flooding. These simple approaches aren\u2019t optimal for distributing packets to n receivers. In RPF each router has information which path it would use for unicast packets. If a router receives a package, it checks whether it received the package via the optimal route, and only forwards it to every other reachable router (except from the router it received the package from). In RPB however, packages are only forwarded according to the routing tables (via the best routes), thus reducing the load of the network.",
        "answer_feedback": "The response does not state why RPF and RPB are more optimal than flooding. The explanation for RPF is correct. The explanation for RPB is incomplete as the answer does not specify what \"according to the routing tables\" / \"via the best routes\" means.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding is an algorithm to allow loop free forwarding of packets (especially for multicast). The source IP of an incoming packet is looked up in the routing table. If the packet would be send on this interface if the source IP would be the destination, the packet is forwarded on all edges but the incoming one. Packets that don't arrive via the shortest route may be ignored. Reverse Path Broadcast is also used for loop free forwarding and works similar to RPF. Though it does not send the packets out on all edged but selects those edges that are on the shortest path (in reverse) to the source of the multicast packet.",
        "answer_feedback": "The explanation of RPF is partially correct because the packets which did not use the best route will be discarded and not maybe discarded.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse path forwarding prevents multicast traffic from entering routing loops by looking up a table which holds all routers the multicast packet already visited. The packet is then forwarded to all routers that are not in the table. Reverse path broadcast is an extension of RPF: in this case packets are only forwarded to this interfaces, where the next router is on the shortest path to data origin.",
        "answer_feedback": "The response correctly identifies the purpose of RPF but the provided explanation is incorrect. In RPF, the packet used the unicast information stored in the routing table to check whether the broadcasted packet took the same route that it would have taken to send a unicast packet in the reverse direction. No explanation is provided as to what forms the shortest route in RPB.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.4
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "In both,Packet used the BEST route with specific selection of the outgoing links. Reverse Path Forwarding:Packet used the best route and resend over all adjacent edges (not including the incoming one). Reverse Path Broadcast:Packet uses the best route and sends packet to adjacent nodes but select the edge at which the packets arrived and from which they are then rerouted to source in reversed direction and include the arrival node.",
        "answer_feedback": "The response explains the difference between the two correctly. The response does not state the purpose behind using the algorithms, namely to reduce duplicates in the broadcast. While both use the best route, how these routes are known is not explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.4
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "PROPERTY: GLOBAL KNOWLEDGE OF THE MULTICAST GROUP\u2019S SPANNING TREE (MULTICAST TREE)  INITIALLY ONLY LOCAL KNOWLEDGE   ALL IS SEND LINK STATE PACKETS PERIODICALLY  -CONTAINING INFORMATION     DISTANCE TO NEIGHBORS     EXPANDED BY INFORMATION ON MULTICAST GROUPS -BY BROADCAST TO ALL THE OTHERS EACH IS CALCULATES A MULTICAST TREE -FROM THE NOW LOCALLY AVAILABLE AND COMPLETE STATE INFORMATION BASED ON THE INFORMATION ABOUT THE MULTICAST TREE - IS DETERMINES THE OUTGOING LINES - ON WHICH PACKETS HAVE TO BE TRANSMITTED",
        "answer_feedback": "The response is partially correct because it lacks the attractive property of spanning trees.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning Tree is known to the IS as, generates a minimum number of packet copies , that IS generates a copy of a packet for each required outgoing line and all spanning tree lines except incoming one have to be defined. It has to know the multicast basic principle, that all IS have to know the multicast tree. So all IS nodes send link state packets periodically.  The IS defines the outgoing lines and which packets have to be transmitted.",
        "answer_feedback": "While it is correct that a spanning-tree generates minimum copies of the message, it is not clear from the answer what the response meant by \"IS generates copy for each outgoing and spanning tree line\". The link-state modification for constructing spanning trees does not explain how each node shares its multicast information with others by adding it to the link state packet.  Each node then has the complete information to build a multicast spanning tree.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "You use Reverse Path Forwarding with pruning. After the tree is set up the broadcast tree you know who belongs to the multicast. - If all child nodes aren't part of the multicast tree the parent knows it itself isn't part of the multicast tree. (Bottom Up) You can modify Link state routing by not only considering the \"distance\" between neighbors but also information on multicast groups.",
        "answer_feedback": "The desirable property is not because it makes Reverse Path forwarding possible. Instead, it is loopless and thereby reduces duplicates when broadcast- and multicasting. We do need to add the information to which group each IS belongs to in the link-state packet but it is not stated how it is propagated and used to construct the multicast spanning tree.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "the spanning tree algorithm determines the packets for the broad and multicasting While the link state packets will be sent, the containing information will be expanded by information on multicast groups - every IS calculate now its multicast tree",
        "answer_feedback": "Though it is correct that a spanning tree determines the path in multi-/broadcast, it does not answer why they are used. The reason is no loops in the spanning tree leading to reduced unnecessary duplicates.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "1. There is no loop",
        "answer_feedback": "The response lacks the explanation of the link state's modification to construct spanning trees. To calculate the spanning trees for multicasting, you also have to know which nodes belong to which groups. The link-state packets have to be expanded with multicast group information so other nodes can construct multicast trees by themselves.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "The advantage of using a spanning tree for broad-/multicasting is that no duplicate messages are sent. This reduces network load while providing the exact same performance, only at the cost of lower reliability. If we want to construct a spanning tree using Link State Routing, each node, after having received the link state packets from all other nodes, calculates a spanning tree using the received information. The node will then use the connections from the calculated spanning tree to distribute multicast packets efficiently.",
        "answer_feedback": "The response correctly answers why a spanning-tree usage is ideal in multicast and broadcast. The explanation for modifying the link-state algorithm to construct a  multicast spanning tree for nodes does not state how a node gets to know about the other members of the multicast group and how this information is propagated to other IS.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Property: subnets of subnets can be displayed and addresses which enables more possibilities for multi-/broadcast for distribution of information\n\nmodification of link state routing for spanning tree multicast: \n\n- all IS have to know the multicast tree. \u2192which group belonging \n\n- Information distribution via link stated routing.\n\n- all IS send updates (link state packages) periodically\n\n\u2192calculate the own tree\n\n\u2192DETERMINE possibilities for transmission",
        "answer_feedback": "The desirable property is not correctly stated. Yes, a spanning tree is a subnet of the subnet, but what makes it unique is that it does not contain loops and thereby reduces unnecessary duplicates during multicast and broadcast. Additionally, the link-state packet needs to contain multicast group information so that each node can discover its fellow group member.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are located between header and payload. They can contain options or other information which extend the header. The main advantage of the extension headers in IPv6 compared to IPv4 is that they are optional. In IPv4 there is a part for the options reserved but in IPv6 when there are no options the space of the extension headers can be used for a longer payload. So the extension headers are more efficient.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The stated main advantage is incorrect as the option field in the IPv4 header is already optional, without any reserved space.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The header in IPv6 has a fixed length and is designed to be used for easy processing, it only contains information needed for routing. Any additional information is stored in extension headers. They carry optional information and can be found in between the fixed header and the playload. Since the whole IPv6 packet is only allowed a certain size, these additional extension headers take up space of the payload. The main advantage of extension headers is that they can be added optionally and help to overcome size limitation.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, the first stated main advantage is incorrect. The other advantage is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "- located between fixed header and payload\n- are optional, modularly including additional information, e.g. Routing information, Authentication, or Destination options\n- Ipv6 has a fixed sized header",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The IPv6 header has a fixed size but the response should state what advantage is concluded from this fact, so it is only partially correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers that can augment the main header. They are located between the main header and the payload. Their main advantages are that they cause less overhead since they can be omitted if not needed and that new headers can be added in the future.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional and could be 0 bits long, there is no added advantage over the IPv4 option field in terms of unnecessary overhead. The other advantage is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers are extensions for the normal header. You can support multiple addresses or specify more options for your header and packet, like e.g. authentication.\n\nThe Extension Headers are located between the normal header and the payload, they will be attached to the normal header. \n\nThe biggest advantage of Extension Headers is the possibility to use broadcasting.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The advantage is incorrect as IPv6 does not support broadcast.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers located between the fixed header and payload. As they are optional, less data can be transferred by leaving them out. They also help overcome size limitations and allow appending new options without changing the fixed header in the future.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, there is no added advantage over the IPv4 option field in terms of the amount of transmittable data.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers contain of additional information like Routing and Fragmentation for the network device to decide how to process the IPv6 packet. Extension headers are placed between fixed header and payload. IPv6 options are placed in separate extension headers that are located between the IPv6 header and the transport-layer header in a packet. They help to overcome size limitation and allow to append new options without changing the fixed header. The main advantage is efficiency: Since there is no extra space for options between fixed header and payload there is a smaller header and therefore more space for payload. This way it is much faster.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The stated main advantage is incorrect as even the option field in the IPv4 header is optional. But as additionally stated in response it provides the flexibility to add extra options without changing the fixed header size. This is a correct advantage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers that store additional information.\nThey are located between the fixed header and the payload.\nA lot of the information that is stored in extension headers in IPv6 is stored the fixed header in IPv4. Because of that the 'space' for the information is always reserved in IPv4, even if you don't need it. In IPv6, because extension headers are optional, you don't have to reserve any space for this information, if you don't need it.",
        "answer_feedback": "The response is partially correct because the stated advantage is incorrect. After all, the options field in IPv4 is optional and also allowed to be 0 bit long, so there are no extra space benefits based on it.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers allow to extend the new, simplified and fixed-size IPv6-header with additional options. These headers are located between the standard header and the payload (upper-level headers and user data). This approach allows to add several additional options without reserving space in the standard header for such optional data,allowing the standard header to be smaller.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. In IPv4, there is also no reservation of space for unused options. The main IPv6 header is most often larger than the IPv4 header in practice. Some benefits result from the main header having a fixed instead of a smaller size.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is used to manually/automatically assign IP addresses to physical devices inside of a network with the help of a DHCP server. A client with no IP address sends a broadcast DHCP discover packet to everyone on the network. The Server will respond with a DHCP offer, where he offers an IP address to the client. The Client will then respond with a DHCP request (telling the server he wants the IP address). The Server will assign the IP address to the Client for a certain time period (DHCP ACK). After this leasing time, the client has to renew the leasing of the IP address otherwise the DHCP Server will remove the IP address of the client again.",
        "answer_feedback": "The response is partially correct because the DHCP definition part is missing in the answer. There is an explanation about how DHCP works but that is not part of the question requirement.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The dhcp protocol is a protocol to configure systems that join a network.\nIt is used to assign ip addresses to systems within the network. \nIf a system joins the network it can ask the dhcp server for network configuration and an ip address that it should use in the future.",
        "answer_feedback": "The response is partially correct because it lacks a DHCP usage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "It is used to assignIP addresses to hosts in a network.",
        "answer_feedback": "The response does not give a definition and does not specify how DHCP assigns the IP addresses, i.e. dynamically or automatically.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a network management protocol which configures end devices on IP Networks (mostly LANs) by assigning them an IP address and other network configuration parameters.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The precise usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "It\u2019s a network protocol used on IP networks to dynamically assign an IP address and other information to any device (host) on a network so they can communicate using IP.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The precise usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP extends the functionality of RARP. It is used for automatic IP address assignment.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The DHCP is used to add clients into a network by giving them the required information/addresses.",
        "answer_feedback": "The response partially describes DHCP as it lacks specifics, like what information is shared, to be considered complete and precise.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Requirement: The interval of two adjacent frames, which are sent by sender, is short.\nSo that we can use piggybacking to response these two frames with one acknowledgement. \nThe communication has to be duplex (so the protocol must not be \"Utopia\").\nAnd the receiving buffer from the Sender must be ,so that it is able to store the ACK plus the additional data!",
        "answer_feedback": "The response contains a duplex connection as one of the requirements, but having to send two frames within short intervals is incorrect. Also, the same data and acknowledgments are tied together in piggybacking. Therefore, the total buffer space requirement should ideally remain almost the same as when they are sent separately.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "There must also be frames directed towards A (sender) in the transmission, so that B (receiver) sends frames back to A in a reasonable amount of time. In addition to that the amount of frames size of both parties must be similarly big, because the acknowledgement is added to frames directed at A. As a result there must be a certain balance of frames in both directions.",
        "answer_feedback": "The response correctly implies duplex operation. However, a balanced approach is difficult to achieve in real scenarios, so there are ways to overcome it, like a dedicated timer signaling a timeout in the absence of data to be sent.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "A receiver of a data frame has to send data frames the ACKs are piggybacked onto at a rate that is high enough so that the sender doesn't have to wait for too long for the ACKs to arrive. Otherwise a timeout might occur and the sender sends the frame again.",
        "answer_feedback": "The response states duplex communication indirectly but a dedicated timer timeout can also be on the receiver side to send acknowledgment separately when sufficient data is not present.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Network has varying transit times for packets, certain loss rate and storage capabilities, as well as\u00a0 packets can be manipulated, duplicated by flooding and resent by the original system after timeout.",
        "answer_feedback": "The response is not precise about the problem or consequences caused when duplicates are present.",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 - 10.255.255.255172.16.0.0 - 172.31.255.255127.0.0.0 - 127.255.255.255",
        "answer_feedback": "172.x.y.z is not in Class A anymore",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 bis 10.255.255.255 private netze",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Network Addresses:\u00a0\u00a0\u00a0 [0-127].0.0.0\nBroadcast Addresses: [0-127].255.255.255",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "For each of the 2^7 = 128 networks the first and the last address are reserved.\n- Network address (all zeros)\n- Broadcast address (all ones)",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    }
]
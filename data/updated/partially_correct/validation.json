[
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would use Token Ring, because it has good throughput even during high utilisation, which is to be expected at 20 systems and it can be expanded later, as it supports a maximum of 250 stations. But you need a central monitor.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws! Why is a central monitor a disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "non-persistent CSMA should be used. Reason 1: It is good for the Networks with a high load medium. Reason 2: it offers a good throughput. Weakness: The delays are longer for each single station.",
        "answer_feedback": "What is the difference between reason 1 and reason 2?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD\nrecommand reason:\n1. the CSMA/CD can reach high speed to transfer the medium.\n2. CSMA/CD can deal with collision\n\npotential weakness:\ncan not avoid collision and need to listen the link while transferring the medium.",
        "answer_feedback": "Why is dealing with collisions an advantage? And why is listening to the link a disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Tokenring wouldn't be impaired by the high channel load and can be easily expanded. Depending on the traffic and number of computers connected time between sends may be long though.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws and it is not easy to expand!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Using Token ring would be a good solution, as it is easy to implement (in terms of hardware) and it provides fairly distributed medium access even in high load situations. A weakness is the ring topology that requires a newly added system to be connected to two other systems.",
        "answer_feedback": "Well, there are other solutions, that would be easier to implement then token ring (Aloha for example)",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I will use Token Ring, because it has random frame lengths and good throughput. That's why Token Ring is expandable.\nBut there is delay because of waiting for token.",
        "answer_feedback": "Random frame length and good throughput do not lead to extendability!\u00a0Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend non-persistent CSMA because it is expected that multiple stations want to send at the same time (high channel load) and it provides good throughput even during high utilization. One weakness of this MAC procedure is, that they are long delays for single stations.",
        "answer_feedback": "What is another reason? You only gave 1",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding due to the good utilization of the bandwidth",
        "answer_feedback": "The response does not provide the second reason behind using the binary encoding in the given scenario which is the lack of need of self-clocking making binary encoding a better option.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding. beacause it has a good utilization of the bandwidth.",
        "answer_feedback": "The response does not provide the second reason behind using the binary encoding in the given scenario which is the lack of need of self-clocking making binary encoding a better option.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Calculate the probability that there are 0, 1, 2, 3, 4 .. 9 packets in the queue.  Sum these probabilities and multiply that by 60 seconds.\n\nProbabilities can be calculated with these formulas where Pn is the probability the queue has n packets in it:\nP0 = (1-R)/(1-R^(N+1))\nPn = (1-R)R^n/(1-R^(N+1))\nR = 9/10 = 0.9\n\nProbability of 0 to 9 packets in the buffer = 0.9492\nSeconds = 56.96",
        "answer_feedback": "The response correctly explains how the number of expected seconds can be calculated. However, the number of expected seconds or the probability is rounded incorrectly. The correct value is 56.952 instead of 56.96 seconds.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "There are multiple possible ways a package could take on its way to G. Problems occur because when a path is chosen it's utlilization is goes up makting this a less attractive option, so another path is chosen, repeating indefinately, causing heavy routing overhead.",
        "answer_feedback": "Why would this be repeated? If another path has less utilization, the path will be used and the packet will be transmitted to G.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Using load can lead to oscillation, which means the path that seems best is always changing after the load is set to one path. \nExample: Path A and Path B have load 0, Path A is selected, its load goes up to 1. Now path B has the lower current load, so therefore the system would select path B and its load goes up ti 1. Now Path A is 0 again and the cycle continues.",
        "answer_feedback": "But why is oscillation a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "It would cause problems because links / connections on the path could be overloaded or get disconnected / failed. This would result that packets could be lost or need a different path. Furthermore, the current load on a link could change after the best path was found.",
        "answer_feedback": "Why would a path get overloaded?\nWhen a current load on a link changes, the best path changes too, there is no problem with this.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "This strategy is problematic, because the utilization can change over time, therfore A can assume paths which are over used at the moment. A update structure for the metric would be needed.",
        "answer_feedback": "To evaluate the path, the current\u00a0load is used: changes in the utilization are considered and A will not use paths which are overused at the moment.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Multiple transmissions with load may lead to oscillation of the load. Another problem in applying the flooding procedure is the inconsistency, i.e. varying states are simultaneously available in the network.",
        "answer_feedback": "We do not apply flooding here!\nWhy is oscillation a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "This strategy could cause problems:\nThe routing algorithm would constantly alternate between using link CF or EI, because when one of them is part of the current route, the other one seems like the better choice because it has lower utilization. So even if using only one of the links exclusively would make more sense because it may have higher bandwidth than the other one, this strategy would alternate between them.",
        "answer_feedback": "But why is alternating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop2:\n(B; C, drop) <= B knows that C does not receive unicast packets via B\n(B, E, forward)\n(C, B, drop)<= C knows that B does not receive unicast packets via C\n(C, D, drop)<= C knows that D  does not receive unicast packets via C\n(C, E, drop)<= C knows that E does not receive unicast packets via C\n(C, F, forward)\n(D, C, drop)<= D knows that C does not receive unicast packets via D\n(D, F, drop)<= D knows that F does not receive unicast packets via D\nHop3:\n(E, C, drop)<= E knows that C does not receive unicast packets via E\n(E, F, drop)<= E knows that F does not receive unicast packets via E\n(E, G, forward)\n(F, E, drop)<= F knows that E does not receive unicast packets via F\n(F, G, drop)<= F knows that G does not receive unicast packets via F\n(F, D, drop)<= F knows that D does not receive unicast packets via F\nHop4:\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2:\n(B, E, forward)\n(B, C, drop) <= A->C is shorter\n(C, B, drop) <= A->B is shorter\n(C, E, drop) <= A->B->E is shorter\n(C, F, forward)\n(C, D, drop) <= A->D is shorter\n(D, C, drop) <= A->C is shorter\n(D, F, drop)\u00a0 <= A->C->F is shorter\u00a0\n\nHop 3:\u00a0\n(E, C, drop) <= A->C shorter\n(E, F, drop) <= A -> C-> F is shorter\n(E, G, forward)\n(F, D, drop) => A->D is shorter\n(F, E, drop) => A -> B-> E is shorter\n(F, G, drop) => A -> B -> E -> G is shorter\n\nHop 4:\n(G, F, drop) => A->C->F is shorter\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop2:\n(B, E, forward)\n(B, C, drop) <= C doesn't send any Packets to A, over B\n(C, D, drop) <= D doesn't send any Packets to A, over C\n(C, E, drop) <= E doesn't send any Packets to A, over C\n(C, F, forward)\n(D, C, drop) <= C doesn't send any Packets to A, over D\n(D, F, drop) <= F doesn't send any Packets to A, over D\n\nHop 3:\n(F, E, forward)\n(F, G, forward)\n(E, F, drop) <= F doesn't send any Packets to A, over E\n(E, G, forward)\n\nHop 4:\n(G, H, drop) <= H doesn't have any adjacent nodes other than G",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop2:\n(B, E, forward)(C, F, forward)\nHop3:(E, G, forward)Hop4:\n(G, H, forward)",
        "answer_feedback": "The reason also needs to be provided when a packet is not forwarded to other nodes i.e. dropped by the receiver as stated in the question \"list all the packets\u00a0which are sent together with the information whether they will be forwarded or dropped at the receiving nodes.\"\u00a0Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, E, drop) not minimal spanning tree(C, F, forward)(D, F, drop) not minimal spanning tree(B, C, drop) not minimal spanning tree(C, B, drop) not minimal spanning tree(C, D, drop) not minimal spanning tree(D, C, drop) not minimal spanning tree\nHop 3:\n(E, C, drop) not minimal spanning tree(E, F, drop) not minimal spanning tree(E, G, forward)(F, D, drop) not minimal spanning tree(F, G, drop) not minimal spanning tree(F, E, drop) not minimal spanning tree\nHop 4:\n(G, H, forward)(G, F, drop) not minimal spanning tree",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1: \n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2: \n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, drop) -> Packet drops because it only has one neighbour and node\u00a0 H does not forward the message.",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.5 for reasoning\u00a0(A,D, drop),\u00a0(C, F, drop)",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, drop) <= H has only one neighbor (G) from which it got the message",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, E, drop) <= because E never send a packet to A from C\n(C, F, forward)\n(D, F, drop)\u00a0<= because F never send a packet to A from D\nHop 3:\n(E, G, forward)\n(E, F, drop) <= because F never send a packet to A from E\n(F, E, drop)\u00a0<= because E never send a packet to A from F\n(F, G, drop)\u00a0<= because G never send a packet to A from F\nHop 4:\n(G, H, drop) <= H is the last node and there is no other link",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward);\u00a0(A, C, forward);\u00a0(A, D, forward).\nHop 2:\n(B, E,\u00a0forward);\u00a0\u00a0(B, C, drop) from A to\u00a0 C ,via B is not the\u00a0shortest;\u00a0\n(C, B, drop),from A to B ,via C is not the\u00a0shortest;\u00a0(C, D, drop),\u00a0from A to D ,via C is not the\u00a0shortest;\u00a0(C, E, drop),\u00a0from A to E ,via C is not the\u00a0shortest;\u00a0\n(C, F,\u00a0\u00a0forward).\u00a0\u00a0\n(D, C,\u00a0drop) ,from A to C ,via D is not the\u00a0shortest;\u00a0(D, F,\u00a0drop) ,from A to F ,via D is not the\u00a0shortest.\nHop 3:\n(E, C,\u00a0drop),\u00a0from A to C ,via E is not the\u00a0shortest;\u00a0(E, F,\u00a0drop),\u00a0from A to F ,via E is not the\u00a0shortest;\u00a0(E, G,\u00a0forward).",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, forward)",
        "answer_feedback": "The reasoning behind which packets are dropped is not stated.\u00a0 Please go through the model solution. Packets will be considered drop if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop),\u00a0(C, F, drop) and (G, H, drop)",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forw.)(A, C, forw.)(A, D, forw.)\nHop 2:\n(B, E, forw.)(C, F, forw.)\nHop 3:\n(E, G, forw.)\nHop 4:\n(G, H, forw.)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "(sender, receiver, drop)\u00a0\nHop 1:\n(A, B, forward)(A, C, forward)\u00a0\n(A, D, forward)\nHop 2:\nNode B:\u00a0\n(B, C, drop) <= C is not on the best path to A\n(B, E, forward)\nNode C:\n(C, B, drop) <= B is not on the best path to A\n(C, D, drop) <= D is not on the best path to A\n(C, E, forward)\n(C, F, forward)\nNode D:\n(D, C, drop) <= C is not on the best path to A\n(D, F, forward)\nHop 3:\nNode E:\n(E, F, drop) <= F is not on the best path to A\n(E, G, forward)\nNode F:\n(F, E, drop) <= E is not on the best path to A\n(F, G, forward)\nHop 4:\n(F, H, forward)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n\n(B, E, forward)\n\nB would not forward the packet from S to C because B knows that from C to S the packet would never go through B\n(C, F, forward)\nSimilar reason, C would not forward the packet from S to D and S to B\nD would not forward the packet from S to C and S to F\nHop 3:\n\n(E, G, forward)\nE would not forward the packet from S to C and S to F\nF would not forward the packet from S to C and S to E\nHop 4:\n(G, H, forward)\nG would not forward the packet from S to F",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 2:\u00a0\nFrom B:\u00a0\n(B, C, drop), (B, E, forward)\u00a0\nFrom C:\u00a0\n(C, B, drop),\u00a0\n(C, D, drop),\u00a0\n(C, E, drop),\u00a0\n(C, F, forward)\u00a0\nFrom D:\u00a0\n(D, C, drop),\u00a0\n(D, F, drop)\u00a0\nHop 3:\n\u00a0From E:\u00a0\n(E, C, drop),\u00a0\n(E, F, drop),\n(E, G, forward)\u00a0\nFrom F:\n(F, D, drop),\u00a0\n(F, E, drop),\u00a0\n(F, G, drop)\u00a0\nHop 4:\u00a0\nFrom G:\u00a0\n(G, F, drop),\u00a0\n(G, H, drop)\u00a0\nBecause vertex H has only one neighbor from which it got the message, vertex H does not forward the message.\nIn total 19 messages are sent during the broadcast.",
        "answer_feedback": "Hop 1 not given and\u00a0(C, F, drop) occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,B,f)\n(A,C,f)\n(A,D,f)\n\nHop 2:\n(B,E,f)\n(B,C,d) <- A,C is faster, not minimal spanning tree\n(C,B,d) <- A,B is faster, not minimal spanning tree\n(C,E,d) <- B,E is faster, not minimal spanning tree\n(C,F,f)\n(C,D,d) <- A,D is faster, not minimal spanning tree\n(D,C,d) <- A,C is faster, not minimal spanning tree\n(D,F,d) <- A,C,F is faster, not minimal spanning tree\n\nHop 3:\n(E,C,d) -> A,C is faster, not minimal spanning tree\n(E,G,f)\n(E,F,d) -> A,C,F is faster, not minimal spanning tree\n(F,E,d) -> A,B,E is faster, not minimal spanning tree\n(F,G,d) -> A,B,E,G is faster, not minimal spanning tree\n(F,D,d) ->A,D,F is faster, not minimal spanning tree\n\nHop 4:\n(G,F,d) -> A,C,F is faster, not minimal spanning tree\n(G,H,f)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, forward)Hop 2:\n(B, C, drop) <= C already has packet from A from Hop #1(B, E, forward)(C, B, drop) <= B already has packet from A from Hop #1(C, D, drop) <= D already has packet from A from Hop #1(C, E, drop) <= E has packet from B with better metric from route ABE (2+1=3) vs. ACE (2+2=4)(C, F, forward)(D, C, drop) <= C already has packet from A from Hop #1(D, F, drop) <= F has packet from C with better metric from route ACF (2+1=3) vs. ADF (2+3=5)Hop 3:\n(E, C, drop) <= C already has packet from A from Hop #1(E, F, drop) <= F already has packet from C from Hop #2(E, G, forward)(F, D, drop) <= D already has packet from A from Hop #1(F, E, drop) <= E already has packet from B from Hop #2(F, G, drop) <= G has packet from E with better metric from route ABEG (2+1+1=4) vs. ACFG (2+1+2=5)Hop 4:\n(G, F, drop) <= F already has packet from C from Hop #2(G, H, drop) <= H has only one neighbor (G) from which it got the message",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. TSAP only valid for one connection; some TSAP are well known\n2. identify each connection by SeqNo; Endsystem must store this information\n3. identify each PDU by SeqNo; higher usage of bandwidth and memory",
        "answer_feedback": "Few advantage and disadvantage are missing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.67
    },
    {
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Sequence numbers: + discards duplicates\n2. PAR: + doesn't block at loss of both frames and ACKs; - long wait possible\n3. NAK: + discards bad frame, can reduce additional traffic",
        "answer_feedback": "the problem of duplicate packets on the transport layer in a connection-oriented service needs to be explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.17
    },
    {
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "One method to avoid duplicate packets is to make the time-out time of the packets larger. The advantage of that is that the sender has enough time to receive the acknowledgment of the packets. The Disadvantage is that if the time-out time is too large and the sender has only a certain window size to send unacknowledged packets, it could results that the sender is sending the data too slow and the receiver has to wait.\n\nAnother method is that the receiver can ignore/ discard the same packets by using sequence numbers. The advantage is that the receiver will not be full of duplicate packets and knows via sequence number which packet should arrive next. The disadvantage is that the packets send with sequence numbers are larger.\n\nAnother method is to use temporarily valid TSAPs. The advantage is that the TSAP is only valid for one connection only. A disadvantage is that server addressing is not possible because the server is reached via a designated/known TSAP.",
        "answer_feedback": "The problem of duplicate packets on the transport layer in a connection-oriented service needs to be resolved.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.67
    },
    {
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "Method 1: to use temporarily valid T SAPs\nGenerate unique (Transport) service Access Point (TSAP) for each communication and they are valid for one connection only\nAdvantages:\n- You can always generate new TSAPs\nDisadvantages:\n- Some TSAPs are standardized(\"well-known ports\") and cannot be used\nMethod2: to identify connections individually\neach connection is assigned a new Sequence number and end systems story assigned Sequence number and remember them\n\nAdvantages:\n- Duplicates from another connection with a Sequence number does not interact with other connection with a different sequence number\nDisadvantages:\n- Only works with connection-orinted service\n\nMethod 3: to identify PDUs individually: individual sequential numbers for each PDU",
        "answer_feedback": "Disadvantage of third method not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.83
    },
    {
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. use temporarily valid TSAPs\n+ easy and quick\n- does not identify the unique packets inside a connection.\n\nFolie 23 / 10 Transport Layer",
        "answer_feedback": "Only one method mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.33
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start (getting to equilibrium) \u201cWant to find this extremely fast and wasting time\u201d. Phase 2: Congestion Avoidance \u201cAdditive increase - gradually probing for additional bandwidth. Multiplicative decrease - decreasing cwnd upon loss/timeout\u201d. Congestion Window (cwnd): Initial value is 1 MSS (=maximum segment size), and counted as bytes. Slow-start threshold Value (ss_thresh): Initial value is advertised window size.\n\t Phase 1: Slow start (cwnd is less than ss_thresh) => After initialize (cwnd =1), each time a segment is acknowledged increment cwnd by one (cwnd++). Then continue until reach ss_thresh (window size) or packet loss. Phase 2: Congestion avoidance (cwnd >= ss_thresh) => When Timeout, that means there is a congestion. And in each time congestion occurs ss_thresh is set to 50% of the current size of the congestion window (ss_thresh = cwnd/2), cwnd is reset to one (cwnd = 1), and slow-start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase is missing details about how ss_thresh changes when a packet is lost. Also, the congestion avoidance phase's explanation lacks how the cwnd is incremented.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The 2 phases are slow start and congestion avoidance. cwnd indicates the number of segments that are send. Each time this sent segment(s) are acknowledged, cwnd and the send segments double (=increments by one per ack) \u2192 send a sement, receive ack, increment cwnd, send 2 segments, receive ack for both, increment cwnd by 2 (because of 2 received ack), send 4 segments\u2026 exponential growth.\nThis continues until ss_thresh is reached (cwnd >= ss_thresh), or a packet is lost. If a packet is lost, then cwnd falls back to initial size 1 and ss_thresh is set to current cwnd/2. If no packet is lost but threshold reached, then don\u2019t double amount of send segments each RTT (add 1 segment per received ack) but only add 1 segment to each incremented cwnd (linear growth).",
        "answer_feedback": "The response is partially correct because if congestion occurs, ss_thresh is set to half of the current size of the congestion window and the congestion window is reset to one, in both phases. This, therefore, also happens in phase 2.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: cwnd is increased by one with each each acknowledgement, which effectively means doubling the cwnd each RTT. When cwnd reaches ss_thresh congestion control goes to Congestion Avoidance phase.\n2. Congestion Avoidance: cwnd is increased by one each RTT.\n\nEach time congestion occurs, ss_thresh is set to cwnd/2, cwnd is reset to 1 and slow-start is entered again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The first phase is called \"slow start\".\nAfter the initialisation, the sender starts sending segments, and waits for the receiver to acknowledge them all. This number will double every Round trip time (RTT) until the advertised window size is reached. If a timeout happens beforehand, phase one is restarted immediately.\n\nWhen ss_thresh is reached, phase two - \"congestion avoidance\" - is entered, when the \nThe RTT will be increased linearily until a timeout occurs. When this occurs, phase one is initialized again.\nThese two phases will be repeated over and over again, the sending rate will never be constant with TCP.",
        "answer_feedback": "The response is correct except that in both the phases when the congestion occurs,  ss_thresh is set to half of the current size of the congestion window and the congestion window is reset to one, which is not the same as the initialized value where cwnd = 1 and ss_thresh = advertised window.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are:\n1. Slow Start\n2. Congestion Avoidance\n\nIn the Slow Start phase, the sender sends as many segments as the size of the cwnd. Every time a segment is acknowledge the cwnd increases by one. So, in the slow start phase the number of sent packets increases exponentially. When the cwnd reaches the ss_thresh or there is a packet loss, the system changes to Congestion Avoidance phase.\nIn the Congestion Avoidance phase, each time congestion occurs ss_thresh becomes cwnd/2 and cwnd is reset to 1. Then, the Slow Start phase restarts again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start\nThe basic idea behind \"slow start\" is to send packets as much as the network can accept. It starts to transmit 1 packet and if that packet is transmitted successfully and receives an ACK, it increases its window size to 2, and after receiving 2 ACKs it increases its window size to 4, and then 8, and so on.. \"slow start\" increases its window size exponentially until the slow-start threshold is reached.\n\nPhase 2: Congestion Avoidance\nAfterwards, the congestion window is only incremented by one unit if all packets from the window have been successfully transmitted. It therefore only grows linearly per roundtrip time. This phase is called the Congestion Avoidance Phase. If a timeout occurs, the congestion window is reset to 1 and the slow-start threshold is reduced to half of the congestion window. The phase of exponential growth is thus shortened, so that the window grows only slowly in case of frequent packet losses.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are Slow start(getting to equilibrium) and Congestion avoidance.\nThe slow start is to discover the correct sending rate where initial congestion window size is 1 and each time the segment is acknowledged, then increasing it by one this is done until the slow start threshold is reached or ends with packet loss.\nIn case of congestion avoidance, when the congestion occurs then we reduce the value of slow start threshold to half the value of the congestion window and then resetting back the value of congestion window to 1 and starting the slow start phase again.",
        "answer_feedback": "TThe response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "cwnd indicates how much data can currently be sent at once before waiting for acknowledgements.  cwnd is doubled every time all remaining acknowledgements are received and as long as cwnd is lower than ss_thresh. Once a packet times out (no acknowledgement received), cwnd is reset to 1 and ss_thresh is set to half its previous value (phase one - slow start).  \n\t After surpassing ss_thresh, cwnd will only increase linearly (phase two - congestion avoidance) and will revert to slow start after a timeout.",
        "answer_feedback": "The response correctly states the name of the two phases. The explanation of the slow start phase and congestion avoidance phase is correct except that  ss_thresh is set to half its current congestion window value, not half of its previous threshold value. When the timeout occurs in phase 2, the congestion window is reset to 1, and ss_thresh is set to half its current congestion window value and then reenter into the slow-start phase.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are called Slow start and congestion avoidance. In the slow start phase the cwnd is less than ss_thresh which mean we send less data than the advertised window. In the congestion avoidance cwnd is greater or equal to the advertised window which means we send more or exactly enough to saturate the receiver, since we started slowly, we know that the network is very likely to handle the traffic.",
        "answer_feedback": "The response correctly states the name of the two phases. The response does not state the condition, nature, and degree of change in the congestion window and slow start threshold. Further, the answer is missing what happens when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "At the beginning (during phase 1, also called slow start), the Congestion Window (cwnd) is set to one and duplicates with each cycle until ss_thresh is reached. Once cwnd >= ss_thresh, phase 2 starts, also called congestion avoidance. Here, cwnd is only incremented by one until a congestion occurs. Then, the process starts again with ss_thresh = cwnd/2 and cwnd=1.",
        "answer_feedback": "The response is partially correct because it is unclear what is meant by the cwnd duplicating every \"cycle\" in the slow start phase. It is also unclear when cwnd increments in the congestion avoidance phase. The slow start phase description is missing details about how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are \"slow start\" and \"congestion avoidance\". In phase 1 cwnd less than  ss_thresh. The congestion window increases exponentially until cwnd >= ss_thresh (so cwnd=1, then 2, then 4 etc.). After the threshold is reached, phase 2 is entered where we have an additive increase and a multiplicative decrease. This means, that the cogestion window now is always increased by 1 every roundtrip time and when a timeout (=congestion) occurs, the ss_thresh is set to 50% of the current size of the congestion window (ss_thresh=cwnd/2), the congestion window is reset to 1 and the slow start (phase 1) is entered again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: Each time a segment is acknowledged cwnd is incremented by one. Continues until cwnd reaches ss_thresh or a packet gets lost. 2. Congestion Avoidance If congestions occurs ss_thresh is set to 50% of the current cwnd an the new cwnd is set to one. Then the slow start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "2 Phases: Slow Start, Congestion Avoidance At first the cwnd size is set to 1 segment. After each acknowledged segment it will be doubled until it reaches a certain ss_thresh. (Slow Start)\n\t If the threshold is reached the cwnd will only increase linearly by 1 segment until a timout occurs. (Congestion Avoidance) The timeout causes the cwnd to reset to one, the ss_thresh to become cwnd / 2 and to enter the slow-start phase again.",
        "answer_feedback": "The Slow start phase's explanation is partially correct as it does not mention what happens when a packet is lost before ss_thresh is reached. Here the slow start threshold also becomes half of the congestion window, and the congestion window becomes 1. The explanation of the congestion avoidance phase is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "phase 1: Slow start\n\t \nthe cwnd starts with one and increases (with factor 2^x until a specific value then it grows linearly with +1) until the threshold is reached or packet loss occurred\n\n\nphase 2: congestion avoidance\nif cwnd >= ss_thresh or package loss occurred, the ss_thresh is set to 50% of the cwnd value.\nThe cwnd is reset to 1 and increased again until ss_thresh is reached or package loss occurs (new slow start)",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. Also, the congestion avoidance description does not state how the cwnd increases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Initial value of cwnd is one and increases by one in phase 1 slow start, when a segment is acknowledged. This phase continues until value of cwnd reaches ss_thresh which equals to advertised window size or data packet loss occurs.\nCwnd increases by only one in each roundtrip time in phase 2 congestion avoidance until congestion occurs. When congestion occurs, ss_thresh is set to 50% of the current cwnd and then cwnd is reset to one and slow-start is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "There are two phases: 1) slow start and 2) congestion avoidance.\nDuring the slow start (cwnd <= ss_thresh) each ACK generates two packets, hence the data rate grows exponentially. In the meantime cwnd grows for each ACK and when cwnd > ss_thresh, we enter congestion avoidance phase.\nDuring congestion avoidance phase, the data rate doesn't grow exponentially anymore and each ACK only generate one new packet. cwnd only grows until we get some timeout/congestion.\nWhenever that happens, ss_thresh is set to half of cwnd and cwnd gets resetted (ss_thresh = 0.5*cwnd; cwnd = 1).",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The 2 phases of congestion control with TCP:\n1)\u00a0Slow start\u00a0\n2)\u00a0Congestion Avoidance\n\nAt 1): The Congestion Window (cwnd)\u00a0becomes incremented each time a segment is acknowledged until a package is lost or\u00a0ss_thresh reached.\u00a0When cwnd >= ss_thresh, TCP slows down the increasing of cwnd by adjusting tge transmission rate .\u00a0\nAt 2): Each time a congestion occurs ss_thresh is\u00a0 set to half of the size of cwnd (ss_thresh = cwnd / 2) and cwnd is set to 1. Then the slow start phase begins again.",
        "answer_feedback": "\" When cwnd >= ss_thresh, TCP slows down the increasing of cwnd by adjusting tge transmission rate . \" should be in phase 2. Also congestion canoccur in phase 1 and change in the cwnd needs to be mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase1 Slow start:\nEvery received Ack increases the cwnd by one until the threshhold of ss_thresh.\nPhase 2 Congestion control:\nWhile avoiding congestion, every RTT increases the cwnd by one. When a congestion occurs the ss_thresh is set to half the current window size. (cwnd) and cwnd are set to 1 and we move back to phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow Start\nPhase 2: Congestion Avoidance\n\nDuring phase 1 each acknowledged segment increases cwnd by 1, which ends up doubling it every round trip.\nDuring phase 1 ss_thresh doesn't change.\nAfter cwnd reaches ss_thresh phase 2 begins, in which cwnd is increased by 1 every round trip until a timeout occurs.\nWhen a timeout occurs ss_thresh is set to cwnd/2 and cwnd is set to 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are\nSlow start (getting to equilibrium) andCongestion Avoidance.In the first phase cwnd is increased each time a packet has been acknowledged until ss_thresh is reached or a package is lost. When ss_thresh is reached, the increasing of cwnd (until now doubled each RTT) is slowed down.If an timeout occures in the second phase, ss_threshold is set to cwnd/2, cwnd is reset to 1 and slow start procedure is executed again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1 is Slow Start. Here the sender tries to send from a small amount of packets to as much packets as possible. For each send the sender increase the amount of packets exponential until the receiver no longer acknowledge the packages. After that the\u00a0Phase 2 is Congestion Avoidance starts where the sender increase the amount of packets linear instead of exponential. This happens until the receiver no longer acknowdledge and then the Slow Start Threshold is set to 1/2 of the Congestion Window size.",
        "answer_feedback": "Phase 2 starts when cwnd>=ss_thresh, not when congestion occurs.\u00a0\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "First Pahse is the slow start phase. Second phase is the congestion avoidance.\nFirst Phase:\nFor every acknowledged packet the cwnd increases by the Maximum Segment Size\nss_thresh is not changing in this phase\nSecond Phase\ncwnd increases with each acknowledgment received linearly. 1/cwnd is the increase\nss_thresh is set to half the congestion window size on timeout",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached(ss_thresh is set to half the congestion window).",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phases of congestion control: \n- 1. Phase: Slow Start\n- 2. Phase: Congestion Avoidance\nFirst, in the slow start phase, where the cwnd is smaller than the ss_tresh, the cwnd is initialized with cwnd = 1. Now the sender can send one segment in the initial set-up. For each acknowledgment, the sender receives the cwnd increases by 1, which means that cwnd doubles his size every RTT. When the cwnd is greater or equal the ss_thresh (cwnd >= ss_thresh) the second phase congestion avoidance begins where the whole cwnd increase by 1 every RTT. ss_thresh doesn't change until congestion occurs.\nEach time congestion occurs the ss_tresh is set 50% of the current size, the congestion wind is set to cwnd = 1, and slow-start phase begins again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1. slow start: cwnd starts smaller then ss_thresh (cwnd < ss_thresh). cwnd gets increased incrementally until it reaches ss_thresh.\u00a0\n2. congestion avoidance (when congestions occurs: decrease ss_thresh = cwnd/2 and set cwnd = 1)",
        "answer_feedback": "In the slow start phase, the case when the packet is lost before the threshold is reached is not covered.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Congestion control consists of the slow start phase, where cwnd<ss_thresh applies and cwnd is increased exponentially until packet loss or reaching ss_thresh, and the congestion avoidance phase with cwnd >=ss_thresh when ss_thresh is set to cwnd/2 and cwnd to 1 after every congestion and the slow start phase restarts.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The name of first phase in slow start(getting to equilibrium) and the name of the second phase is congestion avoidance. For slow start, ss_thresh is a constant value and each time a segment is acknowledged, we increment cwnd by one everytime until it reaches the threshold ss_thresh(cwnd >= ss_thresh) for which it slows down the increase of cwnd. For congestion avoidance, ss_thresh is set to 50% of the size of congestion window(ss_thresh = cwnd/2) and cwnd is set to 1(cwnd = 1)",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\nPhase 2: Congestion Avoidance\nAfter initialization of cwnd and ss_thresh, cwnd is continuously incremented, increasing the sending rate, as long as segments are acknowledged. This happens until cwnd reaches ss_thresh upon which TCP slows down the increase of cwnd. Every time congestion occurs, ss_thres is set to cwnd / 2 and cwnd reset to 1.",
        "answer_feedback": "The response needs to be specific about the rate in both phases.i.e exponential increase in phase 1 and linear increase in phase 2.\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "In the first phase the cwnd starts with very small packet sizes and starts doubling the package size after each acknoledged transmission.\nAfter reaching the treshhold value of package size, the second phase - congestion avoidance is active.\nNow the package size only increases linear. If a transmission fails / times out, the new slow start threshold is set to the half of the last succesfull package size.\nThe first phase is now active again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "first, we set our ss_thresh to a number. Then we are sending one packet until its ackknowledged. After the packet is ackknowledge we increase the cwnd by one. We repeat those steps until we reached our ss_thresh. After we reached ss_tresh, TCP slows down increasing cwnd.",
        "answer_feedback": "Phases Name not mentioned. What happens when congestion occur in either of the phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1. Slow start (getting to equilibrium)\nEach time a segement is acknowledged increment cwnd by one.\nContinue until reach ss_thresh or packet loss.\n2.Congestion avoidance:\nIf ss_thresh is set to 50% of the current size of the congestion window (ss_thresh = cwnd/ 2),\ncwnd is reset to 1 and slow start is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow start: cwnd starts at 1\u00a0and doubles after every roundtrip\u00a0till ss_thresh or packet loss is reached.\nCongestion Avoidance: After this Congestion Avoidance is reached and cwnd grows by 1 every round trip till congestion occurs. Than it gets reset to 1 and\u00a0ss_thresh = cwnd /2. At this point slow start is used again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> C -> A\nB is the least likely, as there is only 1 combination of all 2^6 combinations possible which is exactly this combination.\nC is more likely than B as there are more than 1 combination possible (e.g. HHHTTT, TTTHHH) (10 total).\nA is more likely than C as all combinations that are valid under C are also valid under A plus for example the combination (HHHHTT) which makes it a strict supergroup to the group of combinations valid under C.",
        "answer_feedback": "The response correctly states the sequence of the three given events. C's justification is partially correct as the total number of possible combinations for C is 20, not 10. The justification provided for A is also partially correct as C is a more strict subgroup than A.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "least probable: B (likelihood = 0.6^3 * 0.4^3)\nsecond least: C ( we don't care about order, so there are 6!/ (3!*3!) possibilities)\nmost probably: A since it includes C but also the probability for 4, 5 or 6 H's",
        "answer_feedback": "The response is partially correct because the event C also includes the event P(B). Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Previous defninition: \" ** \" represents power calculation, e.g. 2**2 == 4. (n\u00a6k) means that pick up k elements from n elements, (n\u00a6k) here is a symbol of combination.\n\nEvent A: the converse-negative proposition(CGP) of event A is \"you see at most two H's\". With 1 subtractes the probility of CGP, we are able to get the result of event A.\nCalculation of Event A: 1 - [ (6\u00a60) * (0.6 ** 0) * (0.4 ** 6) + (6\u00a61) * (0.6 ** 1) * (0.4 ** 5) + (6\u00a62) * (0.6 ** 2) * (0.4 ** 4)] = 0.7968\n\nEvent B: since the proposition is \"you see the sequence HHHTTT\", there is only one solution out of all possibilities.\nCalculation of Event B: (1 / 6!) * (0.6 ** 3) * (0.4 ** 3) = 0.0000192\n\nEvent C: you see exactly three H's, the position of each H needs to be taken into account.\nCalculation of Event C: (6\u00a63) * (0.6 ** 3) * (3\u00a63) * (0.4 ** 3) = 0.27468\n\nthe likelihood in increasing order, i.e. least probable \u2192 most probable is\nEvent B \u2192 Event C \u2192 Event A",
        "answer_feedback": "The response is partially correct because it contains the correct order of events, but the calculation part for all events is incorrect.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "p(B) less than p(C) and p(C) less than p(A)\n\np(A) must be smaller then p(C) because cases whith more then 3 H's are invalid. p(C) must be smaller then p(B) because the H's and T's must be sorted.",
        "answer_feedback": "The response correctly states the sequence of the three given events. However, the justification states the opposite which is incorrect.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service (=Frames are transmitted as independent units --> Data can be lost)\nConfirmed Connectionless Service (=Frames still transmitted independently but with acknowledgement)\nConnection oriented Service (=3 Phases: Connection --> Data Transfer --> Disconnection)",
        "answer_feedback": "The response answers the services' names and differences correctly.  But there is no common theme of the differences between them. The last point should also discuss the presence or absence of acknowledgment.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with this system was the fairness. Some nodes had better chanes to reserve bandwith for themselves than other nodes.",
        "answer_feedback": "The response is partially correct as it states the issue in DQDB but lacks an explanation of why some nodes have better chances of reservation rights. The possibility to reserve bandwidth depends on the distance between a station and the Frame Generator or the Slave Frame Generator.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "1. TCP has a flag for indicating that finished\n2. UDP has no sequence number\n3. UDP has no acknowledgement number\n4. TCP has a flag, if the data is urgent",
        "answer_feedback": "There are different types of flags available in the TCP header, but they are all within the flag header field. Therefore, points 1 and 4 are similar and count as one.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "1. TCP header size is 20 bytes, but UDP Header size is 8 bytes.\n2. TCP does Flow Control, but UDP does not have an option for flow control.\n3. TCP does error checking and error recovery, but UDP does error checking but simply discards erroneous packets.\n4. TCP has sequence number field, but UDP does not.",
        "answer_feedback": "The first point is partially correct as the TCP header length varies from 20 to 60 bytes and does not have a fixed size. The second and third points are incorrect as there is no context provided for the header field related to them. The fourth point is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.38
    },
    {
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "Sequence number, Acknowledged number, Urgent pointer, Advertised Window",
        "answer_feedback": "The response is partially correct as it does not state whether these fields exist in UDP or TCP.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption mostly does not hold for real internet traffic as packets are not sent independently and in a steady way. Rather, internet traffic is very bursty. Data and messages are not sent via individual and independent packets so a single packet most likely is part of a larger message with which it is sent together.",
        "answer_feedback": "It is correct that the assumption does not hold because of bursty traffic. However, the explanation for why the traffic is bursty only refers to packet fragmentation which is only a small reason for bursty traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption is not realistic for real internet traffic. It is different between day and night. For example in video traffic, a window buffer may be on to fetch the next segments of video, then off to deplete it, and then on again to fetch the next further segments of video.",
        "answer_feedback": "One can use a function instead of a constant to model the arrival rate to reflect such large-scale, day-night behavioral patterns. The arrivals would not depend on previous arrivals then, only on the time of the day, which is known. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No the assumptions does not hold, since the arrivals depend on each other. For example one request to a server leads to multiple packets being sent back. Also the answer of the server then might lead to multiple requests afterwards. Thus the packets come in stacks with time in between, since the server and the client need to process the packets.",
        "answer_feedback": "The response is correct except that it attributes the bursty nature of the internet traffic to the request/response model which is not always the case.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "After all, this is not less and not more than a simple model. The details of internet traffic including bursty traffic situations, streams like video (e.g. YouTube, Netflix) or audio (e.g. internet telephony) or the traffic depending on daytime or season cannot be modelled accurately using a Poisson process. There is some derivatives of the Poisson process to better model the bustiness, e.g. the compound Poisson process.\nHowever, the Poisson process has some nice mathematical features allowing a simpler math and therefore allowing to analytically describe queueing systems. Therefore it is widely used for a first analysis or for more mathematical modelling. If a system needs to be analyzed in more detail, simulations tools (e.g. Opnet) can be used. This does not lead to a mathematical formula but delivers more precise numerical results and enables the modelling of a wide variety of data source models.",
        "answer_feedback": "The question asks whether it is true that the arrivals at a node depend on previous arrivals for real internet traffic instead of whether the Poisson process is a realistic model. The reasoning is correct for both questions, but the definitive answer to arrival independence is missing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, because sometimes many users want to access the server at the same time, while at other times, only few request the server. For example a livestream of a football match: everybody sends requests to the server at kickoff, but only few do after the game (to watch the highlights). That means that the arrivals are not independent. They can depend on other events.",
        "answer_feedback": "While the response explains the event point of view, internet traffic is generally bursty, independently of specific events. This makes the packet arrival at a node depend on the previous arrivals.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table bridge holds information of the following: table: station \u2192 LAN. This means it stores information on how to reach a certain source address and over which LAN it can be reached. This table is updated with backward learning. The bridge works in promiscuous mode and receives any frame on any of its LANs. The bridge receives frames with source address Q on LAN L. This means Q can be reached over L. This information is then stored in the table. In the forwarding process this can be used to forward it to the next bridge which then can forward it according to its own table. The benefit of that is that an urgent packet can be directly forwarded without any routing required.s",
        "answer_feedback": "The response incorrectly explains how this information is used in the selective forwarding and the stated benefit is also incorrect. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds information about which device/system can be reached on which LAN. The table initially is empty and during backwards learning, the bridge monitors each frame, checks its source address and creates a new entry in the table with the source address and the LAN from which the frame arrived (if necessary).  When receiving a frame, the bridge checks its table to determine if the receiver is on the same LAN as the sender. If that is the case, the frame is dropped by the bridge, because bridging in not needed. If the receiver is on a different LAN, the bridge forwards the frame to the correct LAN, if the bridge doesnt know which LAN the receiver is on, it tries to find that out by using FLOODING.",
        "answer_feedback": "The response does not mention the benefit of using the bridge table in selective forwarding. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Initially, the bridge table, containing fields for the sender, the receive timestamp and the used LAN, is empty and as a result the bridge will simply use flooding to reach an unknown destination. During the backwards learning phase the bridge works in promiscuous mode - receives all frames from all its LANs - and uses the information of those frames in order to build its table. The information from the table then allows the bridge to forward incoming packets to the correct LAN in order for it to reach its destination. The benefit of this method is that the bridge learns and adapts over time and does not have to rely on flooding, therefore optimizing bandwidth usage.",
        "answer_feedback": "The response does not state which information is learned from the received packet and how it is used while selectively forwarding packets. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "A bridge connects some different LANs the bridge table contains the information which LAN needs to be accessed to reach a certain destination address. At the beginning the bridge does not know the topology and uses flooding to forward packets to the right destination. Once a correct route is found a new table entry with this new information is added to the table. This process of slowly getting to know the topology is called backwards learning. When a packet arrives at the bridge and its destination address is already in the bridge table there is no need for flooding the packet can be forwarded directly according to the table entry.",
        "answer_feedback": "The response does not mention what is learned and interpreted on receiving a packet from source S over link L, i.e. S can be reached over L. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "-The table holds the next hop, i.e. the next LAN (output line), for each station. Is the station not known or the table is initialized flooding will be used.\n-\tBackward learning: Bridge can learn about the other LANs with every received frame from each LAN it is connected to.\n-\tThe decision procedure is as follows: If the \n      o\tsource and destination LANs are identical, the frame is dropped, if the\n      o\tsource and destination LANs differ, the frame is rerouted to destination LAN and if the\n      o\tdestination is unknown, flooding is applied.\n-\tBenefit: The network is transparent and adaptive.",
        "answer_feedback": "The response correctly states what information the bridge table contains and how the selective forwarding uses this information. In the backward learning process, the bridge learns about the mapping between outgoing LANs and stations, not just about connected LANs. The stated benefit is not related to the selective forwarding process.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds the information to what LAN/other bridge a package should be send to in order to reach its destination. \n\nIf a frame is received and\n- its destination lies in, or is routed over the over the same LAN from where it is received it is dropped.\n- the destination is unknown the network is flooded\n- otherwise the package is rerouted to the next LAN according to the table.\n\nThis makes the Bridge self sufficient and other components of the network does not need to know about the bridge. \n\nWhenever a frame is received the Bridge knows that the source can be reached over the incoming LAN and creates a table entry or updates its table accordingly. \nTo update for changes in the topology a timestamp is used for each entry and refreshed for each corresponding received frame. If a timestamp gets to old it is assumed that the entry is valid anymore and flooding is used the next time a frame with a corresponding destination is received.",
        "answer_feedback": "The response does not mention the benefit of using the bridge table in forwarding packets, i.e. fewer duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table assigns each adress the bridge knows about to a network. When the bridge is asked to forward a packet to a receiver, it looks at the table to see to which network it has to send the packet. That way packets that have the same source and destination network can be ignored by the bridge and packets with a different source network can be sent on the direct path to that network. Initially the table is empty, but the bridges listens to all packets of the networks it is connected to and  whenever a packet is sent, the bridges wirtes an entry into its forwarding table containing the sender adress and the its network. If the bridge is asked to send a packet to an adress it has no entry for, it floods the packets in all other connected networks.",
        "answer_feedback": "The response does not mention the benefit. Additionally, the table contains station, outgoing LAN, and the timestamp which is not clear from the first line. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Transparent bridges maintain a table that maps each station to an output line or LAN the bridge is connected to. Initially the table is empty, the bridge reads packets from all its output lines and learns where the stations are located. This phase is called the backwards learning phase. During the forwarding process, the bridge uses the table to make the following decisions. If the source and destination are known and are from the same LAN, the packet is dropped. If the source and destination are known and are from different LANs, the packet is forwarded to the correct output line. If the destination is not known, then the packet is flooded on all its lines. Benefits of such a forwarding process is, only frames that need to cross the bridge are forwarded. They also reduce collisions by creating a separate collision domain on each line of the bridge.",
        "answer_feedback": "By observing packets, the bridge learns through which LAN a station can be reached, not where the station is located. This information is used in selective forwarding. Apart from this, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table possesses information about the stations and in which  LAN they are located.The bridge works in promiscuous mode, that means that it receives any frame on any of its LANs and with the information stored on these frames of how the stations can be reached, it will create a table entry. In the forwarding process if the source and destination LANs differ the frame will be rerouted to the destination LAN (the information of where this destination is found is on the table).",
        "answer_feedback": "The response does not mention that if a packet arrived over link L from source S, it would use the same link L to forward packets destined for S. The response does not state the benefit of using the bridge table information. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table contains information, which source addresses can be reached via which output line. \nWhenever a bridge receives a frame from one of its LANs, it extracts the source address and updates its entry for this address with the corresponding LAN and time stamp. \nAfter some amount of time all entries, which have not been renewed are purged to prevent outdated information from being used. \n\nKnowing via which output line to forward a frame, the bridge does not have to resort to flooding thus reducing the total network load. \n\nAn advantage of transparent bridges is that the bridge is invisible to the network which simplifies other components.",
        "answer_feedback": "The response does not mention how a packet from source S received over LAN L can be interpreted as \"destination S can be reached over L\" which forms the base for backward learning. Also, a benefit of the transparent bridges in general is stated which was not required. Apart from that, The response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The forwarding table maps stations to LANs and has a timestamp for each of its entries.\nWhen the bridge receives a frame with the source address Q on LAN L in the backward learning phase, it adds a table entry Q can be reached over L. If this entry already exists, the timestamp is updated, the timestamps are used to regularly delete old entries.\nWhen the bridge receives a frame in the forwarding process, it looks up the LAN of the destination. If the source LAN and the destination LAN differ it reroutes the frame to the destination LAN. If the destination address is not available in the forwarding table the packet is flooded.\nThe benefit of this is that the bridge is not visible to other components in the network, this simplifies the other components.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Additionally, there is one more condition during forwarding where the packet will be dropped when the source LAN = destination LAN. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table of transparent bridges holds the frame with address on every connected LAN and the frame arrival timestamp, including sender and receiver.\nDuring the backwards learning phase because of learning in the promiscuous mode the bridge can know any frame from any connected LAN and can trace back to the source address on the LAN of the received frame to create a table entry in the bridge table. In the forwarding process if the source and destination LANs are identical, the frame will be dropped, otherwise(if they are different), the frame will be forwarded. If the destination LAN is unknown, the frame will be flooded. \nThe benefit is forwarding frames without considering types of LANs and without changing the configuration tables to achieve transparency.",
        "answer_feedback": "The bridge table does not contain both sender and receiver information; it only maps the destination station to the incoming LAN. The stated benefit is not correct as the response mentions the benefit of transparent bridges in general, not of using transparent bridge table information in forwarding. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Network Topology and Access Points- Network topology and Access Points changes faster when the devices are moving around which is not the case in fixed network. Security - As the network could be available even inside and outside the building, it possesses a threat to security. User Mobility: The users can be moving from one network to another and want to communicate anytime with anyone without losing the network.  Device Portability: The device can change in a period of time but having the portability with the other device to connect anytime and anywhere to the network.",
        "answer_feedback": "The response is partially correct because the device portability is not an actual challenge beyond what is already stated in the previous challenges.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminal: If multiple nodes are hidden from each other, the transmission to a common node of them results in a collision at the common receiver. Two nodes are hidden from each other, when they cannot sense each other (distance > detection range). Near and Far Terminals: Stronger signals drown weaker signals. That means that the distance of the nodes can influence the communication behavior because the signal strength depends on the distance to the sender.",
        "answer_feedback": "The response correctly states and describes the hidden terminal problem. The near and far terminal challenge does not specify the relation between distance and signal strength.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "ONE CHALLENGE IS THE USER MOBILITY. IN CONTRAST TO FIXED AND WIRED NETWORKS, USERS CAN MOVE AROUND WHILE COMMUNICATING WIRELESSLY ANYTIME, ANYWHERE AND WITH ANYONE. THIS BRINGS ADDITIONAL COMPLEXITY TO ROUTING MECHANISMS SINCE THEY HAVE TO TAKE INTO ACCOUNT THAT OPTIMAL PATHS MAY CHANGE FROM TIME TO TIME AND HAVE TO BE RECALCULATED. Another challenge is the device portability which is a requirement for the ability of devices to connect anytime and anywhere to the network. Properties like power consumption or space play a more significant role when compared to fixed and wired networks.",
        "answer_feedback": "The response is partially correct because portability is not a challenge itself. However, the description mentions the power consumption of mobile devices which is an important challenge in mobile routing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Challenges: Hidden Terminals, Exposed Terminals, Near and Far Terminals Hidden Terminals:  Each terminal has its own maximal detection or transmission range. Nodes A and C cannot hear each other, and the transmissions of nodes A and C may collide at node B, then nodes A and C are hidden from each other. Hidden terminals can cause more collisions, waste of resources, etc. Exposed Terminals: Assumed terminal B is able to hear A and C, C can hear B and D. If B sent to A, C wants to send to D is not A or B. At this time C must wait, indicating that a medium is being used, but A is outside the radio range of C, then C is \"exposed\" to B. There is a problem of underutilization of channel.",
        "answer_feedback": "The exposed terminal description part is partially correct because the reason behind the busy medium is incorrect. C wants to send data to D but senses the medium busy and waits. The wait is unnecessary because D is outside the range of B. Therefore, C and B are exposed to each other.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "The challenges are: * Security:the packet of data is not much secure.Through neighbor authentication, a user can know it neighboring users.For security of packet we must have to use technique of data encryption. * QoS * Scalability * Heterogeneity * Adaptation:Network has to adapt to Dynamic positioning of nodes.This is necessary and nodes may join the network or may leave the network dynamically. * Dependability\t-",
        "answer_feedback": "The response names 5 requirements but describes only two. Further, the description of security is partially correct as it does not clarify how security is a challenge in a wireless network as encryption is common in both wired and wireless communication.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. Hidden Terminals  Exposed Terminals\uff0c Near and Far Terminals 2.Hidden Terminals\uff1a There are maybe 3 nodes. Because of the maximal range. Nodes A and C cannot  hear each other. They can only hear B. The transmissions of A and C may collide at B. So they are hidden from each other. This is a waste of resources and it may lead more collisions.\n\t Exposed Terminals:  There are 4 nodes. B can hear A and C. C can hear B and D. When B sends to  A and C sends to D, C must wait because there is signal that a medium is being used. But because A is out of the range of C . So this wait is unnecessary. C  is exposed to B . This leads to the problem of underutilization of channel and lower effective throughput.",
        "answer_feedback": "C does not need a signal that the medium is busy because it can use its carrier sense to detect B's transmission.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "One challenge is the hidden terminal problem. CSMA doesn't work in mobile networks because a node cant see the whole transmission medium. They can't detect if there is some other node sending,to the same destination, at the moment so the send to and cause a collision. An other challenge is the exposed terminal problem. It can happen that a node tells all other nodes in range not to send, to avoid collisions at their receiver. But some nodes then need to be silent even though they aren't even in range with the receiver of the other transmission and therefore can't create a collision. So the wait unnecessary and Utilization is lower than it could be.",
        "answer_feedback": "The description of the exposed terminal problem is partially correct. It states that a node tells all other nodes not to send. But instead, the node wanting to send senses the medium is busy and waits until it is free again.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "When we have mobile routing some problems begin to appear. One of them is called \u201chidden terminals\u201d and this is caused because the nodes are not within each others transmission rate, causing that they are invisible to each other and they have to communicate through a third node ,which is within range of BOTH of this nodes, the issue with this case is that because they can\u00b4t know at first if the other node is sending something, collisions may occur. Another challenge is the near and far terminals , in which if two nodes are sending signals at the same time, the stronger signal will drown out the weaker signal, making the receiver not being able to receiver the weaker signal.",
        "answer_feedback": "The response states the hidden terminal challenge correctly except that nodes communicate 'to' the common node, not 'through' the common node. The near and far terminal challenge description is incomplete because it does not mention the signal's relation with increasing distance.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. Hidden terminals: For given nodes A, B and C, nodes A and C cannot hear each other if their transmissions collide at node B. In this way, nodes A and C remain hidden from each other. 2. Exposed terminals: For given nodes A, B, C and D, B sends to A and C wants to send to another terminal like D, but not A or B. C has to wait and signals a medium in use. But A is outside the radio range of C, therefore waiting is not necessary. In this way, C is now \"exposed\" to B.",
        "answer_feedback": "The response correctly states and describes the exposed terminal problem. But in the hidden terminal problem, not only the collision but also the transmission of the other sender remains undetected.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Revers Path Broadcast are used to forward multicast packages without using loops (and therefore not creating duplicates on the way). In Reverse Path Forwarding every node has its own spanning tree. When a package is received by an intermediate station, the station checks if it would send packages to the sender over the used link. If that is the case, then it should be the best route and the intermediate station forwars the package to all connected nodes (except the incoming edge). If it is not the case, then it gets discarded, because it is very likely a duplicate, from another node. Reverse Path Broadcast is an improvement on reverse path forwarding. Instead of forwarding packages to every node (if conditions for forwarding are met) it only forwars it to the node, from where packages would normally arrive from.",
        "answer_feedback": "The purpose of Reverse Path Forwarding and Reverse path Broadcast is not limited to the multicast but also used in broadcast. In Reverse Path Forwarding, only the sender needs to know the spanning tree, and it makes use of unicast information to forward the broadcast package. The explanation of RPB is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose is to reduce packet duplicates due to flooding. * REVERSE PATH FORWARDING: The receiving router checks, if the incoming packet used the usual (proboably best) path to it. If it is the case, the packet will be forwarded to all links except the one at which the packet arrived. If a packet arrives at an unusual path (probably not the best path and a duplicate), the incoming packet will be ignored (discarded) and not forwarded any farther. * REVERSE PATH BROADCAST: Similiar to _Reverse Path Forwarding except _that a node gains information about the best paths between neighbouring nodes due to observation. That is, a packet will not be forwarded via every link but only via those links, which are unlikely to create more duplicates. For example a node A will not forward a packet to X coming from a node Z, if it knows (due to obeservation), that there exists a best path between X and Z for that packet.",
        "answer_feedback": "The response identifies the purpose of RPF and RPB correctly. The RPF explanation is partially complete because it is unclear what the \"usual path\" means and how it is determined. If a node X receives a broadcast packet from Source S, node X checks its routing table to see if it would have used the same route to send a unicast packet to S, if yes the incoming packet followed the best route. The RPB explanation is also not complete as it also does not explain how nodes learn the best path between nodes, namely through the unicast routing algorithm (e.g. link state) or observing previous unicast traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose of Reverse Path Forwarding and Reverse Path Broadcast is to send (loop-free) multicast packets in multicast routing. Reverse Path Forwarding is more reliable (it can compensate if one router of the network has an error). Reverse Path Broadcast is more efficient, it relieves links, which are not the best path and therefore not necessary. Reverse Path Forwarding: If the router gets a packet, it looks where the packet comes from. If it is from a link, which its (unicast) routing table would also suggest to send via this link, this link is the best path and the router distributes the packet. Otherwise the packet is discarded. Reverse Path Broadcast: If the router B gets a packet, it also looks in the routing table, if the packet comes from the best path. If it is the best path (from router A), the router looks in the routing table whether a packet is ever send from C to A. If this is not the case, the link to C is not the best path and B doesn\u2019t send it to C. So not necessary links are relieved.",
        "answer_feedback": "The response is partially correct because both RPF and RPB explanations didn't clearly explain how the packets are forwarded in a network. Additionally, the IS would also look whether packets are sent from A to C in Reverse Path Broadcast. The algorithms avoid loops not only in multicast but also in broadcast.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Reverse Path Broadcast are used for Broadcast Routing (a Sender send a message to all Recievers(1:all communication)). The sender has its own Spanning tree to calculate the routes. The receivers however do not. So they have to deciede how to handle the received packets. So they check if the received packet was received through the edge, which is usually used by packets from the sender. If this is not the case, the packet will be discarded. However if it is the case, in Reverse Path Forwarding, the packet will be resend over all edges.  With Broadcast Routing on the other hand the receiver checks if the received packet used the best route until this point and only forwards it over those edges which belong to the best routes,",
        "answer_feedback": "The response correctly explains RPF and RPB. However, the response lacks the purpose which is to minimize the number of duplicate packets during broadcasting. In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.7
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Using Reverse Path forwarding ensures loop-free forwarding of multicast packets. The idea behind this algorithm is: If a packet station X arrives at an IS over an entry point over which the packets for station X are usually sent, this might probably be the correct and fastest route. Therefore only if this is the case packets distributed over all edges. If the packet is received over another entry point it will be discarded. Reverse Path Broadcast is a refined version of this algorithm. It differs from Reverse Path Forwarding by the fact, that if the packets have taken the best route until their arrival at a certain node they will be forwarded to the best next edge taken from the routing table. If not, they are NOT sent over all edges. This is achieved by the knowledge, which other nodes usually receive their unicast packets via this note.",
        "answer_feedback": "The answer is partially correct as the purpose of RPB is not explicitly mentioned. In RPF, the optimal/best route may or may not be the \"fastest\".  In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding :  purpose:Variation of the Spanning Tree,Each sender has its own Spanning Tree,but IS do not need to know the Spanning Trees Algorithm:Has this packet arrived at the IS entry port over which the packets for this station/source are usually also sent? Yes: -Assumption: Packet used the BEST route until now   Action: resend over all edges (not including the incoming one) No: -Assumption: Packet did NOT use this route (it is NOT the best route)     Action: discard packet (most likely duplicate)   Reverse Path Broadcast: purpose:Has packet arrived at the IS entry port over which the packets for this station/source are usually also sent? Yes: -Packet used the BEST route until now -resend over ALL edges(not including the incoming one) No: Discard Packet did NOT use this route (it is NOT the best route)   Algorithm: packet from S(ource) to D(estination) Like REVERSE PATH FORWARDING with specific selection of the outgoing links Has this packet arrived at THE IS entry over which the packets for this station/source S are usually also sent? YES: Packet used the BEST route until now?      YES: select the edge at which the packets arrived and from which they                 are then rerouted to source S (in reversed direction)       NO:  DO NOT send over all edges (without the incoming one), i.e., not as              in Reverse Path Forwarding (RPF) NO: discard packet",
        "answer_feedback": "The response correctly explains  RPF and RPB but the stated purpose is incorrect. It should be to reduce the number of duplicates and unnecessary packets in flooding/broadcasting by inspecting the optimal unicast paths.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "All intermediate stations periodically have to send link state packets via broadcast to all others. The link state packets contain the distance to the neighbors and additional information about the multicast group.  Each IS then calculates a multicast tree on the locally available data and determines the lines on which packets must be sent.",
        "answer_feedback": "The response does not mention the spanning-tree property that makes it appealing for broadcast and multicast. The explanation for modifying the link-state algorithm to construct a  multicast spanning tree for nodes is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees are appealing to broad- and multicasting scenarios, because they allow the packets to only travel one path (except travling backwards). This removes the need for looking up specific tables as in RPF / RPB.",
        "answer_feedback": "It is true that there is a unique path between nodes but that not only does away with the need to look at routing tables in RPF/RPB but reduces duplicates by removing loops(unnecessary links). No explanation was provided for modifying the link-state algorithm to construct a  multicast spanning tree for nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "-The property of spanning trees, that it is a subset of subnets including all routers with no loops, makes them appealing for broad- and multicasting.   -If link state routing is used and each IS/router knows the complete topology, including which hosts belong to which groups,then the spanning tree can be pruned  from bottom of each path to root, all routers are removed that do not belong to the group.",
        "answer_feedback": "The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast. The second part does not answer how the link-state is modified. It assumes hosts have already discovered which nodes belong to which group, which is not correct. In the link-state additional multicast-group information is added and send to all other nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees includes all routers with no circles, so the packets are not sended (infinitely) often around.\n\nIf the frequently sended packets of the Link State Routing also contains informations on multicast groups, every IS has enough information to construct a spanning tree for multicasting.",
        "answer_feedback": "The explanation behind using a spanning tree for multicast and broadcast is partially correct because though the network is loop-free, using the tree results in the minimum number of message copies required to be forwarded and not just the prevention of forwarding loops.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "A spanning tree has no loops, includes all routers (of the subnet) and has a root IS (intermediate system). This is appealing for broad- and multicasting, because you only need to send the data to the root IS. From there every node (or a specific set of nodes) can be reached. Link State Routing constructs a spanning tree. For multicast routing, the information, which systems belong to one group, must be provided. Therefore the link state packets are expanded to contain the information on multicast groups. These are then propagated from a predefined root IS to calculate the tree. A spanning tree has no loops. Link State Routing constructs a spanning tree, it needs to know which systems belong to a group.Therefore the link state packets are expanded to contain the information on multicast groups. These are then propagated from a predefined root IS to calculate the tree.",
        "answer_feedback": "The stated property may be correct for specific types of spanning trees but is not the general property of a spanning tree. Not all spanning trees need to have a root node. The reason why a spanning tree is used in multicast and broadcast is the lack of loops, which reduces the number of duplicates needed. The modification description of the link-state algorithm is correct, except that it does not need to be propagated from the root node.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "The property is, if each router knows which of its lines belong to the spanning tree, it can copy an incoming broadcast packet onto all the spanning tree lines except the one it arrived on. There is no loop in a tree. Therefore in order to build a spanning tree by modifying Link State Routing, the loops need to be cut. Assuming that a router is a vertice and when two routers are connected, there is an edge between them. After five steps of LSR, it can be abstracted as a weighted directed graph. Below is the basic idea. Divide the vertices in the graph into two groups, S and U. S contains vertices that has already computed shortest path. U contains vertices that the shortest path is uncertain. Add following steps after regular LSR. a. Originally, S only contains source vertice v, U contains the rest of them. b. Pick up vertice k from U, which has shortest distance from v, put k into S. c. Let k be the new intermediate vertice, changing the distances from k to the rest vertices in U. d. Repeat step a and b until all vertices are in group S",
        "answer_feedback": "The response is not correct about how the link-state algorithm is modified. The link-state packet is expanded to contain multicast group information and exchange it with other nodes to calculate their multicast spanning tree. Dividing the graph into two parts and calculating the shortest distance does not help in sharing the needed multicast group information of each node.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees have all nodes covered with the minimum number of edges, so there can't be any loops. That makes them appealing for Broad- and Multicasting. If we modify Link State Routing so that each IS calculates a multicast tree. All IS send link state packets periodically and from the now locally available and complete state information each IS calculates a multicast tree.",
        "answer_feedback": "The response correctly answers why a spanning-tree usage is ideal in multicast and broadcast. The provided information for modifying link state to construct a multicast spanning group is not complete as it does not state what additional information is added in each link-state packet apart from the regular information.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers describes a form of additional information, which can be added to the packet. If and how many extension headers are added is optional and completely up to the higher layers to decide. In the packet they are located between the header and the data (unit). The main advantage is that you are not forced to put an option-field in the header, like in IPv4, anymore. So that space is not wasted, if the option-field is not used.",
        "answer_feedback": "The response answers the description correctly. The location of the extension headers is not precise. They are located between the fixed header and payload. The stated main advantage is incorrect as the option field in the IPv4 header is already optional, so there is no added advantage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers in IPv6 are a way of enlarge the header in order to put additional information. They are placed between the fixed header and the payload. The main advantage of extension layer compared to IPv4 is that they are optional while in IPv4 the options field is required. Therefore in IPv6 you can add bigger variable length optional information without changing the fixed header. So if you want to change IPv6 you are able to put in information in these extensions.",
        "answer_feedback": "The advantage given in the response is partially correct because the option field in IPv4 is optional as well. It had a variable length of 0-40 bytes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional and located between the header and the payload. \nThey can extend the IPv6 datagram by telling a router that the payload is used for identification, is fragmented or give additional information to hops.\nThe main advantages are more flexibility of messages and a reduced headersize, because they are optional.",
        "answer_feedback": "The response is partially correct because the advantages are incorrect. The header size is not reduced based on the optionality, as the options field in IPv4 could already be 0 bits long. Additionally, the response does not explain what type of flexibility is gained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are headers that can provice additional information for a packet. They are located between the fixed header field and the payload.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The advantage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are the way to put additional information in the packet and are placed between fixed header and payload. The main advantage compared to IPv4 is that they are optional and extensible, so they don't consume additional space and can be modified easily later on (should the specification change).",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, there is no added advantage over the IPv4 option field in terms of space consumption.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers can provide information in addition to the IPv6 header. They are placed between the fixed header and the payload. They are optional, they only have to be transferred if they are actually used. The fixed header remains small.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional and can be 0 bits long, there is no added advantage over the IPv4 option field in terms of unnecessary transfers. The main header remaining smaller is not an advantage in itself.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional fields in IPv6 address, placed between the header and the playload. The main advantage compared to IPv4 is that extension headers allow for extra information to be headed, overcaming the address size limitation",
        "answer_feedback": "The response is partially correct because the advantage part is somewhat ambiguous. Extension headers help to overcome the fixed header size instead of the address size of the IPv4 packet.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension headers are located between the regular Header and the payload, the extension headers are optional and are only used of needed.\nThe main advantage of having extension headers compared to IPv4 is, that they are optional and can reduce traffic if not needed and that they allow to append new options without having to change a fixed header.",
        "answer_feedback": "The response is partially correct because the advantage part is somewhat ambiguous. Extension headers are optional but they are not helpful for reducing traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is used to increase the throughput of data in a network without changing the cables or connection within it.\nThe wait time is used by the clients to burst up a sequence of up to three packets, then they take their waiting period.\nIt should not be used with more than 3 clients because too much data can kill the throughput for the whole network.\n\nadvantage:\nbetter efficiency\n\ndisadvantage:\nneed frames waiting for transmission",
        "answer_feedback": "The response answers the advantage and disadvantage part correctly. However, the definition is incorrect as it contains details, such as a maximal burst of three packets and a recommendation not to use it with more than 3 clients, that do not hold for frame bursting in general.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a communication protocol feature for the principle of shared broadcast mode in gigabit ethernet. \nAdvantage: frame bursting has a better efficiency than carrier extension. \nDisadvantage: needs frames waiting for transmission",
        "answer_feedback": "The response is partially correct as it answers the advantage and disadvantage parts correctly, but the definition of frame bursting is too broad as it does not explain what the feature does. Additionally, frame bursting can be used in other scenarios than gigabit ethernet as well.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a network management protocol that is primarily used to automatically assign an IP address to each device/host on a network so they can communicate with other IP networks or endpoints in the network.",
        "answer_feedback": "The response is partially correct because the DHCP usage part is not specific enough. DHCP is used to assign automatic, manual, and dynamic IP addresses in a network.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a protocol which is used to configure network-data and allocate IP-adressen (manual or automatic) within a network:\nEvery Host can request and process IP-Configurations from a \nDHCP-Server. This simplifies the process of IP adress assignement and extends the functionality of the former used RARP.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly except it does not configure network data.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is the new version of ARP (Address Resolution Protocol), basically DHCP does the same : \nThe DHCP (=Dynamic Host Configuration Protocol) is a communication protocol (for network management) on internet protocols.\nA DHCP server is used for assignment : A DHCP server assigns network configurations and IP addresses to devices on a network in \na dynamic way. As a result, the devices are to work and communicate with other IP networks.\nSo, DHCP simplifies installation and configuration of end systems, it can assign manual and automatic, dynamic IP addresses to devices in a network and can provide network parameter information like netmask, DNS server, default router and so on.\nThe IP address assignment is for limited time available only.",
        "answer_feedback": "The definition part in the response is partially correct because DHCP is not a new version of ARP. DHCP uses ARP requests during the IP allocation process but they are separate protocols.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "Simplifies installation and configuration of end systems , Allows for manual and automatic IP address assignment, May provide additional configuration information(DNS server, netmask, default router, etc.)\nUsed for:  Request can be relayed by DHCP relay agent, if server on other LAN",
        "answer_feedback": "The stated usage is not a usage but the process of how it is carried out.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol  is a network management protocol.\n\nUsed for intranet or network service providers to automatically assign IP addresses to users.\nUsed by the intranet administrator to centrally manage all Computers.",
        "answer_feedback": "The response is partially correct because DHCP also allows the manual and dynamic allocation of IP addresses.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a new version of RARP and BOOTP but more or less does the same thing. Most of the time its a router that can both manually or automatically assign IP addresses (and other information such as DNS Server, netmask, default router etc. ) to clients in its domain. The request from the clients is a broadcast request, the DHCP server then answers. \nA big advantage is the dynamic allocation of IP addresses. Each address is given out with a lease timer. A client has to refresh his lease before the timer runs out. If it does, and the client has not refreshed his lease, that IP address can once again be assigned to a new client. Therefore it allows to reclaim addresses of disappearing hosts.",
        "answer_feedback": "The definition/description is partially correct as it is a separate protocol or a successor protocol with extended functionality and not a new version of RARP and BOOTP.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a newer version of RARP. \nSystems use this protocol to resolve their own IP address in a network from their hardware/MAC address. \nA specific DHCP server assigns the IP addresses and is contacted to resolve them.",
        "answer_feedback": "The response answer is partially correct because the statement regarding the DHCP server is correct. However, DHCP is more a replacement of RARP and not a new version. Also, ARP is used to resolve the IP address from the MAC address.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Frames may contain an implicit ACKs.\nDuplex Operation. \nIt has to have an initial SeqNo. of 0",
        "answer_feedback": "Apart from the correct answer of duplex operation, the response also contains other requirements. The first point is true, but it refers to what happens in piggybacking in general. The last point is incorrect as it is specific to the example given in the slides.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Piggybacking in sliding window allows to send the acknowledgment of a received frame together with data.\n- Sender and receiver needs two sequence counter. One for its own frames and one for the acknowledgment of the received frames.\n- Benefits the most from a duplex connection\n- when a frame is send, the SeqNo of the frame and the SeqNo of the last received frame (for acknowledgment) are send together.\n- The SeqNo is initialized with 0 and is increased before a new frame is sent.",
        "answer_feedback": "The response is correct as it identifies duplex connection as one of the requirements, but the sequence number need not be initialized with 0.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "in the Host block two adresses are reserved:\n- all 1\u00b4s as host acts a a broadcast address\n- all 0\u00b4s as host acts as the network",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 to 10.255.255.255 are reserved",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    }
]
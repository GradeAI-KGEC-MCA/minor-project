[
    {
        "id": "35dc2a7ccb65425a9cb73e7b09ba5a5b",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend using non-persistent CSMA for the following reasons:\n1) The channel is expected to have a high load and non-persistent CSMA ensures a low number of collisions compared to other protocols and works thus better on high loads.\n2) More systems will be added to the LAN in the future so scalability is very important. Non-Persistent CSMA is perfect in this case, because it requires no changes no matter how many systems are added in the channel.\n\nOne Potential weakness of using non-persistent CSMA is that the throughtput will be low if only a few Systems are trying to use the channel. It can happen that multiple stations wait, even when the channel is idle.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "c59c6dc2977340e6802f883475ae4dc0",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Non-persistent CSMA would be recommended in this case for 2 reasons. At first, CSMA avoid collision as much as possible by checking the channel before sending anything. Non-persistent CSMA provides the highest throughput in case the channel load is high. However, it has a potential weakness, the channel will be delayed longer than other MAC procedures. Other MAC procedures are not sufficient in terms of channel load and efficiency.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "a8fbca380920480087cce283c20832af",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "The company should go with a standard CSMA/CD MAC procedure. The main reason for this is that it is possible to add stations easily to an existing network. Furthermore, it is a cost efficient MAC procedure which is a benefit due to the tight funding. A possible weakness is that the througput could be poor due to collisions due to high channel load.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "ba0f888392a24c789d5465d6cf83b921",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I'll choose CSMA/CD, because it costs efficient, it can detect the collision,but it has short frame.",
        "answer_feedback": "Why is the collision detection an advantage? Why is the short frame an disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "id": "fd4c2ed3ef33471aa8623d886f013575",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend the non-persistent CSMA.\nFirst, it has high efficiency and high throughput. From a rate of about 5 attempts per packet time, it beats most of the other systems in terms of throughput.\nSecond, the normalized throughput increases with increasing attempts per packet time. This enables and even supports expandability.\nA potential weakness is the throughput at low attempt rates. In these cases the non-persistent CSMA is inefficient.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "2fdcf31a653f496c8baf88c2fdc31bd4",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Based on the channel utilization slide of the lecture I would recommend the use of p-persistent CSMA (with a low value for p), because it has a high channel utilization that:\n(1) scales well for more systems sharing the channel\n(2) reduces the amount of collisions with the use of fixed time slots and medium sensing before sending (wait until medium is not busy)\n\nOne disadvantage is that it can not terminate the sending when a collision occurs, instead, it waits until a random time interval, senses the channel and retransmit the frame if the medium is not busy.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "bc92956350924f388abf3b9d0e6cff44",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend CSMA/CD.\nThe reason is that CSMA/CD checks the channel before and during sending frames.\nAlso if the sending station detects a collision it immediately interrupts the transmission, which saves time, bandwidth and avoids wasteful transmissions.\nAnd because of the collision detection, its efficiency is better than the \"simple\" CSMAs.\nFurthermore, the advantage in contrast to coordinated access like Token Ring is if more and more systems are added it takes more time until a station that wants to send gets the token.\n\nA weakness of CSMA/CD would be if more systems are added the performance decreases.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "b62d60e79e15403dbe371f9978c63972",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Not sure what is meant with \"... expect the channel load to be high compared to the hardware they can provide.\"\n\nToken Ring: \n+ Good throughput even if the utilization is high and scalable for the future growth of the company.\n- It can come to some delays because of the waiting time for the token.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "631ec5f5b9824a7187ce95c88b0d20c9",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I recommend the token ring, because (1) only one system can send at a time (the one with the token), so the channel load is reduced and (2) the token ring procedure is collision-free and works decentralized. \nPotential weakness: the token can be lost if the system which is currently holding the token unexpectedly disconnects from the network.",
        "answer_feedback": "Actually, the token ring needs a central monitor (See slide 73, LAN slide set).",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "27bd9e4bc7ed4a61855153ba4f048c4d",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Token ring\nAdvantages: still expandable, high usage, due to token ring: sending only possible when sender has ring; prevention of collisions, no random waiting time\nDisadvantage: more expansive than other MAC procedures",
        "answer_feedback": "Why is high usage an advantage?\u00a0Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.79
    },
    {
        "id": "0632012c4c3443dd93391906b49d75ce",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Token Ring,\n+, Good throughput - even during increased utilization\n+, expandable\n\n-, Delays because of waiting for token",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "35f08ec0d5d145279ba820b579de6063",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend to use token ring for the following reasons:\n- As they expect the channel load to be high it is no advised to use CSMA/CD or a variant of it as that will cause a lot of collsions. Whereas token still performs reasonable well during increased utilization\n- The other requirement is that it should support 20 systems and should be expandable later. Token ring can support a maximum of 250 stations and can be extended with coax or optic fiber later on (for increased transmission rate). \n\nOne potential weakness is:\n- As their funding is tight CSMA could still be considered as it is a lot more cost efficient than token ring.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "e4b1b84d9c6c448583f83c76012704a3",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "For this use case I would recommend CSMA/CD as MAC. The reasons for this are 1. that due to the high channel load collisions are expected and terminating the sending after an detected collision minimizes the time the channel is clogged up compared to CSMA without collision detection or even ALOHA. 2. CSMA is easier scalable than contention free access MACs since there is no need to authorize a sender in any form before sending, every sender checks the channel on its own and can send data when the channel is free. A potential weakness could be, depending on the access mode used by CSMA. Since currently the channel load is high compared to the hardware capacities using the non-persistent access mode with its higher efficiency could further strain the hardware. Although this could change in future when the system is expanded.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "8eab99174102430e8b26503d79b31717",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD is recommended because 1) it provides better throughput than other MAC procedures in overall, especially when the number of systems sharing the same channel is expected to increase further. This is important because the channel load is high due to the limited provided hardware. Secondly, 2)  CSMA / CD saves time and bandwidth due to interrupting the transmission when a collision is detected (which is highly probable ). A potential weakness of CSMA \\ CD's ability of collision detection depends on the maximum distance between the stations within the network. If the LAN network expands further there's a risk that CSMA / CD won't be possible anymore.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "e78ffd9918dd45488b2f20a132b3f867",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend non-persistent CSMA. It does not need a precise timer which is great for a tight budget and it has a high efficiency at the cost of longer delays for single stations. It can be expanded pretty easily by just adding new stations.\nALOHA is not a good choice here, because of its low channel usage and CSMA/CD needs a reliable collision checking while 1-persistent CSMA has a low throughput at higher load and p-persistent CSMA needs a precise timer.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "8835128bfe9e4fb3950daf85a4d73da8",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD will be better.\nBecause it's random access procedures and can add new systems more easily than coorinated ones. Using a collision detection will save more time and bandwidth than ALOHA and other CSMA procedure.\nThe weakness is that the station in CSMA/CD procedure cannot send and receive frame at the same time, so  this procedure is only suitable for Half-duplex commnuication.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "b1c313aaa9ec4a51b6e4a38e21516730",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD:\nReasons:\n1. there are 20 systems sharing the channel, so it should be Random Access.\n2. stations know whether the channel is in use or not before trying to use it, so it should be With carrier sense\n\nPotential weakness:\n1. CSMA/CD has no maximum waiting time.",
        "answer_feedback": "Why exactly should it be Random Access, what advantages has it for sharing? The other CSMA procedures are also with carrier sense, why then choose CSMA/CD?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.43
    },
    {
        "id": "911e5332765042adaa0fd1d9923b0463",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Because the network has a perfect clock, we don't need an encoding technique with the self-clocking feature.\nBecause the network is often congested we need a good utilization of the bandwidth (1 bit per Baud).",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "e0170025e4aa4a38966d5752e6dda1b0",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "The network should use binary encoding:\n- Because of the perfect clocks there is no need for a self-clocking encoding\n- Binary encoding has better bandwith utilization than Manchester/differential Manchester encoding which is important because this network is often congested",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "fbbd86bebd8245889a8d554eeb7f20d7",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Recommended encoding technique: Binary Encoding (NRTZ).\nReasons:\n1. Of all the encoding techniques presented in the lecture, binary encoding offers the best use of bandwidth (1 bit per baud versus 0.5 bits per baud for (differential) Manchester encoding) for the scenario described (heavy link utilization).\n2. Since all participants have perfect clocks, there is no risk of clock drift/deviation.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "2375e1c3e99a43d5a1c636b6193d79bb",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding.\n1.All users have perfect clocks, so good \"self-clocking\" feature is not necessary.\n2.It is mentioned that all users generate more traffic than the link\u2019s capacities. But the utilization of the bandwidth of Manchester Encoding or Differential Manchester Encoding is 0.5 bit/Baud, only half of the utilization of Binary encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "2db28f0a9bcc420ab9ec451983558147",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding as it is simple to implement and uses the bandwidth well. It's also easily doable as all parties have a perfect clock and therefore there is no problem receiving and differentiating multiple bits of the same type after another.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "63adee7dffda41398cd1a177984d8aec",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "In this scenario, the simple Binary Encoding technique should be used. That is, because it has the best utilization of the bandwidth among the presented techniques, which is important to use such a congested network as efficiently as possible. Furthermore, the downside of the technique not having a self-clocking feature is not a problem here since all users are interconnected and have perfect clocks.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "4018c3639fb14ecd896d7681e153a497",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Character oriented encoding is used.",
        "answer_feedback": "The response is not related to the theme of the encoding types.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "f076b3fc5e594613a7de8fbec979a3be",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding should be used in this network because all three end systems have perfect clocks wherefore a self-clock feature isn't necessary. It also provides better utilization of the bandwidth than Manchester encoding or differential Manchester encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "96d136cbffb7427187b532fcf76c937c",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "They should use \"Binary Encoding\" because of the perfekt timed clocks and furtermore this mechanism has the best transfer rate (1 bit per Baud).",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "e5ead8d68deb4e10867d31889a72cfe2",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Bianry Encoding, since it has good utilization of bandwidth which could solve the traffic problem. On the other hand, the 3 users have already perfect clocks, the no \"self-clocking\" feature of binary coding could be neglected.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "de01b6580f24430d952fd6a2d331f158",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I would use Binary Encoding. It is efficient since it uses 1 bit per baud. It has no self-clocking feature but that is not needed since all user have perfect clocks.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "e8a8f92c80014aada0537f98b769e0f2",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding can be used. It has the highest bandwidth (1 bit per Baud) and is simple and cheap. The 'self-clocking' feature of the more complex manchester encoding and differential manchester encodings is not necessary since the users have perfect clocks.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "1d29fe5a84494080a610351dfca82c0e",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I would use the binary encoding in this network. The \"self-clocking\" feature of the Manchester Encoding isn't an advantage in this scenario since all users have perfect clocks. Therefore the perfect clocks can even out the Binary Encoding's disadvantage of not having a \"self-clocking\" feature and only the advantages of being cheap, simple and the good utilization of the bandwidth remain.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "80259d8b9ffa42c084d7672be0eb3dd2",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Because all users have a perfect clock, the binary encoding is best to be used. It is simple, cheap and the bandwith is with 1 bit/Baud well utilized. (The Manchester encodings in comparison have only 0.5 bit/Baud.)",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "86e7c60f1c3c49d3b896f55aa2ad57e6",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding als Non-return to zero-level (NRZ-L) w\u00e4re zu empfehlen, da es hierbei eine gute Ausnutzung der Bandbreite (1 Bit pro Baud) bereitgestellt werden kann. Des Weiteren erfordert diese Kodierung eine sehr akurate Zeitmessung der Teilnehmer, da kein self-clocking stattinden kann (bei einer Abfolge gleicher Werte ver\u00e4ndert sich die Kurve nicht), dies ist aber gegeben.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "7d7106f912d84354b9cd9588bcffe129",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I would suggest to use binary encoding, because they are all interconnected and have perfect clocks, that is why they do not need a self-clocking encoding and it is simple and cheap, so it has a good utilization of the bandwidth ( 1 bit per Baud), what helps against congestion.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "09ea7603b299452b9ea508d6baeab95c",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding. Because the Binary Encoding uses the least bandwidth among these three techniques. And the local network with 3 users is tolerant of frequency errors happened in Binary Encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "477d2f072f734d6bb621393387ad6a6c",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "We can use Bit encoding - it has a good utilization of the bandwidth, and it can work because the users have perfect clocks. \nanother technique we can use is Manchester encoding - is it not sensitive to  \u05deoise on the line and therefore can deal with more users using the line.",
        "answer_feedback": "Manchester encoding is not correct as we do not require self-clocking, also bandwidth utilization is less. (Note: as per the notification in the quiz slide, an additional\u00a0 incorrect answer will also be counted while providing grading )",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.5
    },
    {
        "id": "14835f75095a4407a3ca36cb391abd71",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Since we want to transmit with as high of a baud rate as possible we first look at binary encoding. For binary encoding we need perfectly synchronous clocks. Since all our users have perfect clocks, binary encoding is better, since the baud rate is twice as high.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "f9b9a7f5e7604f16b204577971dd448d",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I would use binary encoding, since the premise is that all users have perfect clocks. This makes the \"self-clocking\" feature of the other two encoding methods not necessary. Binary encoding also makes good utilization of the bandwidth (1bit per baud) which is good, since the 3 users generate lots of traffic.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "c722b046ff224cb79476dead7a7e084f",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "In this local network with 3 users, the encoding technique Binary Encoding should be used.\nThis technique is simple, cheap, and has a good utilization of the bandwidth. The disadvantage of the no \"self-clocking\" feature is compensated through the fact that the users have perfect clocks.\nIn contrast, the Manchester and Differential Manchester Encoding have a worse utilization of the bandwidth  (0.5 bit/baud) than Binary Encoding (1 Bit/Baud) and the \"self-clocking\" feature is unnecessary in this case.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "6efb094124aa4047af4c2c7e2947b000",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "A binary encoding would be most beneficial. Since all clients have perfect clocks, it doesn't need a self clocking feature as used by Manchester encoding and differential Manchester Encoding. It also can transmit a double of the data, effectively increasing the channels capacity",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "610e3a89ce244329a63e3d75b0d2a013",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding should be used. \nPerfeclty clocked users do not need any self-clocking encoding such as Manchester.\nMore traffic would benefit from more efficient encoding technique.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "6a73616796c24eb1b031a33e908c02c6",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Baudot time multiplex system.\n3 users have perfect clocks, so all channels can be processed in a fixed grid within a certain time. Each channel is assigned a fixed time window (time slot).The time windows can be synchronized and of the same length or asynchronous and depending on requirements. This is Time Division Multiplex.",
        "answer_feedback": "The question asks for the type of encoding rather the channel access type.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "d85fdcdbaf6e464bbf60b0117105d485",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "We expect the system to be in a state with less than 10 packets in the queue for 56.94s during the measured interval of one minute. \nFirst we calculated the utilization: (9pkt/s)/(10pkt/s) = 0.9\nThen, we calculated the probability for the system to be in state 10, i.e. the probability that the system is full. p_10=((1-0.9)*0.9^10)/(1-0.9^11) = 0.051\nNext, we calculated the counter probability of p_10 as we actually want to know with which probability the system is not in state 10. 1-p_10 = 1-0.051 = 0.949\nTo get the number of seconds the system is not in state 10, we calculated 60s * 0.949 = 56.94s as we measure the system for 60 seconds.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5db28d57875c4ebebe8d5e363769e814",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "First we need to calculate the utilization. In this scenario the utilization is quite high with 90%. However we want to know the time that the buffer has less than 10 entries. This means that not the case if the buffer is full. We can calculate probability for that using the above-mentioned utilization. The probability is about 5%. So it\u2019s likely that the system will most likely have the full minute less than 10 packets.",
        "answer_feedback": "The response is partially correct because calculating the probability when the buffer is full is correct. However, it lacks the step where the probability is multiplied with the time to get the expected number of seconds, that is 56.9512 seconds.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "0bdd8976152742e295ad115f3a72e7e1",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "p = 9/10 = 0.9\nThe expected Number of Customers in the System are p/1-p = 0.9/0.1 = 9. I expect that the queue always has less than 10 packets waiting in it.",
        "answer_feedback": "The stated justification is incorrect as the given rates are not constant and can vary through time, so an average needs to be calculated for the given time. Therefore, the stated time (60 seconds) is also incorrect.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "72b79f6bc33d40689534b4bfe55950e8",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "\u03bb = 9pkts/s\n\u00b5 = 10pkts/s\n\u03c1 = 9/10 = 0,9 = 90% Utilization\nE[n] = (0,9/0,1) \u2013 ((11*0,9^11) / (1-0,9^11)) = 3,969 = 4 Expected number of customers in the system.",
        "answer_feedback": "The response is incorrect because the question requirement is to find out the expected number of seconds where the system has less than 10 packets waiting in the queue while the response states the number of expected packets in the system, which is incorrect as well.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "698bd1bdea784ebba436e95ebc7dc7e3",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "We need to calculate the probability that the system has 9 or less packets. To do that we calculate 1 \u2013 p_10.  p_10  is the blocking probability P_B which can be calculated using the formula on page 31 from the performance evaluation slides. \nWith a utilization of 9/10 we have a probability of around 1 \u2013 0.05 = 0.95.\nWhich means we expect the system to spend around 0.95 * 60s = 57s seconds in a state with less than 10 packets.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e0e9a43eb7b643fe8a675aba18461116",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "\u03bb = 9     less than equal to packets arriving per second\n\u00b5 = 10   less than equal to packets served per second\nN = 10   less than equal to buffer size\n\n=> \u03c1 = 9/10\n\nThe probability that there less than 10 packets in the system is E(n less than 10).\n\nE(n less than 10) = 1- E(10) = 1 - Blocking probability\n= 1 - 0.457324\n= 0.954276\n\nSo in 60 seconds, there are less than 10 packets in the queue for E(n less than 10) * 60 = 57.2561 seconds.",
        "answer_feedback": "The steps stated are correct but the obtained blocking probability and the final time are not correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "de5df85b40d1454eb86eb1ad231ffde2",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "We expect the system to be in a state where there are less than 10 packets in the queue roughly 57 seconds of the 1 minute we observe.\nFirst we need to calculate the mean load of the system (ro) as 9/10 as we can serve 10 packets while 9 arrive per sec.\nThen we can use the formulas from the lecture to compute with what percentage the system is in the state where 10 packets are waiting in the queue. \nThis is roughly 5% of the time, which means there are 10 packets waiting roughly 3 seconds of our observed minute.\nWhich means there are less than 10 packets waiting ~57 seconds of our minute.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ae833bec03d64d0c98936e0fafd4e60a",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "The system reaches equilibrium when the arriving packet rate is the same as the serving packet rate.\nThe system will never reach equilibrium because the arriving packet rate is less than the serving packet rate, thus the packets in the queue will always be less than 10 packets.",
        "answer_feedback": "The response states that the system will never reach equilibrium, but it is stated in the question as an assumption already. Also, the given rates are not constant and can vary through time, so an average needs to be calculated for the given time. Instead of 60 seconds, the buffer is less than full for 56.9512 seconds on average.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "5757a21e6a794267a5d6afc0488bca99",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "With a buffer of size 10, the probability of less than 10 packets in the queue is the probability that there is any other number of packets in the system than 10. With the utilization being 9 / 10, the probability for this case after reaching equilibrium is 1 - p(10) = 0,9492 (rounded).\nWhen monitoring the system for one minute (60 s), I would therefore expect the system to be in a state with less than 10 waiting packets for 0,9492 * 60 s = 56,952 s",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "776aeda0bec149108911799e9364c045",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Since the system reached an equilibrium the probabilities do not change anymore ( dp_n(t)/dt = 0 ). So it can be assumed that the queue is emptied by one package each second on average (10 served - 9 arrived).\n\nNow assuming that the queue is full at obvservation start, it will be empty after 10 seconds, from which 9 seconds the queue has less packets then 10.",
        "answer_feedback": "The response is incorrect because the stated number of expected seconds is incorrect as the correct number of seconds is 56.95 seconds.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "a1214cc1f9ba496a83728c0967ab1f17",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Yes. When low utilization makes it more likely that the corresponding path is used, the load on this path rises and a state might occur where the routing path starts oscillating. This will lead to inconsistency of the routing table of sender and receiver.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "734ee0b96a6541da91c518db143f3baa",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "During the transmission of the data, the most favorable path could change, causing the second part of the data to take a different path.\n\nIf the second part of the data arrives at the destination first, the receiver must wait for the second part and arrange the two parts correctly again.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ee6e7073d7c24a2bb95bede53f4aa7f4",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Ja, da es zu sogenanntem Oszillierendem Verhalten kommen kann wenn die aktuelle Auslastung einer Leitung als Metrik benutzt wird, wodurch permanent die Route gewechselt wird. Dies passiert dadurch das ein Packet \u00fcber Route X zu G geschickt wird, wodurch die Auslastung dieser Route steigt, was wiederum A dazu animiert eine andere Route zu w\u00e4hlen -> usw.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "9a4fbd9d9d0941ad8e1181fa49f0e099",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "One problem can be that the selected path utilized so that it will have to take another path. This path will be longer and will need more traffic.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "977b32d610ff49aabe50cfeb0d77e4c2",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "it certainly could, firstly, evaluating currend load without taking path transfer capacity into consideration could lead to misjudges.\nSecondly, avoiding certain busy path could lead to more hops, for example, a packet may take the path A-B-D-E-C-F-I-J-H-G, the packet may have avoid a few busy pathway but the total routing time could be longer.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e52b7f8302bb4afe8cad08acfbae796e",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Nein, denn dies f\u00fchrt zu Schwankungen, wenn es mehr als einen Pfad zwischen einem beliebigen Paar\u00a0 von Endsystemen im Netzwerk gibt. Das bedeutet, dass das Routing instabil ist und es gibt uneffiziente Paketumlagerungen.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8a62fb50838c4ad79cc9e16dc106b4fb",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "I think the network topology is perfect. Because there so many paths between A and G, A-B-C-F-G, A-E-I-F-G etc. If some paths is fail, the data could through another path to send. But the problem is the right part is a little simple. It's not reliable and steady. It will be better, if there are more connection between G and the other nodes in right part, for example G-I, or G-j.",
        "answer_feedback": "The question is not about the network topology but about the routing strategy!",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "a8b173eb40ac4073aa7cec081c6b12f7",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "When there are multiple transmiisions, with load may lead to an oscillation of the load, i.e., if A wants to send a message to H under the condition that CF is overload and EI is avaliable, it will choose EI to transmit message, when CF is avaliable, it will choose CF. Hence, routing tables may oscillate frequently.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f43de9e3373e4af89e2db8dc5728c113",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "A k\u00f6nnte viele packets in einer kurzen Zeit senden und damit G (oder einen der Zwischenknoten) \u00fcberlasten. Flow control muss also beachtet werden.",
        "answer_feedback": "But this is not a problem specific for the routing strategy",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "6fe79c2d01194a8cb269f1d2f54bb8f0",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "It could be a problem that the packets are oscillating. So the packets will never arrive to G.",
        "answer_feedback": "Oscillating does not mean, that packets are never arriving in G!",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "abcc8400d43845d2a0d224f3b9929c78",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Using metrics like load or utilization can lead to 'oscillations' - which means that every time the load or utilization changes, the path taken will change, and the load and utilization will change while packets are being routed.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "9f3c3451dfe1433ab8b4c522fecdce0e",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Yes, because the path load is then reduced for the chosen path and if \nthe receiver needs a certain path on which throughput is critical (e.g. G\n to F), it can not use it properly, because the load is higher at the time of receiving. For example F to G is on the path \nfrom A to G, then the load on F to G is reduced at the time of \nreceiving. If at the time of receiving the receiver G wants to send \ncritical data fast to F, it cannot use the path G to F properly and \nneeds maybe to elude to path G to H to F.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b519f276308344fba72d3e4742750ee1",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Yes this strategy could lead to problems as it could lead to oscillations. If there are two possible path (i.e link CF or link EI) the choice of which path to take could flip around as choosing one path increases the load on that path and in return making the other path more favorable increasing the load on this other path, so the decision on which path to take could swap around repeatedly.",
        "answer_feedback": "But why is oscillating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6f95174a3466467f84710807a514cf38",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, forward)\n\n\n\n\nHop 2:(B, E, forward) (B, C, drop) <= A->C is shorter\n\n(C, B, drop) <= A->B is shorter\n\n(C, E, drop) <= A->B->E is shorter\n(C, F, forward)\n\n(C, D, drop) <= A->D is shorter\n\n(D, C, drop) <= A->C is shorter\n\n(D, F, drop)  <= A->C->F is shorter \nHop 3: \n\n(E, C, drop) <= A->C shorter\n\n(E, F, drop) <= A -> C-> F is shorter\n\n(E, G, forward)\n\n(F, D, drop) => A->D is shorter\n\n(F, E, drop) => A -> B-> E is shorter\n\n(F, G, drop) => A -> B -> E -> G is shorter \nHop 4:\n\n(G, F, drop) => A->C->F is shorter\n\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "1c8af18448da49ab8f466a5e28c81cd7",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, drop) // C and F do both not send packets to A over D\u00a0Hop 2:(B, E, forward)(C, F, drop) // E and G do both not send packets to A over D\u00a0Hop 3:(E, G, forward)Hop 4:(G, H, drop) // no destination other than G is left for H",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "87ebea20f832442a92639697dbac1ab6",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward), (A, C, forward), (A, D, forward)\n\nHop 2:\n(B, E, forward)\n(C ,F, forward)\n\nHop 3:\n(E, G, forward)\n\nHop 4:\n(G, H, forward)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning\u00a0(A,D, drop),\u00a0(C, F, drop) and (G, H, drop)\u00a0.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "id": "58bfe839c1f44dc4a6721516a0dea74c",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2:\n(B, C, drop), C hat das Paket bereits empfangen.\n(B, E, forward)\n(C, B, drop), B hat das Paket bereits empfangen.\n(C, D, drop), D hat das Paket bereits empfangen.\n(C, E, drop), E bekommt Pakete von A normalerweise \u00fcber B.\n(C, F, forward)\n(D, C, drop), C hat das Paket bereits empfangen.\n(D, F, drop), F bekommt Pakete von A normalerweise \u00fcber C.\n\nHop 3:\n(E, C, drop), C hat das Paket bereits empfangen.\n(E, F, drop), F hat das Paket bereits empfangen.\n(E, G, forward)\n(F, D, drop), D hat das Paket bereits empfangen.\n(F, E, drop), E hat das Paket bereits empfangen.\n(F, G, drop), G bekommt Pakete von A normalerweise \u00fcber E.\n\nHop 4:\n(G, F, drop), F hat das Paket bereits empfangen.\n(G, H, drop) H hat keine Nachbar, an die das Paket weitergereicht werden kann.",
        "answer_feedback": "In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "31653180a8714c98ac4491d909505e06",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop) <= D can't forward packet further because costs would be higher than on other routes\nHop 2:\n(B, E,\u00a0forward)\n(C, F,\u00a0drop) <= F can't forward packet further because costs would be higher than on other routes\nHop 3:\n(E, G,\u00a0forward)\nHop 4:\n\n(G, H, drop) <= H can't forward packet further, no further nodes there",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "99b483a54ae14b6ab2d0119a4806776b",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop) because A sends to D, but D doesn't forward anywhere else (neither to C or F, because those nodes are reached over other links)\n\nHop 2:\n(B, E, forward)\n(C, F, drop) because F won't forward the packet anywhere else (F / E / G, because those nodes are )\nHop 3:\n(E, G, forward)\n\nHop 4:\n(G, H, drop) because H is only connected to G and receives the packet from G, so it doesn't need to forward it anywhere else",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "5b68249deb784b7f9d8eab2069fed07c",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop): Because D recognize that F and C won't receive packets via D.\n\n\nHop 2:\n(B, E, forward)\n(C, F, drop):\u00a0Because F recognize that E,D and G won't receive packets via F.\n\n\nHop 3:\n(E, G, forward)\n\n\nHop 4:\n(G, H, drop): There is only one possibility for\u00a0 H to receive the packet (via G ) and it can't be send it anywhere else.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "b7400185afd94f8cbf01c2f329a467e9",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1:\u00a0\n(A, B, forward)\n(A, C, forward)\n(A, D, drop)\u00a0// D will receive the packet and won't forward it\u00a0\n\nHop 2:\u00a0\n(B, E, forward)\n(C, F, drop)\u00a0// F will receive the packet and won't forward it\u00a0\n\nHop 3:\u00a0\n(E, G, forward)\n\nHop 4:\u00a0\n(G, H, drop) // H will receive the packet and won't forward it",
        "answer_feedback": "The reason should explain why it is not forwarded, for example,\u00a0(A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A.\u00a0 For (G,H,drop), it has no other neighbor",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "id": "fd8bbe7105284be8baf8f62f0b95b326",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1: \n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, C, dropped) C would not send packets to A via B, costs 4, direct path to B costs 2(B, E, forward)(C, B, dropped) same reason as before(C, D, dropped) C would not send packets to A via D, costs 4, direct path to D costs 2(C, E, dropped) E would send packet via B(C, F, forward)(D, C, dropped) same reason as (C, D)(D, F, dropped) f would send packet via C, costs of 1 instead of 3\nHop 3:\n(E, F, dropped) e would send packet via B, costs of 1 instead of 2 (over F and C)(E, G, forward)(F, E, dropped) same reason as before(F, G, dropped) g would send packet via e, because costs of 1 instead of 2\nHop 4:\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "6b26ecb4abba4853bd60baf4308116e4",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,C,forward)(A,B,forward)(A,D,forward)\nHop 2:\n(B,E,forward)(C,F,forward)\nHop 3:\n(E,G,forward)\nHop 4:\n(G,H,drop) => dropped because the package arrived from the port with shortest path to S but there is no other port to forward the package to.",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.5 for reasoning (A,D, drop), (C, F, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "id": "7bf37f373a0340e5830fded62a295f9a",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1\n(A,B, forward)\n(A,C, forward)\n(A,D, forward)\n\nHop2\n(B,E,forward)\n(C,F,forward)\n\nHop3\n(E,G,forward)\n\nHop4\n(G,H,forward)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "ab6e09685e35475b86d2c2bb6c1e60e6",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2\n(B, E, forward)\n(C, F, forward)\nHop 3\n(E, G, forward)\nHop 4\n(G, H, forward)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "id": "088c16c3e31c45d0b54b22585ce56063",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop) <= There is no edge over which it is the best route to send packets over D to A\n\nHop 2:\n(B, E, forward)\n(C, F, drop) <= There is no edge over which it is the best route to send packets over F to A\n\nHop 3:\n(E, G, foward)\n\nHop 4:\n(G, H, drop) <= H does not have any other neighbor to send the packet to",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "f8ae04f88ac643efbe0e71f7660c189d",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, E, forward)(B, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C,A)(C, B, drop) <= not the minimal route / part of minimal spanning tree. B would use (B, A) (C, E, drop) <= not the minimal route / part of minimal spanning tree. E would use (E, B, A)(C, F, forward)(C, D, drop) <= not the minimal route / part of minimal spanning tree. D would use (D, A)(D, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C, A)(D, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)\nHop 3:\n(E, C, drop) <= not the minimal route / part of minimal spanning tree. C would use (C, A)(E, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)(E, G, forward)(F, D, drop) <= not the minimal route / part of minimal spanning tree. D would use (D, A)(F, E, drop) <= not the minimal route / part of minimal spanning tree. E would use (E, B, A)(F, G, drop) <= not the minimal route / part of minimal spanning tree. G would use (G, E, B, A)\nHop 4:\n(G, F, drop) <= not the minimal route / part of minimal spanning tree. F would use (F, C, A)(G, H, forward)\nHop 5:\nNo further transmissions because no more routes except to G available.",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "95df8c7e73454f28b3347e3ebb873b9d",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. TSAPs\nAd: TSAPs only\u00a0 for one connection\u00a0\nDis: process server addressing method not possible\n\n2.identify connections individually\nAd:each individual connection has a new Seq\nDis: End system will be switched off\n\n3. identify PDUs individually\nAd: Seq never gets reset\nDis:higher usage of bandwidth and memory",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "5ed8ede7f3ab461b926b623a78946bf6",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. to use temporarily valid TSAPs\nMethod: TSAP only valid for one conntection, generates always a new TSAP\nAdvantage: complete unique connection\nDisadvantage: too many connections will be generated in worst case, in general not always applicable\n\n\n2. to identify connections individually\nMethod: each individual connection is assigned a new SeqNo\nAdvantage: each connection relies on SeqNo and will be remembered from endsystems because of the assigned SeqNo\nDisadvantage: endsystem must be capable of storing this information\n\n\n3. to identify PDUs individually\nMethod: individual sequential numbers for each PDU,\u00a0SeqNo basically never gets reset\nAdvantage: packets have a \"lifetime\" within the network\nDisadvantage: higher usage of bandwith and memory",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "e5ee9787c06b48a6ac36b91ace0758e2",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1)Temporarily valid TSAPs\nAdvantage: Easy to implement\nDisadvantage: Doesn't work for Connections that use a static and known TSAP.(server addressing)\n\n\n2)Identifying the connections individually with SeqNo\nAdvantage: Each connection gets its own SeqNo making it unique and easily idnetifiable.\nDisadvantage: The End Systems has to remember already assigned SeqNo.\n\n\n\n3)Identifying the packets individually with SeqNo\nAdvantage: Makes it easy to identify duplicates, as each packet has its own SeqNo.\nDisadvantage: Requires a lot of bandwidth and memory.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "c3815abdeb2e4cd59345d6d49a7d03db",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. To identify each connection, i.e., Each connection is assigned\u00a0 an unique number and endsystems remember already those number.\u00a0\nPro: + not complicate\nCon: - endsystems need largememory capacity to save this information.\n\n2. To identify PDUs individually: Assign individual seq number for each PDU.\nPro: + higher usage of bandwidth and memory.\nCon: - hard to choose a suitable sequential number range because of different packet rate and the limited lifetime of a packet within the network.\n\n3. To use temporarily valid TSAPs\nPro: + high availiablity\nCon: - not always applicable , sometimes no possible to address method",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "00e007d8140a41a1b9f9ce9c3a456466",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. to use temporarily valid TSAPs,\u00a0\u00a0TSAP valid for one connection only,but\u00a0process server addressing method not possible\n2.\u00a0to identify connections individually,\u00a0\u00a0each individual connection is assigned a new SeqNo and\u00a0endsystems remember already assigned SeqNo, but\u00a0endsystems must be capable of storing this information\n3. to identify PDUs individually,\u00a0individual sequential numbers for each PDU,\u00a0SeqNo basically never gets reset, but\u00a0higher usage of bandwidth and memory",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "25423917820f4267a5e6ad1ced2d4a8a",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "Temporarily valid TSAPs\nIdentify connections individually\nIdentify PDUs individually\nPros\nTSAP valid for one connection only\neach individual connection is assigned a new SeqNo and endsystems remember already assigned SeqNo\nSeqNo never gets reset\nCons\nserver is reached via a designated/known TSAP\nendsystems must be capable of storing this information\nhigher usage of bandwidth and memory",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "fa576afb82964c24834b965e9b8b4194",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1) use a unique TSAP from the beginning till the end of the connection\n+ ensures there won't be problem with misinterpreting connections at the same port\n- requires large name size of TSAP\n- practically impossible, because there exist \"well-known\" TSAP that exist always\n\n2) define unique connection sequence number, with end systems also remembering previous sequence numbers\n+ by remembering the previous connections too, the end systems can differentiate all the incoming data\n- can't work easily for connectionless services\n- end systems need enough space for storing this information\n\n3) use sequence numbers for packets\n+ you don't need to get bothered for resetting the sequence numbers, because for example (also stated in the lecture), a 48 bit number will practically never reach to an end\n- however, the bandwidth and the memory you need to send a packet gets increased, because the sequence numbers never get reset and need a number with many digits (e.g. 48 bits)",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "d722cab1495148ebbb3485c2d933154e",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. to use temporarily valid TSAPs\nadv:simple\ndisadv:may cause some comflict with some \"well-known\" TSAPs\n\n2. to identify connections individually\nadv: reliable\ndisadv: end systems must be able to store SeqNos\n\n3. to identify PDUs individually: individual seq num for each PDU\nadv: higher usage of bandwidth and memory\ndisadv: higher cost",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "8b53925e7cc04964ac94057c747ee970",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Use of temporarily valid TSAPs\n[\u2013] not applicable in most cases as TSAP must be known to address the correct process\n[+] in case of always changing TSAPs which are known to sender and receiver (i.e. through a known sequence) an attacker has a harder time tracing/interrupting the targeted traffic in case there are multiple senders/receivers\n2. Identify connections individually by assigning new SeqNo\n[\u2013] requires state / storing SeqNo (connection oriented system)\n[+] easy differentiation of connections\n3. Identify PDUs individually with SeqNo's\n[\u2013] a notion of a \"lifetime\" of a packet in the network is needed\n[\u2013] higher usage of bandwidth and memory\n[+] works with connectionless system",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "0e420f86be8245eeb9c9dccdef0b66f4",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. to use temporarily valid TSAPs.\n\u00a0pro: sure would solve the duplicate problem\n\u00a0contra: some servers should be reached through a known TSAP, which should not be discarded after single usage.\n2.to identify connections individually\npro: better than temporarily valid TSAPs, at least can be used to solve duplicate problem in a connection-oriented system.\ncontra: in a connection-less system it does not work.\n3.to identify packets individually\npro:solve the duplicate problem with dexterity, man can choose the sequence number range for individual case.\ncontra: the usage of bandwidth and memory will be higher due to the packet sequence number.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "e23a6e87c6a54f06bc39181e2dc0d162",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "Unique TSAP for each connection:\n\u00a0+ addresses duplicated across connections (e.g. after a crash)\u00a0- usually limited number of TSAPs\u00a0- some TSAPs are well known and therefore not usable (e.g. for HTTP)- doesn't address duplicates within the same connection\n\nSequence Number for each connection:\n\u00a0+ solves problems specific to unique TSAPs\u00a0- but also doesn't address duplicates within one connection\u00a0- endsystems need to store sequence numbers, even after switching off\n\nSequence Number for each PDU:\n\u00a0+ addresses duplicates between and within connections\u00a0+ only need info about last few used SeqNrs\u00a0- higher usage of bandwidth/memory\u00a0- have to choose a range for SeqNrs",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "5dd2ee9f69bc4dc19f50c1fd114b715c",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1.\u00a0to use temporarily valid TSAPs\nAdvantages: TSAP valid for\u00a0one\u00a0connection only(always newly generated)\nDisadvantages: in general not always applicable:\u00a0\nprocess server addressing method not possible, because:\u00a0\n-\u00a0server is reached via a designated/known TSAP\n- some TSAPs\u00a0always exist as \u201cwell-known\u201d\n\n\n2.\u00a0to identify connections individually\nAdvantages:\u00a0\neach individual connection is assigned a new SeqNo and endsystems remember already assigned SeqNo\n\nDisadvantages:\u00a0endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed\n\n3.to identify PDUs individually:\nindividual sequential numbers for each PDU\n\nAdvantages: SeqNo basically never gets reset\nDisadvantage: higher usage of bandwidth and memory",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "53f831ca87d84ab781d5f69454dddae0",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start \nPhase 2: Congestion avoidance \n\nIn the beginning of Phase 1 the cwnd is increasing exponentially starting at cwnd=1 by doubling cwnd after every transmission until a threshhold ss_thresh is reached. From this point onwards, cwnd is increased linearly until congestion occurs. This initiates Phase 2, in which ss_thresh is set to the half of the value of cwnd at the moment when the congestion occured (ss_thresh_new = cwnd/2). Afterwards cwnd is reset to 1 and Phase 1 starts again with the new value of ss_thresh.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The congestion avoidance phase starts when the cwnd becomes equal to the threshold, not when the congestion starts.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "id": "33fb973f368749d8852cc0be6af51a7c",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow Start\n              In this phase, every time the segment is acknowledged, cnwd plus one until it reaches ss_thresh or packet loses, but when cnwd is not smaller than ss_thresh, cnwd will increase much slower.\n\nPhase 2: Congestion Avoidance \n            In this phase, the new ss_thresh is half of the cnwd, and cnwd will be reset to one, then slow start starts.",
        "answer_feedback": "The response correctly states the name of the two phases. The Slow start phase's explanation is partially correct because though it mentions that packet loss can occur before ss_thresh is reached, it does not provide details of the changes in the value of ss_thresh and the congestion window. Also, a linear increase of the congestion window happens in phase 2, not 1. The second phase does not state when the threshold and congestion window changes are done.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "9d5a3e937992436d91610224083c2154",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are called slow start and congestion avoidance. In the first phase grows the cwnd exponentially until it reaches ss_thresh. From this moment, the second phase begins and the cwnd grows only linear. If a timeout happens, cwnd is reset to one and ss_thresh is set to the half of the (former) cwnd. Then the two phases start again.",
        "answer_feedback": "The Slow start phase's explanation is partially correct as it does not mention what happens when a packet is lost before ss_thresh is reached. Here the slow start threshold also becomes half of the congestion window, and the congestion window becomes 1. The explanation of the congestion avoidance phase is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "74467bb80cf240f19fda9d75dce58c1c",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are:\n1. slow start\n2. congestion avoidance\n\nAfter initialization (cwnd = 1, ss_thresh = advertised window size), during the slow start, cwnd is incremented by one each time a segment is acknowledged, so that cwnd grows quickly (cwnd = 1, 2, 4, 8; so in effect, it is doubled every round-trip time).\nIn case of packet loss (congestion) ss_thresh is reset to the half of cwnd, cwnd is then reset to 1 and the slow start phase is started from the beginning, otherwise cwnd is incremented as long as the condition cwnd less than ss_thresh holds.\nWhen ss_thresh is reached, the second phase (congestion avoidance) is entered and cwnd is now increased more slowly (linear versus exponential increase in the first phase: cwnd = 9, 10, 11...; it is increased by one every round-trip time) until a timeout (congestion) occurs.\nIn case of timeout (congestion), ss_thresh is reset to the half of cwnd, cwnd is then reset to 1 and the slow start phase is started again.",
        "answer_feedback": "The response is correct and complete as it provides the phases' names and changes in the value of the congestion window and threshold correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a3c11b128cec4b999cde4c41c0deea1c",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1 is called Slow Start. In this phase, the congestion window (cwnd) grows exponentially until the slow start threshold (ss_thresh) is reached, and then it grows linearly. Then, everytime congestion occurs, Phase 2 or congestion control starts. In this phase, ss_thresh is set to 50% of the current cwnd value. cwnd is then set to 1 and slow start starts again.",
        "answer_feedback": "The response is partially correct because the second phase is congestion avoidance and not congestion control. The slow start phase is missing details about how ss_thresh changes when a packet is lost. Also, in the congestion avoidance phase, it's unclear how the cwnd increases, till which condition it's done, and also when the ss_thresh is set to half of the current cwnd.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.38
    },
    {
        "id": "6f5c9d7f6fcc4c0daaa085b3cb61a8ba",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "In the first phase, the slow start, cwnd grows exponentially with base 2. \nWhen cwnd equals ss_thresh, the second phase, the congestion avoidance, starts, where cwnd now only grows linearly. \nWhen congestion occurs, ss_thresh is set to cwnd / 2 and cwnd is reset to 1 and the system is back in phase 1.",
        "answer_feedback": "The response is correct, except that it is not clear whether the congestion and the corresponding changes occur in phase 2 or both phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ef074d2d3a544e9eb229b33d270d6e01",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are a slow start and congestion avoidance. Lets assume the slow start threshold is X. After initialization, the \"slow start\" begins with checking if one segment arrives at the sender (receiving ACK) and increases the number of segments until the procedure throws an error. The new ss_thresh is half of the last segments that arrived successfully (cwnd/2). Then in phase two, the congestion window is reset to 1, and the process starts with a new ss_thresh (cwnd/2).",
        "answer_feedback": "The response states the two phases of congestion control correctly. However, it's unclear how cwnd is incremented. The response also lacks details about how and when ss_thresh changes in both phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "564482b6a1804060bd137d85477f2b05",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Two phases of congestion control: slow start and congestion avoidance \n\nCongestion Window (cwnd) and the Slow Start threshold (ss_thresh) changes in: \n- Slow start: cwnd starts with 1 and then after every successful ACK, 2 packets are sent instead of just one, so that cwnd increases exponentially with the power of two, as doubles after every round-trip time (RTT). When cwnd >= ss_thresh, congestion avoidance phase begins. Otherwise when a timeout (congestion) occurs during the slow start phase, cwnd is reset to 1 again (cwnd = 1) and the ss_thresh is set to half of the cwnd (ss_thresh = cwnd / 2). Then slow start repeats with the new ss_thresh value. \n- Congestion avoidance: cwnd is not doubled after every RTT anymore, but only incrementally increases until a timeout (congestion) taking place again. Congestion avoidance is terminated and it gets back to slow start phase with the ss_thresh = cwnd / 2 and cwnd is now reset to 1 again.",
        "answer_feedback": "The response is correct and complete as it provides the phases' names and changes in the value of the congestion window and threshold correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "819a664a753e4f0ebc14d02babc7bd81",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "First, in the \"slow start\" phase, the sender sends one segment, then two, four, eight segments, etc. (always doubling the cwnd), until the ss_thresh is reached. Then, in \"congestion avoidance\" phase, the sender only increments the cwnd linearly (+1). If a congestion occurs, the ss_thresh is set to 50% of the current size of the cwnd, the cwnd is set to 1, and the sender starts again with slow start.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6a081dc2d8144936915fd22ece5aa846",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: After the initializtion, cwnd is increased by 1 each time when a segment is acknowledged. This continues until cwnd == ss_thresh or a packet gets lost. When cwnd >= ss_trhesh, TCP slows down the increase of cwnd. Especially, slow start increases the rate exponentially if each ACK generates 2 packets. 2. Congestion avoidance: Each time congestion occurs, ss_thresh is set to ss_tresh = cwnd / 2 and cwnd is reset to one s.t. cwnd = 1. After that, slow-start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. In the congestion avoidance phase, the extent of slow down of the congestion window rate is not precisely mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "e48263e414fc444ab613293ae86cc3fb",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are Slow Start and Congestion Avoidance. At the slow start, the cwnd is initialised with 1 and then ex the slow start treshold (ss-thresh) is set on the advertised window size. While in the slow phase, the cwnd is counted up exponentially, but smaller than the ss_thresh. The congestion avoidance phase is reached, when the cwnd is as big as the ss_thresh. After that, the cwnd is increased linearily. Whenever there is a timeout, then the ss_thresh will be set on half the amount of the cwnd, and cwnd will be reset at 1 again and phase 1 starts again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "e86dcb903acf4f76a8afc5e60ffb88ac",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1. Phase: Slow start\n2. Phase: Congestion Avoidance\nIn the first phase Congestion Window is doubled until there is a time out or the Slow Start Threshold is reached. \nIf there isn't any time out, the Congestion Window is incremeted by one.\nAfter a timeout the Congestion Window will be set to one and the ss_thresh willl be set to the half cwnd.\nThis process continues at the begining Congestion Window size of 1.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "761da792fe0e461abec7e3d4f4221ae8",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The congestion control consists of two phases: slow start and congestion avoidance. After the initialization of cwnd and ss_thresh, the slow start tries to discover the proper sending rate as quickly as possible by incrementing the cwnd by 1 for each acknowledged package. This is continued until the ss_thresh is reached or a packet gets lost, then the congestion avoidance starts. Now each time a congestion occurs, the ss_thresh is set to cwnd/2, the cwnd is reset to 1 and the slow-start is entered again.",
        "answer_feedback": "In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.\u00a0 Congestion avoidance starts ONLY WHEN the threshold is reached , not when packet loss occurs.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.68
    },
    {
        "id": "39f345cfe0d34f13bb6391be4ae55cac",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are \"Slow start\" and \"Congestion Avoidance\". To make sure the network is not overloaded immediately, a TCP sender will start to send \"slowly\": First one segment, then as long as the segments get acknowleged double the rate each time, until the ss_thresh value is reached. This means after the first ACK is received, the sender will send two segments at once, then four, eight, etc until the ss_thresh value is reached or no ACK is received. If the ss_thresh value is reached (phase 2) the sender will increase the rate linearly by one each time. If a packet times out the ss_thresh value is set to 50% of the current rate and the cycle is repeated with phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached. This needs to be explicit for both the phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "1121de9a02b5493da2475fc76a148886",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The first phase, slow start, will double cwnd every round-trip time by increasing it by 1 for each received ACK. When cwnd reaches ss_thresh, the congestion avoidance phase is entered. The congestion avoidance phase will additively increase cwnd by 1 every round-trip time.If congestion is encountered in any of the phases, ss_thresh is set to half the value of cwnd, cwnd is set to 1 and the slow start phase is entered.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c65ebb109af14c89bfced5206c1977bc",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start (cwnd < ss_thresh)\nPhase 2: Congestion avoidance (cwnd >= ss_thresh)\nIn Phase 1, cwnd is initialized to 1, then it increases exponentially until it reaches to ss_thresh.\nIn Phase 2, cwnd increases one by one until it reaches the congestion, then new ss_thresh will be set to 2. Then cwnd is reset to 1 and phase starts.",
        "answer_feedback": "\"then new ss_thresh will be set to 2(half of current cwnd). Then cwnd is reset to 1 and phase starts(which phase starts??).\"\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ff38c95d4c5d4bb5a9d795446670fbe1",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The goal of the slow start phase is to quickly find a good sending rate. \nFor each ACK that is received, the cwnd is incremented, effectively doubling the cwnd within the round trip time.\nThe congestion avoidance phase starts as soon as either a packet loss occured or cwnd is greater or equals to ss_thresh and the cwnd is only incremented once each round trip time. \nWhen a timeout occures, ss_thresh is set to half of cwnd, cwnd is reset back to one and the slow start phase is entered again.",
        "answer_feedback": "Congestion avoidance phase\u00a0starts only when\u00a0 cwnd >= ss_thresh. Packet loss can occur in both phases, resulting in\u00a0ss_thresh = cwnd / 2 and\u00a0cwnd = 1",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "04ccef7128e74ef1a1c1c9b0846fd178",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1 - Slow Start:\nFor each received ACk, the cwnd is increased by one until the ss_thresh (Threshold) is reached.\nPhase 2 - Congestion Control:\nDuring congestion avoidance the cwnd increases linear by one per RTT. If a timeout occurs (congestion) the ss_tresh is set to half of the current window size (cwnd) and cwnd is set to 1. Then slow start will begin again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "52d29d98e18543f1a123359e92b32b95",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\nPhase 2: congestion avoidance\n\nPhase 1: start with a cwnd with one and double it every time a/the acknowledgment/s comes Until the ss-thresh(default: advertised window size) is reached or a congestion occurs.\n\nPhase 2: if a congestion didn't occur increase cwnd by one each time. until a congestions occurs. Then set ss_tresh to the half of the cwnd right now. and repeat phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "b4e2a0c56155424aaadbbe00679de1b6",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases of congestion control are the \"Slow Start\" and the \"Congestion Avoidance\" phase. In the slow start phase, the cwnd is incremented by one whenever a segment is acknowledged until the the cwnd reaches the value of ss_thresh or until packet loss occurs. In the congestion avoidance phase, the ss_thresh value is set to cwnd / 2 whenever congestion occurs. Afterwards, cwnd is reset to one in that phase.",
        "answer_feedback": "In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6042178a75504a03b95d3b817d08aafd",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1) Slow start\ncwnd verdoppelt sich nach jedem roundtrip (exponentiale Erh\u00f6hung)\nss-thresh bleibt w\u00e4hrend slow start gleich.\u00a0\n2) Congestion Avoidance\ncwnd wird nach jedem roundtrip um MSS/cwnd erh\u00f6ht (lineare Erh\u00f6hung)\nwenn congestion eintritt wird ss_tresh = cwnd/2 gesetzt und cwnd auf 1 gesetzt.",
        "answer_feedback": "\"ss-thresh remains the same during slow start.\u00a0\" not always as congestion can occur in this phase also.\u00a0After a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "d3fc1d4d6c2d431a80fbe6ee85bfe7f7",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start(getting to equilibrium)\nPhase 2: Congestion Avoidance.\n\nIn phase 1, cwnd is < ss_thresh, and initialize cwnd=1,\u00a0 and then it increases expotentially until reach the ss_thresh, in phase 2, cwnd>=ss_thresh, it increases slowly one by one until reaches the congestion, then set the new ss_thresh=cwnd/2, and reset the cwnd=1 and continue start from phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "deeffb080d7d4651a96af5a2870b6df7",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1. Slow start -> discover proper sending rate\nWhen starting traffic on a new connecting or when experiencing an increase in traffic after congestion, the cwnd is initalized with one.\nWhenever a segment is acknowledged, the cwnd is incremented by one until eather ss_thresh is reached or packet loss is experienced.\nPhase 2: Congestion Avoidance\nAfter leaving the slow start phase (cwnd >(=) ss_thresh), cwnd may be incremented by 1 MSS every RTT to a maximum of SMSS.\nWhen a timeout occurs, meaning a congestion is experienced, ss_thresh is set to half the current cwnd. Cwnd is reset to one and slow-start is entered again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "80770f18ce89499d9f3a9be6d1fca1a5",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow start: in Slow Start phase, cwnd is increased by one from 1 each time a segment is acknowledged i.e. cwnd is increased exponentially, but untill cwnd reaches ss_thresh (cwnd = ss_thresh) or when there is a packet loss. The increament is lowed down when cwnd >= ss_thresh i.e. cwnd is increased successively. \nCongestion Avoidance. when congestion occurs, the size of ss_thresh is set to 50% of the current size of the congestion window i.e. ss_thresh = cwnd / 2 and cwnd is reset to 1. After that, Slow Start phase is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached. Also \"The increament is lowed down when cwnd >= ss_thresh i.e. cwnd is increased successively.\" happens in the congestion avoidance phase.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "7ec9f23baf274ac3a173a4f245b02c3e",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are called \"Slow start\" and \"Congestion avoidance\". When cwnd is smaller than ss_thresh the slow start phase is in action and cwnd is rapidly increased in a short amount of time by incrementing it by one each time a segment is acknowledged resulting in doubling the rate exponetially by doubling it every RTT. If cwnd is greater (or equal) than ss_thresh the congestion avoidance phase starts where as long as non-duplicate ACKs are received the cwnd may be increased by 1 MSS every RTT (AIMD). When a timeout occurs ss_thresh is set to cwnd/2 and cwnd is set to 1 and another slow start phase is entered resulting in alternating slow start and congestion avoidance phases.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "44b6f3be922b4b7ba8531388aa99fb8d",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "phase 1:slow start\u00a0\nphase2: congestion avoidance\nIn the slow start :Each time a segment is acknowledged,\u00a0 increment cwnd by one (cwnd++)\nContinue until\u00a0 reach ss_thresh or packet loss\nIn the phase 2:Each time congestion occurs: ss_thresh is set to 50% of the current size of the congestion window:\u00a0\n\u00a0ss_thresh = cwnd / 2\ncwnd = 1",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f1898ed509f54de69b0628b719c86bbb",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow Start\nThe congestion window is increased exponentially, for every acknowledge the congestion window is increased by 1. \nPhase 2: Congestion Avoidance\nThe congestion window is increased by 1 for every round trip. If a congestion happens it is reset to 1 and the ss_thresh is halfed",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "291460fcc69849eca71182c9a59ff017",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "cwnd = 1 MMS, ss_thresh = window size\n\nPhase 1: Slow start\ncwnd < ss_thresh\n\nPhase 2: Congestion Avoidance\ncwnd >= ss_thresh",
        "answer_feedback": "Changes in both ss_thresh and cwnd in both the phases need to be explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "782da21a639142e69b485adc2fb97454",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start:\n\u00a0-1- initialize cwnd =1\n\n\u00a0-2-\u00a0Each time a segment is acknowledged, increment cwnd by one (cwnd++)\n\n\u00a0-3-\u00a0\u00a0Continue until\u00a0reach ss_thresh,\u00a0packet loss\n\nPhase 2: Congestion Avoidance:\u00a0\n\u00a0 Timeout = congestion\n\u00a0 Each time congestion occurs: ss_thresh is set to 50% of the current size of the\u00a0 \u00a0 \u00a0congestion window:\u00a0 ss_thresh = cwnd / 2 and\u00a0 cwnd is reset to one: cwnd = 1\u00a0 \u00a0 \u00a0and\u00a0 slow-start is entered",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "d79fd62aa45e4439b0a9d0e7f26a9ff9",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow_start (cwnd <= ss_thresh)\u00a0 cwnd is doubled each RTT which is equal to an increase by one for every acknowledged segment.\u00a0 Phase continues until ss_thresh is reached or packet loss occured Congestion Avoidance (cwnd >= ss_thresh)Additive increase multiplicative decrease cwnd+1 per RTTIf timeout occurs ss_thresh = ss_thresh / 2, and cwnd = 1enter slow_start again",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "491709da634b4a019f73c20483eb748a",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The phases are slow start and congestion avoidance.\nIn the slow phase, the cwnd starts getting bigger in size, first slowly, then rapidly, until the ssthresh is reached. Once reached, the congestion control phase begins, where the cwnd slowly grows in size until a congestion occurs (timeout). In this case the slow start phase is entered again, the cwnd is reset and a new ssthresh is calculated (half of reached cwnd before timeout.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "511a59eb34ed4b70826aa064791cfbe6",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "2 phases: Slow start and congestion avoidance. In the slow start phase,\u00a0 the cwnd is doubled from 1 to 2, 4, 8, after each ACK is received,\u00a0 when cwnd >= ss_thresh,\u00a0addiitively to 9, 10, 11... until timeout(congestion) or packet loss. When congestion occurs, ss_thresh is set to 50% of the current size of cwnd, cwnd is set to 1. Repeat again with slow start.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "f45994b97aad412ab04c764b01cf7062",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\u00a0(cwnd < ss_thresh)\ncwnd wird mit 1 initialisiert. Senderate, sprich cwnd, wird solange langsam erh\u00f6ht, bis Aufstauung (congestion) entsteht.\nPhase 2: Congestion Avoidance\u00a0(cwnd >= ss_thresh)\nss-thresh wird auf cwnd/2 gesetzt, wobei cwnd der aktuellen Fenstergr\u00f6\u00dfe entspricht.Anschlie\u00dfend wird cwnd wieder auf 1 gesetzt und zur \"Slow start\"-Phase \u00fcbergegangen.",
        "answer_feedback": "In phase 1, cwnd increases exponentially.\u00a0In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occurs is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "id": "d39c509b1c514acc94b772817224f9d4",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The first phase (\"slow start\") doubles the cwnd after every RTT until ss_thresh is reached. After that, the second phase (\"congestion avoidance\") starts and furthermore only increases cwnd +1. Each time a congestion occurs, the ss_thresh is set to 50% of the current cwnd and cwnd is reset to 1, after which \"slow start\" phase is entered again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "2aab0210604040a4917d3988056aec72",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start(getting to equilibrium)\nPhase 2: Congestion Avoidence\u00a0\nIn both phases,when timeout occurs, ss_thresh is set to 50% of the current size of the congestion window, cwnd is reset to one, and slow-start is entered.And each time a segment is acknowleged , cwnd increase by one ,when cwnd=ss_thresh or packet loss , congestion avoidence is entered from slow-start .",
        "answer_feedback": "At what rate cwnd increases in the congestion avoidance phase is not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "e39ff23d7ff040ed8755b5b438857754",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "By calculating using the binomial distribution probability formula, it can be concluded that the probability of event C occurring is 0.2765. \nThe probability of event A occurring is: P[event A]=P[you see exactly three H\u2019s]+P[you see four H\u2019s]+P[you see five H\u2019s]+P[you see six H\u2019s] =P(k=3)+P(k=4)+P(k=5)+P(k=6)= 0.2765+0.311+0.1866+0.0467=0.8208 (P is the binomial distribution)\nThe probability of event B occurring is 0.6*0.6*0.6*0.4*0.4*0.4=0.013824. \nTherefore the order is event B to event C to event A.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0f73374d82ff4996a3a584a99b009285",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> C -> A \nA: 0.6^3 * 0.4^3 * 20 + 0.6^4 * 0.4^2 * 15+ 0.6^5 * 0.4 * 6 + 0.6^6 = 0.8208\nB: 0.6^3 * 0.4^3 = 0.013824\nC: 0.6^3 * 0.4^3 * 20 = 0.27648",
        "answer_feedback": "The response states the correct order of events with the calculation of all event probabilities.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "613f0f3f354441499170e0cfdce23078",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B: 0.6^3 * 0.4^3 = 0.013824 = 1.3824%\nEvent C: P(X=3) = 20\u00d70,6 ^3\u00d7 (1-0,6)^3 = 0.27648 = 27.648%\nEvent A: P(X>= 3) = P(X=3) + P(X=4) + P(X=5) + P(X=6) = 0.8208 = 82.08 %",
        "answer_feedback": "The response does not explicitly state the events' order, but it contains the correct probabilities of all events, which is sufficient to identify the correct order.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "41959a1144bc4421b0c0dd4ebadc3cea",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "- Event B (lowest probability because there is only one sequence fulfilling this property)\n- Event C (superset of Event B and additionally containing all other sequences containing three H's)\n- Event A (superset of Event C and additionally containing all sequences with four H's, five H's and six H's)",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "40a50ed0871e41fe96c0e19d7f3ba215",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "From less probable to most probable: B, C, A.\nEvent B is very unlikely, because it is requiring a strict sequence of heads and tails, which can be calculated by 0.6^3 * 0.4^3 = 1.4%.  Event C is more likely than C, because it just requires 3 heads, which is less strict than B and includes B, it can be calculated by BinCoef(6, 3) * 0.6^3 * 0.4^3 = 27.7%. A is more likely than C, because it is less strict and having at least three heads as well includes C, having exactly three heads. It can be calculated as the sum for all k=3 to 6: BinCoeff(6, k) 0.6^k * 0.4^{6-k} = 81.1%.",
        "answer_feedback": "The response correctly answers the events' order with appropriate justification, but the final result of the probability calculation of events C and A is incorrect. The correct values are 27.648% (rounding mistake) and 82.08%, respectively.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "060923cceba24615b6b9c8edce080b91",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Order: Event B, Event C, Event A\n\nReason: Actually , Event B is a subset of C, and C is a subset of A, so the order must be  B -> C -> A.\n              P(Event B) =  0.6 * 0.6 * 0.6 * 0.4 * 0.4 * 0.4 = 216/15625 = 0.013824\n              due to the equation of the binomial distribution\n              P(Event C) = 864/3125 = 0.27648\n              P(Event A) = 5213/625 = 0.8208",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "93399343214b4656b7c825d74cb4415d",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event A ist the most probable event, because there are many sequences that accomplish it. So all the probabilities of these sequences can be added to calculate the probability of Event A.\nEvent C iss less proabable than A, because there are less sequences to accomplish ist than there are to accomplish A. For example A sequence with 4 H's would fulfill A but not C. But all of the sequences that fulfill C also would fulfill A. So C has the same probabilities that are beeing added to calculate the probability of A, but A has additional probabilities.\nEvent B ist the least probable, because the only sequence that fulfills it, is HHHTTT. And since this sequence also fulfills A and C, the probability of the occurrence of this sequence is just one of the probabilities that are been added to calculate the probability of A and C.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "644acd5953ec4041865c0d84876a2c0d",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B\nEvent C\nEvent A\n\nEvent B: This is because the probability of seeing exactly in sequence HHHTTT is very low as there can be many different combinations such as HTHHTH, HTHTHT and others similar to that but for having the sequence HHHTTT there is only a single combination so the probability is very low.\n\nEvent C: Then the probability of seeing exactly three H's as the probability for showing up a head is 0.6 so there is more chances that a head shows up so the probability of head is more so exactly three H's probability is also low.\n\nEvent A: The probability of seeing head at least three is more because the probability of head is more 0.6 so the probability of a tail is 0.4 so we consider for this circumstance 0 T, 1T, 2T and 3T so summation of all these gives us the probability of at least three H's",
        "answer_feedback": "The response correctly states the sequence of the three given events. The justification for the events C and A is incorrect as the exact probability of H is not relevant (except when P(H) = 1 or 0) as C is a subset of A.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "408af08475a74de9b575e67e9507262b",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 * 0.4 = 0.013824\nEvent C = P(k = 3) = 6 \u00fcber 3 * (0.6)^3 * 0.4^3 = 0.27648\nEvent A = P(k >= 3) = 6 \u00fcber 3 * (0.6)^3 * 0.4^3 + 6 \u00fcber 4 * (0.6)^4 * 0.4^2 + 6 \u00fcber 5 * (0.6)^5 * 0.4^1 + 6 \u00fcber 6 * (0.6)^6 = 0.8208\n\nFrom least to most probable = B,C,A",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "36ea53e824e046a49ba51885d80ad549",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B as it is 0.6^3*0.04^3 ~ 0.0138\nEvent C as it is 6 over 3 * 0.6^3 * 0.4^3 ~0.276\nEvent A as it is 6 over 3 * 0.6^3 * 0.4^3 + 6 over 4 * 0.6^4 * 0.4^2 + 6 over 5 * 0.6^5 * 0.4^1 + 6 over 6 * 0.6^6 * 0.4^0 ~ 0.82\nB is a specific sequence and c is different permutations of the same amout of hits so it B + the other combinations, A is all combinations to get 3 plus all the ones to get 4 5 and 6.",
        "answer_feedback": "The response correctly answers the order of the events and justifying it with probability calculations.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7ca9d950fee846fe9d4f7eb088ea36fb",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B:\nP(HHHTTT) = 0,6^3 * 0,4^3 = 0,013824\n\nC:\nP(X = 3) = (6 \u00fcber 3) * (0,6)^3 * (0,4)^3 = 20 * (27/125) * (8/125) = 0,27648\n\nA: (most likely)\nP(X >=  3) = 1 - P (X less than equal to 2) = 1- P(X = 0) + P(X = 1) + P(X = 2)\n= 1 \n- ((6 \u00fcber 0) * (0,6)^0 * (0,4)^6\n+ (6 \u00fcber 1) * (0,6)^1 * (0,4)^5\n+ (6 \u00fcber 2) * (0,6)^2 * (0,4)^4\n= 1- (0,004096 + 0,03686 + 0,13824) =0,8208",
        "answer_feedback": "The response does not explicitly state the order of the events, but it contains the correct calculation of all events probabilities, which is sufficient to identify the correct order.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d320327c66d34c839f45290e1606d3f1",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> C -> A\n\nB is the least propable, because there is only this one combination out of 64 possible combinations. The propability is 1,38% ( 0,6*0,6*0,6*0,3*0,3*0,3). C is more propable, because out of the 64 combinations, 20 combinations can satisfy this condition (formula would be 6!/3!*3!) and results in 27,63% propability. The most propable case is A, because it contains B and has the propabilities of having 4 Hs, 5Hs and 6Hs added onto it, which results in 82,08% propability.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification but the probability of event C is incorrect. The correct value is 27.648% (rounding mistake).",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6f6550c9213f4d01af3ca99f7a2f3e50",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> least likely \nC -> in between\nA -> most likely\n\nAll three Events have the same minimum amount of H's in their sequence which indicates that the probability order is based on the number of permutation. \nB has least likelihood because it has the least amount of possible permutations (exactly one).\nA has the most amount of permutation because it also allows permutations with more than 3 H's. So the permutations of C are a proper subset of the permutations of Event A. \nIn this case the probability of H is larger than 0.5 so that is even more likely that there are permutations with more than three H's. But it would also hold true if this was not the case.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c88fdaad3a424cc4823084415ad4105d",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Order: B-C-A\nThe probability of having HHHTTT is the lowest because its the most specific outcome. Only one possible path. \nThe probability of seeing exactly three H\u2019s is the second lowest. It includes the probability of HHHTTT and all other possible orders to achieve exactly three H\u2019s. \nHaving at least three H\u2019s is the most probable outcome of those three. It includes the probability of B and C plus all outcomes with more than three H\u2019s.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "afcab339bcdd4bedb487a639de9cb198",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "- Unconfirmed Connectionless Service \nThe Unconfirmed Connectionless Service sends data to the receiver, without announcing it (building up a connection) first in data frames without any flow control. Because of the missing connection and flow control, it is possible that complete data frames can get lost.  \n- Confirmed Connectionless Service \nWheras the confirmed connectionless service sends the data frames and waits for an acknowledgement of the corresponding recipient. If the recipient confirms the data frame, the next data frame is being sent. If the recipient doesn\u2019t answer for a long time, the data frame is being resent. If for some reason, the ackknowledgement gets lost, the recipient will eventually get a data frame twice, and will not be able to detect the duplication. The correction has to be made on a higher level. It is much slower than the unconfirmed, because of waittime for timeouts and ackknowledgement messages. \n- Connection Oriented Service \nIn the connection oriented service, the overhead is a lot higher, but the advantages are a detailed flow control, in which the recipient can detect duplicates, ask for a certain frame and can align the frames in the right order. And if the recipient reads slower than the sender transmit, it is possible to make a transmission. The participants first exchange a handshake and afterwards are transferring data. Afterwards the connection is disconnected.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "73417bd9dd0f4cf4b207dc567d8df6e8",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "\"Uncomfirmed Conn.less Service\", \"Confirmed Conn.less Service\", \"Connection-Oriented Service\"\n\"Uncomfirmed Conn.less Service\":\nData is just send, without any feedback, that it is received. Both partys assume that no bit is missing. There is no Flow Control or a connect and disconnect feature. This way of sending data is only used on L1 communication channels with very low error rate.\n\n\"Confirmed Conn.less Service\":\nEverytime data is send, the receiver sends an ACK flag, that he received it. If he doesnt send an ACK flag in a given time frame, the transmission times out and the data is retransmitted. There also is no Flow control or a connect and disconnect feature, duplicates and sequence errors can happen because of a retransmission. This service is used on L1 communication channels with high error rate.\n\n\"Connection-Oriented Service\":\n3-phased communication: 1. Connection(initializing the counters/variables of the sender and receiver) 2. Data Transfer 3. Disconnect. The data sent in setp two of this class is bidirectional, sequentiell and acknowledged, which means no data loss, no duplicates and no errors.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "29a6962e5edb4f7eb84919f23f7371ad",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "1. Unconfirmed Connectionless Service \n2. Confirmed Connectionless Service\n3. Connection-Oriented Service \n\nIn the 1. service, transmission of the data happens isolated and independent. A loss of data is possible. \nWith service 2. if the receiver do not answer, the data is retransmit after a certain time, so there are no loss. In both is no connection or disconnection. \nIn the 3. service, first a connection is initialized, data is transfered and then the connection is abort.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0bce05fc060743c395a867ff626f64ec",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Data Link Layer service classes:\n- Unconfirmed Connectionless Service\n- Confirmed Connectionless Service \n- Connection-oriented Service\n\nIn Unconfirmed Connectionless and Confirmed Connectionless services there is no established connection between the sender and the receiver, while in Connection-oriented service, the data is only transferred when a connection is established between the sender and the receiver. In Connection-oriented there is also a disconnect phase.\n\nIn Unconfirmed Connectionless service when the sender sends a frame, it hopes the frame reachs the receiver without a problem. The sender does not receive any confirmation, from the receiver, that the frame was received. This is not what happens in Confirmed Connectionless and Connection-oriented services. When a message is received on the receiver side, the receiver sends a confirmation message to the sender. So, in this two services the sender has the confirmation that the message reached the receiver.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0eb61c5ec96a42668d9d7915180a9e71",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service: You send data without a steady connection and without any feedback if the data arrived and if it arrived correctly.\n\nConfirmed Connectionless Service: You do not use a steady connection between sender and receiver, but you get a feedback whenever data is received.\n\nConnection-Oriented Service: You use a steady connection between sender and receiver. Each transmission process consists of 3 phases, at first you establish a connection then you send the data and at the end you disconnect.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "42debf189efc4d1abcd4235ef78cd2df",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "1.Unconfirmed Conn.less Service\n2.Confirmed Conn.less Service\n3.Connection-Oriented Service\n\nDifferences: \n1.Unconfirmed Conn.less Service and Confirmed Conn.less Service have no flow control. But Connection-Oriented Service has no flow control.\n2.confirmed Conn.less Service and Confirmed Conn.less Service have no connect or disconnect. But Connection-Oriented Service has connect or disconnect.\n3. Connection-Oriented Service has no loss, no duplication, no sequencing error. But Confirmed Conn.less Service has loss, duplication,sequencing. And Unconfirmed Conn.less Service has more errors than Confirmed Conn.less Service.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4cfc2482e48041adadfc1b99f764bf65",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed connectionless service (UCS), confirmed connectionless service (CCS) and connection-oriented service (COS). UCS doesn\u2019t have correction mechanism and flow control. The data probably gets lost. CCS reply ACK when receiving the correct data and it has timeout-and-retransmit mechanism. It is more reliable than CCS but probably incurs some sequence errors. COS has connection and disconnection mechanism. It can achieve flow control, no loss and no sequencing error.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e93cffe717804606b639e53e398e0c55",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "1.Unconfirmed Conn.less Service \n Features\n   No flow control\n   No connect or disconnect \n 2.Confirmed Conn.less Service\n Features\n   No flow control\n   No connect or disconnect\n   Duplicates and sequence errors may happen due to \u201cretransmit\u201d\n 3.Connection-Oriented Service\nConnection over error free channel\n  No loss, no duplication, no sequencing error\n   Flow control\n3-phased communication\nThey are different in features.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cf3ffd67be0c42f4ba6586833fd5a5d4",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "The three classes:\n\nUnconfirmed Connectionless Service\n-Transmits isolated independent units\n-Data units may be lost\n-No flow control\n-No connecting or disconnecting\n\nConfirmed Connectionless Service\n-Transmits independent data units \n-Receiver acknowledges the reception of each single frame\n-Timeout + Retransmission if sender does not receive acknowledgment within a certain amount of time \n-Thereby no loss of data, but duplicates and sequence errors may occur\n-No flow control\n-No connecting or disconnecting  \n\nConnection-Orientated Service \n-Transmits data over error free channel (through acknowledgments)\n-No loss of data, no duplications, no sequencing errors\n-Flow control\n-3-phased communication: connect, data transfer, disconnect",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2bcec1f99c4041e8aeb9ea7e11dc4ce4",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service: no flow control\nConfirmed Connectionless Service: no flow control, duplication and sequencing error may happen\nConnection-Oriented Service: flow control, no loss, no duplication, no sequencing error",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "238c575e0be14037bc76806354de8b05",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Here are the 3 service classes the data link layer offers: \n\n1) Unconfirmed connectionLess Service: there is no \"connect and disconnect\" between sender and receiver. There is no flow control or error management\n\n2) Confirmed connectionLess Service: There is no \"connect and disconnect\", there is no flow control but there is an acknowledgment for each frame sent. \n\n3) Connection oriented Service: there is a flow control and a \"connect and disconnect\" protocol between the sender and the receiver",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "91c288d2581a421da24379d13faf11c7",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service:\n-sends data without establish a connection\n-if data units get lost, didn\u2019t get any feedback\n-no flow control and no disconnect \n\nConfirmed Connectionless Service:\n-if data units get received, the receiver send an acknowledgement, so no loss\n-if sender does not receive an ack within a certain time, then retransmit, it can lead to duplicates and sequence errors\n-no flow controls\n-didn\u2019t establish a connection or disconnection with the receiver\n\nConnection-Oriented Service:\n-3-phased Communication: 1. Establish a connection, 2. Transfer data, 3. Disconnect\n-Flow control, no loss of data units, no duplication and no sequencing error",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5704b93caccb4a49bdb63196fcca5779",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "The three classes: \nUnconfirmed Connectionless Service\nConfirmed Connectionless Service\nConnection Oriented Service\n\nIn Unconfirmed Connectionless no confirmation of data transmitted is received, loss of data units is possible and also no flow control is present.\nIn Confirmed Connectionless  acknowledgement of data transmitted is received, no loss of data units because of retransfer and timeout mechanisms and also no flow control is present.\nIn Connection Oriented data is transferred over an error free channel, no loss of data units possible and flow control is also present,",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "046864f20b2346409a823479c57ed4a3",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "The 3 service classes are: Unconfirmed connectionless service (1), Confirmed connectionless service (2) and Connection-oriented service (3). The main difference between this services is the handling of the loss. While in (1) data packets are only send to the receiver, packets can get lost and loss is not being corrected. In service (2) data packets has to be acknowledged by the receiver and packets will be resend after a certain timeout. This leads to inefficient communication and can be done on a higher level. Service (3) consists of 3 phases (Connection establishment, data transfer, Disconnect). Just like in service (2) we have no loss, but Flow Control is possible in contrast to services (2) and (1).",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f326adeda6fd4a3faa2ae4bd3dd82b88",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "(1) \u201cUnconfirmed Connectionless Service\u201d: Transmission of isolated, independent units (loss of data units possible)\nis a good choice on communication medium with very low error rate\n\n(2) \u201cConfirmed Connectionless Service\u201d: Receipt of data units get acknowledged by receiver, therefore no loss, but duplicates and sequence errors may happen due to retransmit if no acknowledgement within a certain time.\nis a good choice on communication medium with high error rate e.g. wireless communication\n\n(3) \"Connection-Oriented Service\u201d: This service has flow control. Consists of 3-phases of communication, with Connection Initialization, Data Transfer and Disconnection. \nis a good choice if bi-directional communication is needed\nand is a good choice for an error free communication medium (therefore no loss, no duplication, no sequencing error)",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5634bcd481474cf3ad93fc38b0efba31",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Before sending data from Node A to Node B, A has to reserve a frame in the bus where A comes after B. Outer nodes are restricted to sending only in one direction, while nodes in the middle may make reservations in both directions and thus have a higher chance to get a reservation.",
        "answer_feedback": "The response is partially correct as the nodes located close to the generator have a higher chance of getting a reservation rather than the \"middle\" node. So unidirectional or bidirectional alone does not decide fairness.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "83d72740e0cd4a73b5b3245e67f46114",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with DQDB is that if we have several participants in the network and they want to exchange data, the distance between them and the propagation delay causes a fairness issue. This is because when stations are closer together, they can communicate over the bus faster than stations further away, this would still be acceptable for normal data packets, but network control packets are also sent over the bus and so network changes can spread much slower than in other architectures.",
        "answer_feedback": "The response correctly states the fairness issue in DQDB and also provides an appropriate reason for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bafcedfaaa274543a669ca42dd9473c6",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with DQDB is that there is a difference in fairness depending on the location, as not everyone has the same access to data.",
        "answer_feedback": "The response correctly identifies the fairness issue in DQDB which is due to the location of station.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8299940c5d894ef6be2d55097d49d953",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with the \u201cDistributed Queue Dual Buses\u201d is that it does not ensure fairness. The location of the node has an influence on its likelihood of gaining access to the data or acquiring the right to send, which results in an inequality between the nodes.\n\nAt the beginning of a bus all frames generated by the frame generator are empty. So the first node can reserve however many frames it wants. At the end of the bus it can happen that all frames are already reserved so the last nodes may not be able to send anything.",
        "answer_feedback": "The response is correct as it correctly explains the fairness problem with Distributed Queue Dual Buses.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c86fc1493d724e759af520ce0a9895af",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The Distributed Queue Dual Bus (DQDB) architecture uses two unidirectional buses for sending and receiving data. The main challenge here ist to guarentee fairness between all participating nodes as different nodes may have advantages (if at the beginning of the bus) or disadvantages (if at the end of the bus) in write access depending on their position in the bus.",
        "answer_feedback": "The response correctly states and explains the fairness problem of reserving transmission right in DQDB.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0156bb098aa841f199053d2f19a157e9",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Depending on the stations location in the network, they might be able to more easily reserve bandwidth on the BUS for sending data. Stations which are farther back will have less opportunities for reserving a BUS than stations at the front.\nThis can be fixed by introducing some formulas describing how often each station can reserve a BUS.",
        "answer_feedback": "The response is correct as it correctly states the issue with Distributed Queue Dual Buses.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bc31ae67273d4b01937cec7ed7d6de57",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem that was discussed in the lecture is fairness. The nodes reserve slots on one bus and send on the other bus. One node might reserve a lot of the available slots which makes it hard for the following nodes to reserve the space they need.",
        "answer_feedback": "The response correctly identifies the problem associated with Distributed Queue Dual Buses based on the station location.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b9ddc632e14a4bdcb9bb7d911bad52fb",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Distributed Queue Dual Buses have a fairness problem, meaning that dependent on the position of the node it will be advantaged or disadvantaged for certain comunications, as each bus only works in one direction and frames have to be requested.",
        "answer_feedback": "The response correctly states the problem in DQDB and gives an appropriate reason for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "27347428ce4c4a2fa1a3976a5e9ce2b3",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with DQDB Architecture is that the waiting time for a node to be allowed to send is heavily dependant on its location in the queue.If you are location on any far end of a bus (extrem right  or extrem left) then you will have to wait the most before you can send, if you want to send in the opposite direction.",
        "answer_feedback": "The response correctly explains the fairness problem with Distributed Queue Dual Buses.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2319ca9ff4794ed3b5972826def38e60",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Distributed Queue Dual Buses = 2 buses (transfer data in opposing directions), every node connected to both buses\n\nDistributed Queue Dual Buses is an architecture whereby every node is connected to 2 buses (write and read access). These buses are responsible for data transmission in opposing directions. The problem is because of transmission of data:\n\nBoth buses are connected to a frame generator which generate a fixed size frame every 125 milliseconds. Depending on the position of the nodes in the bus they can reserve the bus for sending data with a higher probability. E.g. for a node in the middle we have a probability of 50% to successfully reserve a bus. As a consequence, fairness is a problem due to the bus topology: Depending on the position of the node the node may be more or less successful in reserving a bus for data transmission.",
        "answer_feedback": "The response correctly explains the fairness problem of reserving transmission rights in DQDB.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e40c20fca4c248918443e31b4a4d9c17",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "The UDP header is a short header (only contains Receiver Port, Packet Length and optional Sender Port, Checksum). \nThe header of the TCP is more complicated. Additionally to Sender Port, Receiver Port and Checksum, it has a sequence number (to identify the segment or the starting sequence number). It also has an acknowledgement number (which is needed i.a. for the connection setup). In the TCP header you can also set flags (like FIN for the disconnection). You can also add further information in the options field.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers. Also, note that the ACK field is not just limited to connection setup.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "246d5668a09e462c977508f9c7a43889",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "Some additional features which are present in the TCP header are: sequence number, acknowledgement number, options, urgent pointer and flags (the TCP header contains possible additional information and has protocol specific services). The UDP header has a size of 8 bytes, while the size of the TCP header is at least 20 bytes.",
        "answer_feedback": "The response correctly states four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0f06ef0d39aa4a63a6bec495b4f04a0d",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "The UDP header includes the packet length (header + data) whereas the TCP header only includes the header length. The TCP header includes an acknowledgement number, advertised window and a sequence number which you do not find in the UDP header. The acknowledgement number states the sender which packets have arrived yet. The advertised window field gives the sender a feedback about how many more bytes the receiver will accept using the sliding window protocol. And the sequence number is necessary to be able to compute the packets in order.",
        "answer_feedback": "All the stated differences between a TCP header and a UDP header are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8fbd7350171b4115b1374641de2411ac",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "UDP: sender and receiver port, packet length, checksum, data\nTCP Headers are much longer than UDP Headers. There are some fields in the TCP header, which an UDP header doesn't contain:\n- Sequence Number\n- Acknowledge Number\n- Flags\n- Advertised window size\n- Options",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a9854db5434545dfa7a7bb51a022ee48",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "UDP-headers include:\n- source port\n- destination port\n- packet length\n- (optional to use) checksum\nEach of the fields is 16 Bit long (in sum 8 Bytes). UDP does not need much header-informations, since its a fast, connectionless protocol.\n\nTCP-headers also include a checksum, source and destination port, but also much more information, like:\n- a sequence number\n- an acknowledgement number\n- different control flags\n- the data offset\n- the window size\n- an urgent pointer\nThe much larger (min. 20 Byte) header is needed since TCP is a connection-oriented protocol, which sets more on reliability than speed.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0d4ab1eefc544820b71bc8bf554ba41f",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "UDP: Header consists of three mandatory and one optional header.\nSource-port, destination-port, packet length are mandatory, checksum is optional and just calculated for the header\n\nTCP: The checksum is calculated over header and user data, to ensure correct transmission.\nTo ensure reliablity, the TCP-Header has additionally fields for a sequence number, the acknowledgement number and certain flags to reduce/avoid congestion and enable flow control.\n\nThe TCP header is more complex but ensures reliable transmission at the cost of speed and use of bandwidth.\nThe UDP header just contains necessary information, is very fast but unreliable.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "860f32198bc54c88a467279608d34201",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "UDP headers do not include the followings: 1. sequence number 2. acknowledgement number 3. HL/RESV/Flags 4. advertised window 5. urgent pointer",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers. However, the terms HL and Resv should be properly named.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "294ee57d9f81437d9283465857d5e783",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "In TCP there is a Sequence Number field to identify packets individually for reliability. There is no Sequence Number in UDP. The UDP header does not have an options field, while the TCP header does. In TCP there is an Advertised Window field for the Sliding Window Protocol for Flow Control. There is no Flow Control and therefore no Advertised Window field in UDP. In TCP there there is only a Data Offset field that specifies the header length. In UDP the whole Packet Length is transmitted.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3cafbab1273d41f09b20d27b40912f6a",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "TCP has a Error Control. So the users can be sure, that all packages have been transmitted in the right order.\nTCP has an included flow control, to assure, that the two clients don't get an overflow of packages.\nMulitplexing: In UDP you only have on port at the receiver, where to send the data. For TCP you have to, one at each side.\nConnections are established and torn down in TCP, with the three-way-handshake. For UDP there is no guarantee the connection is established or closed.",
        "answer_feedback": "The response states general differences between UDP and TCP while the question requirement is to identify TCP and UDP header differences.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "1419baa6d7e34374b64f6c015e05bf30",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "In TCP the ports are at both ends.\nIn TCP it has error control, flow control, congestion avoidance while in UDP has only checksum.",
        "answer_feedback": "The question requirement is to identify the difference between UDP and TCP headers while the response states general differences between UDP and TCP.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "73dd79b9fafa49c9b83a744d1d386f36",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "TCP-Header has the following information in his header that UDP not have: \nSequence Number (to identify the lost segments and maintain the sequencing in transmission), \nAcknowledgment Number (to send a verification of received segments and to ask for the next seg-ments), \nUrgent (Used to point any urgent data in segment), Flags, Window size (Used to set the number of segments that can be sent before waiting for a confirmation from the destination), Options",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers. However, there is a slight correction of Urgent Pointer instead of Urgent.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "51be16f927014204866a3b9b8c3593cc",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "A UDP header has a length of 8 bytes whereas a TCP header has a length of 20 bytes. A UDP header has a field for the packet length, unlike a TCP header. A UDP header doesn\u2019t contain a sequence number, while a TCP header does. A UDP header neither contains an acknowledgement number but a TCP header has an extra field for that.",
        "answer_feedback": "The response correctly identifies and states the four differences between TCP and UDP headers except that the TCP header can be between 20 and 60 bytes long.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "cbf4789710d040c7859ea335a9224d9a",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "It\u2019s not realistic, because in the Real-World the high traffic in the internet depends often on the day and the time of the day. For example, weekend or holiday and morning, afternoon or evening. So, it can be that in the morning there are many zeros in the time slots and the evening there much ones, because for example everyone is watching Netflix in the end of working day or is doing some other internet things. So there can be more than one on\u2019s in time interval delta t.",
        "answer_feedback": "One can use a function instead of a constant to model the arrival rate to reflect such large-scale behavioral patterns like having more traffic in the evening. The arrivals would not depend on previous arrivals then, only on the time of the day, which is known. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1afdd7ac12e848fca02d8434504aaae2",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption does not hold in real internet traffic. In case of a video streaming service for example the packages are send in big burst. So the arrival of the first package does indicate the arrival of more packages and the more packages are received in a short time the more likely it gets that no more package will arrive for some time because the buffer for the video stream is full and the streaming service stops sending.",
        "answer_feedback": "The response is correct as it associates the probability of a packet arrival happening at a node with previous arrivals at the node. The extent of the probability of receiving a packet after receiving several packets depends on many factors and may differ from client to client.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f818f0264bc944a7ab500b7f47fd1a3d",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption does not hold true for the internet as when someone uses the internet he will continue using it for a certain time and not just have a single request and then nothing for a while. Also a lot of traffic is in a burst like nature so some requests until a certain buffer is filled and then again when it is somewhat deplenished. So in general the previous state or states can hold information for future states.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bc240e66eec147f58e7ac2e864b192cf",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption can not hold for real internet traffic because the underlying assumption of independence is false. Over a higher timescale the behavior of the user is undergoing changes. For example a user checks his mails in the morning for which packets arrive but then he goes to work and in that time no packets arrive. Another example disproving the assumption of independence is the on/off bursty traffic while watching videos. For some time packets arrive continuously and then if the buffer is full no packets arrive until the buffer is empty again and needs to be refilled.",
        "answer_feedback": "The first example in the response is partially correct because the arrival process' parameters can be time-dependent. That can model such intra-day variations like people going to work. Knowing previous arrivals no longer has to capture this information for us, thus making the inter-arrival times independent in this regard. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f3adf484a72b4fddad2e42a4917ee1b6",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, since the real internet traffic is complicated and there are dynamic behaviors of real-world service. For example, real internet TCP traffic has high burstiness and exhibits long range dependence properties at large time-scales. The arrivals for each interval are not independent and there are dependence and correlation of the traffic arrival process in the internet. Meanwhile, the real internet traffic has self-similar characteristics, thus, this assumption cannot hold for real internet traffic.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node. Take note that the burstiness is more a general nature of internet traffic, not just limited to TCP traffic.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "06d77aa7429b4764b3b2f04a89c18d01",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "Yes the assumption that time between packet arrivals are independent holds true for real internet traffic. Packets transferred between one node to another suffer from various delays such as propagation delay, processing delay, queuing delay, transmission delay etc. Each of these are independent of each other. Hence the arrival interval of packets which is a function of these delays is also independent.",
        "answer_feedback": "The response is incorrect because the real traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "0e58cbd3ed094438abe64299d02fcf79",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, because sometimes many users want to access the server at the same time, while at other times, only few request the server. For example a livestream of a football match: everybody sends requests to the server at kickoff, but only few do after the game (to watch the highlights). That means that the arrivals are not independent. They can depend on other events.",
        "answer_feedback": "The arrival depends on other events in such cases but the dependency can also be observed in the normal scenario when no such event is happening. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f7e347ca7aad4b6a97c0bad7aa8e1f69",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No this assumption does not hold for real internet traffic. Internet traffic is often very bursty, e.g. if we load a website we need to load a lot of resources at once, while we won't load nearly as much if we just look at the website. This means the probability that traffic arrives, if traffic arrived in the previous interval is greater than if there was no traffic in the previous interval.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5d0372661bde40b08f0e5ea4359a32a5",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, this assumption of the arrivals being \u201cmemoryless\u201d does not hold for real internet traffic. \nIf \u0394t = 1ms, for example, that means every of these time intervals has to be considered independent from each other. So in each of these intervals it is a \u201ccoin flip\u201d whether data is sent or not. \nObviously this is not true for real internet traffic because while streaming a movie or playing an online game, for example, the arrivals are connected and dependent on each other.",
        "answer_feedback": "The response is correct as it correctly associates the probability of an arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "56cf679b3132443d98235f15c1632aba",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, this assumption will most likely not hold true for real internet traffic. This has multiple reasons:\n\nPackets on the internet are grouped into frames for sending, making lone packets being sent separately rather unlikely.  \nThe nature of data transfer on the internet also makes lone packets very unlikely. When making a request for data through the internet (for example loading a web page), the response includes a lot of data (markup, text, images) which are all sent in a short amount of time, and after the page has been loaded the user will most likely spend some time browsing the page before making another request.  \nTherefore we cannot treat arriving packets as independent from one another, because there is a very high chance that an arriving packet is related to the previous packet.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ebe7b34d7e2144908f4cb6e63f97470b",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, it is not true for real time internet traffic. The arrivals of packet is not independent on time interval as while loading or streaming something the next video is loaded automatically if the previous video is about to end.eg. youtube,netflix etc.. this proves that it is not independent of the time interval.",
        "answer_feedback": "The response correctly associates the probability of arrivals at a node with previous arrivals. However, the example given does not illustrate this well because the next video's auto-load can be turned off in the application setting. A better example would be on-demand video streaming in general, as the traffic is bursty depending on the segment loading.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1eaca04860e44ee5b98b495ca47f5220",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption does not hold for real internet traffic. Real traffic is for example dependent on the daytime. Furthermore the application is relevant. Some might use bursty traffic.",
        "answer_feedback": "The response is partially correct because the arrival process' parameters can be time-dependent. In this way, the arrival rate wouldn't depend on the previous arrivals, but instead on the time of the day.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b3600d8e4f604d10a840010c8ec0b36b",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption is a simplification, that makes it easier to work and calculate with. In reality, there is seldomly only one packet is send and then nothing happens afterwards, but communication consists of multiple packets. Therefore, if there is one packet, then it is very propable, that there will be a lot following packets. I.E. if someone is streaming in the afternoon and causing a number of packets, the propability of the packets will concentrate on one timeframe. And this (and the behaviour of the other participants) causes the propability for a larger timeframe like the morning to be different then in the evening.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "56da012aa0ec4e9484848011001692fc",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "Yes. Because Internet traffic can be also modeled as a sequence of arrivals of discrete entities, such as packets, cells, etc. Mathematically, this leads to the usage of two equivalent representations: counting processes and interarrival time processes.",
        "answer_feedback": "The correct answer is \"No\". In real internet, the arrival of the packet at a node is affected by previous arrivals.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "8face0c9b01e4ab0a31e60ce262e142a",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "At the start, the bridges table is empty, it uses flooding for an unknown destination. During the backward learning process, the bridge works in promiscuous mode as it receives any frame on any of its LANs, then the bridge receives frames with sources address Q on LAN L, Q can be reached over L and therefore create table entry accordingly.",
        "answer_feedback": "The response correctly describes how transparent bridges build their bridge table. However, the response does not provide information on how the table is used during the forwarding process and what benefits this brings.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "bc0333d46f4a40ab8ecccfa236fbcdf2",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table stores the information, which station it can reach over which LAN (output line). The bridge works in the promiscuous mode, which means that it receives every frame of each connected LAN and during the backwards learning phase when the bridge receives frames with a source address S on a LAN L it \"learns\" that S can be reached over L and creates a table entry accordingly. These entries are associated with timestamps and updated when new frames were received from the source (e.g. S). To forward a frame the bridge will look at the source and destination LANs and drop the frame if they're identical (and therefore prevent unnecessary traffic) but if they are different the bridge can look up in the table to which LAN the frame has to be rerouted. Only if the destination is unknown the network will be flooded with the frame. Because the bridge is not visible as such for the other components of the network, these other components are simplified and they don't have to deal with the forwarding process.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "a0f9e11165c64bfe8aba92d16a678988",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "table holds infromation that a certain address can be reached by a certain LAN. During backwards learning the bridge updates its table by the incoming traffic knowing that the source of the received packet is reachable over the LAN form where the packet came. Look if the address is in the table if yes then send it to the LAN over which it is reachable if not use flooding. No longer need to flood if the path is known. Another one would be that it is a rather simple approach.",
        "answer_feedback": "The response answers all the four requirements of the question correctly. Also note that if the source and destination LAN is the same, the packet is dropped.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "47f747b961b14b90b27457550dcef882",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds the routing entries to forward packets to their destination. The table is initially empty and will be filled with the information of routes during the backwards learning phase. The table entries will be scanned and updated when receiving frames, thus it will adapt to changes in topology, which is a benefit.",
        "answer_feedback": "The response does not state which entries are present that are used for forwarding the packets. The backward learning process does not explain which packet information is inspected and used for building the table. The correct benefit is that there is less traffic because of selective forwarding, not just topological change adaption.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "5315d21e3fbe4b389134a50743810e81",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The invisible bridge contains a table which holds information about which address can be reached in which of the connected LANs around it. This table is initially empty, but then filled during the process of backward learning - when the bridge receives a packet from a LAN L with the sender address A, it can be concluded that A is part of the LAN L and therefore routable on this network. As the name clearly states, the bridge in the network is transparent as such, instead it is just addressed with the network receiver address by any senders in one LAN, so that it then can use its table to figure out in which destination-LAN the package should be sent. So one can conclude that the table prevents flooding from the transparent bridge and therefore unnecessary traffic. The other overall feature of the usage of a transparent bridge is the decreased complexity of transmission for all nodes in the combined LANs, because they can just sent packages to all nodes in all connected LANs without having to deal with the routing between the LANs by itselves.",
        "answer_feedback": "The response does not mention how flooding is done when there is no entry for a packet destination in the table, so flooding can not be prevented completely. Apart from that, the response is correct. It also additionally states the benefit of the transparent bridges in general, which was not required.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "79f192ddf4824d5d919b4f49a39bcb40",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "All bridges inspect all the traffic and build up tables (bridge tables), these tables hold information to manage the traffic and each entry contains an address and the LAN that leads to that address.\n\nThe bridge table is initially empty and uses flooding for an unknown destination. \nDuring the backward learning phase (Learning process) the bridge works in promiscuous mode and receives any frame on any of its LANs. If the bridge receives frames with source address Q on LAN L and Q can be reached over L, then it will create table entry accordingly. \nThese tables are adapted to changes in topology. Each entry is associated with a timestamp (frame arrival time), and the timestamp of an entry (Z, LAN, TS) is updated when the frame received from Z. \nThe table scanned periodically and old entries purged if no update for some time, usually several minutes (e.g., because the system moved and reinserted at a different position, or flooding was used if the machine was quiet for some minutes).\n\nThe main benefit of bridge tables in the forwarding process is to increase reliability by connecting LANs via VARIOUS bridges in parallel.",
        "answer_feedback": "The response incorrectly mentions the benefit of using multiple transparent bridges but the question asked for the benefit of using bridging information in forwarding frames. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "4c290bd0081f4d619305f53264c31c2d",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table contains mappings of station to the output line that has to be used to reach the station. The bridge uses promiscuous mode to observe the sent frames and if a frame from a specific source station is sent over a connected LAN the bridge knows that this station can be reached over that LAN and the bridge table is updated. In the forwarding process the destination is looked up in the bridge table and the frame is rerouted to the correct output LAN, if it differs from the current LAN. If the station is not found, flooding is used. A benefit of this setup is that the stations can transparently reach other stations in a different network like they were in the same.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "86e6b9ad11e843329076a97542f94ac7",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "This bridge table has MAC addresses and ports of bridge in it. At the very beginning, the table is empty, then for example, bridge sees that a frame on port 1 coming from source address A, it knows that A must be reachable via port 1, then it makes an entry in its table.\n\nBridge receives a frame, then it looks up the corresponding destination on its table, if the destination is found, and source address and the destination is identical, the frame would be dropped, if not identical, the bridge will forward this frame to its destination. But if the destination is not found, it will flood.\n\nThis table increases the reliability.",
        "answer_feedback": "The response states reliability as the benefit but it is not mentioned how the table usage increases the reliability. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ee695615d03e412c902731d6057cfe76",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "HIDDEN TERMINAL Because there no cable connecting every Terminal together it can happen that two or more station can not reach each other and therefor hidden. This can be a problem if for example a terminal has to be quiet so it doesn\u2019t disturb the communication of a neighbor but it can\u2019t get the communication request signal from the communication partner because it is hidden. This can be solved by using a busy signal or by listening to acknowledgments from the neighbor. NEAR AND FAR TERMINALS A signal from a station gets weaker with distance by the inverse square law. This lead to the situation that nearer stations are overpowering stations which are further away and drowning there signal. As a result stations which would normally be able to communicate with each other can\u2019t do so anymore. This can be a severe problem and can only be handled with precise power control.",
        "answer_feedback": "The response correctly describes the near and far terminal problem. In the hidden terminal, two stations cannot reach each other because they are out of one other's detection range, not just because they are wireless nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "2d892e47e54c4281971127dc3b171a4a",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. application layer - the discovery of services - you will need service awareness and need places for the services security\n\t - outside you can always be attacked, or the mobile routing could be disturbed.",
        "answer_feedback": "The response states application layer related challenges, but there is no clear relation to the wireless network routing challenges. The second point does not specify what is meant by outside. Even the optical fiber lines of wired networks are laid outside, even in seas.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "67f931a2deb04f80a80a22a0ec153a8d",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "In mobile routing autonomous systems are not stationary or in fixed location unlike fixed and wired networks. Autonomous system is free to come and join one network at one time and later leave and join another network while maintain same communication session between sender and receiver and vice versa. Two challenges: 1. Reconnecting sender and receiver when they try to connect through different intermediary networks while being on motion. 2. At user application level awareness by the sender that receiver has left, so save the user session, so that when receiver reconnects, sender is automatically notified and previous user session is resumed.",
        "answer_feedback": "Both challenges are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "44a81e9660c9442081383d1b307e26dd",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "- The network structure is changing fast, so the routing tables must be adapted to these changes. The routing algorithm needs to converge fast.\n\t - Because all nodes share the same communication medium (the ether), the signaling overhead needs to be minimized, to reduce the load in the medium.",
        "answer_feedback": "In the second challenge, it should be made explicit that the usable frequency is limited.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "ff2831704b4f463aa73963b3b5cdae51",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "CSMA/CD does not work for wireless transmission. Main issue is the broadcast nature of mobile transmission. * Hidden Terminal Problem: Firstly, carrier sense fails because a station that want to send cannot \u201csee\u201d another station already  sending to its destination as it is not in the transmission range of the other sending station. Secondly, there is no collision detection after the collision arised. This leads to a higher amount of collisions, a wastage of resources and unreliability. For example: * station A sends to station B; station C is not in the range of A, thus, does not receive A\u2019s signal * C performs carrier sensing as it wants to send to B, senses a free medium * C sends to B which causes collision at B; A cannot detect the collision (as it is a wireless scenario) * station A and C are hidden from each other\n\t * Exposed Terminal Problem: The \u201cexposed\u201d station is waiting to transmit a signal, as it hears a signal from another transmitting station. Thus, it tries to prevent a collision which actually will not occur as the receiver of the other sending station is outside of its range. This leads to underutilization of the channel and a decreased effective throughput. For example: * station B sends to station A; station C wants to send to another station outside of B\u2019s transmission range * C performs carrier sensing and senses a busy medium, thus it has to wait * A is outside of C\u2019s transmission range, thus, C actually does not need to wait as it would not cause a collision at A; C is exposed to B",
        "answer_feedback": "The response correctly states two challenges of mobile routing.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "21ee865c91f5410b9451eecebeaa2a8c",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Two of the many challenges of mobile routing compared to fixed / wired networks are Hidden Terminals and security issues. Hidden Terminal can occur, when the nodes are quite far apart, while some nodes are not able to detect nodes anymore, while more centered nodes are able to detect messages from both the distant nodes. Then the distant nodes are not able to detect collisions occuring in the \u201emiddle\u201c of the network at the centered nodes, because the signal is not transmitted over all network nodes. One of the security issues can be, that wifi is set up inside of a building. A normal ethernet network over cable would connect all the nodes inside, and then can be configured to discard all the internal packages at the outgoing router to the internet. A wifi network cannot be configured, to only nodes inside of the building are able to receive the packages. If the network is available outside of the building, then any node outside will be able to detect the network.",
        "answer_feedback": "Both the stated challenges are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f757b3c9e40447ea826faf8af3833fd3",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "(Due to the question in the forum, i will relate to slide 3, not to Challenges in Mobile Communications, which are on slide 10ff).\n\nOne basic challenge in Mobile Networking is the Power control: mobile devices have only a limited amount of power which should be used wisely and as little as possible.\n\nIn addition, the routing in Mobile Networking has to deal with a high amount of dynamic so it needs to find new routes as nodes move or conditions change.",
        "answer_feedback": "The response correctly states and describes two challenges of mobile routing.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "be964548e8ec49dc85c2b053425530c7",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Transport Layer, in which it writes an error detection and correction also increases energy efficiency.  Network Layer, in which it adapts routing protocols and multicast routing.",
        "answer_feedback": "The response doesn\u2019t answer any challenges of mobile routing.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "d0cebb8817eb47bc93fffb46565e5c56",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "- Hidden Terminal Problem:\nAssume we have 2 senders s1,s2 and one receiver r build like this:\n\ns1 \u2192 r \u2190 s2\n\nThe radius of s1 can just sense the receiver and s2 can also just sense the receiver. \n\nS1 is sending something to r. But since S2 cannot sense s1 it assumes the receiver is free and starts sending to r too. Hence s1 is hidden to c the collison detection fails \u2192 Hidden Terminal Problem.\n\n- Exposed Terminal Problem:\n\nAssume we have 2 senders s1,s2 and two receiver r1, r2 build like this:\n\nr1 \u2190 s1 --- s2 \u2192 r2\n\nNow s1 sends to r1. s2 wants to send to r2 but it gets the signal from s1 that it is sending data at the moment. Since s1 is sending to r1 and s2 can not sense r1, it assumes r2 is busy, and hence waits unnecessarily.  --> Exposed Terminal Problem",
        "answer_feedback": "The response related to the hidden terminal is partially correct as s1 assumes the medium instead of the node to be free. The description of the exposed terminal problem is partially correct as well. S2 senses the medium is busy and waits, not because it assumes r2 busy. The wait is unnecessary as S2 wants to send data to R2, and R2 is out of the range of S1.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "4839df0eb5ff405a8ec50f133e1c4323",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminals: Two nodes may be out of range for each other, but want to send to a third node that is in range for both of them. The first two nodes don't know when/if the other one is sending, so if both just send whenever they want, there might be a collision at the third node that can't be detected by the first two nodes. Exposed Terminals: When two nodes, that are in range for each other, want to send to nodes that are in range for them, but out of range for the other, they could, in theory, both send at the same time, because each of the receiving nodes will only receive a signal from the corresponding sending node (because the other one is not in range). However, without additional communication, the two sending nodes can not know that they aren't interfering with they other node. That's why one of the nodes will wait for the other to finish \u2192 underutilization.",
        "answer_feedback": "The response correctly states and describes the hidden and exposed terminal problems in wireless networks.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ac153b32c030408bb93cb0c791ed1b93",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "ADAPTATION OF ROUTING PROTOCOLS due to the inherent variability of a mobile network: the packets must be routed despite convergence problems due to frequent changes in topology and environment due to mobile nodes, i.e. frequent routing table updates, unreliable/unavailable links due to receiving/transmitting problems (near and far terminals, energy saving, insufficient transmission power), so THE PATH FROM SOURCE TO DESTINATION IS MUCH MORE SHORT-LIVED OR SUBJECT TO CHANGES MUCH MORE OFTEN THAN IN FIXED NETWORKS. * ADDRESSING (AUTO-CONFIGURATION): the automatic configuration of the end systems IS DIFFICULT DUE TO ROAMING and this obviously also affects routing. When roaming at Layer 2, the IP address can be retained, but the packet must be routed via an appropriate access point: if there is a handover, the old AP would have to forward the packets to the new one; when roaming at Layer 3, a subnet change may result in the assignment of a new IP address.",
        "answer_feedback": "The response correctly states and describes the challenges faced in wireless network routing.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "188fda241e5449849ad735cc128bff02",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "One issue for mobile networks is the hidden terminal problem: When there are two stations A and C out of reach of each other want to send to a station B in reach of both stations, A and C will not hear the signal of the other sender, assume the medium is free and start sending. However, at station B, both signals will collide. A second issue for mobile networks that cannot occur in wired networks is the exposed terminal problem: Sending from one station A to another station B might be blocked because of A receiving a signal from another sender C in reach of station A, despite the signal of this sender C not reaching the proposed receiver B. This issue can reduce the utilization of a link between two nodes unessessarily.",
        "answer_feedback": "The response correctly explains the hidden and exposed terminal challenges.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7dd8bf2d13634d3d9a06ffd276bfa041",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminals is one challenge in Mobile Routing. The problem is that nodes can only comunicate in a certain range and  those ranges overlap with others, leading to nodes receiving data from others, which do not know about one and another. This leads to data coalition. Another Challenge is Exposed Terminals. The problem again lies in the nature of the overlapping data transmission. To solve the previously mentioned problem only one can send at a time in their range, however this can lead to blocking communication to outside nodes which would not be effected by having communication with another one outside of the receivers range.",
        "answer_feedback": "The response correctly states and describes the hidden and exposed terminal problems in wireless networks.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5dfea8f265874ea6a1eb45545d00b997",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminals: Two nodes (A and B) that are out of each other's range can not detect transmissions from either node to a third node (C) which is within their ranges. This can cause A and B to simultaneously attempt to transmit to C. Hence, A and C are \u201chidden\u201d from each other. This leads to more collisions and reduced efficiency. Exposed Terminals: When two nodes are too close to each other it can interfere with their transmissions. When one node is transmitting, it signals to all other nodes in its vicinity that a medium (destination node) is in use and therefore the other nodes should wait before transmitting themselves. This becomes a problem when the other node in its vicinity has to delay its transmission to an entirely different node outside of the transmitting node's range. This leads to an underutilization of the channels.",
        "answer_feedback": "The response correctly states and describes two challenges of mobile routing.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "62773a7fc5a94d998e5fee8947003965",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1- Hidden terminals [Tobagi75]: it occurs when \u201cfor example\u201d we have three nodes A, B, C. Nodes A and C cannot hear each other, and the transmissions by nodes A and C can collide at node B. That makes nodes A and C are hidden from each other. And that can cause more collisions, unreliability as a result, and waste of resources.\n\n2- Exposed terminals: it happens when \u201cfor example\u201d we have four nodes A, B, C, D. Node B sends to node A, and node C wants to send to another node like D (not A or B), node C has to wait and it is prevented from sending packets to other nodes because of co-channel interference with a neighboring transmitter (medium in use). But node A is outside the radio range of node C, therefore waiting is not necessary, and node C is \u201cexposed\u201d to B. That can cause underutilization of channels and lower effective throughput.",
        "answer_feedback": "The response correctly states and describes the hidden and exposed terminal problems in wireless network.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4d79a4d4b8db41ac885a9c214311e999",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1.Hidden Terminals: it is when two stations simultaneously transmit data to one of the stations is unaware that reception is already receiving data from another station and a collision occurs at the receiving station. E.g.  We have three station A,B,C -A sends to B, C cannot receive A   -C senses a \u201cfree\u201d medium (carrier sense fails) , C sends to B   -Collision at B, A cannot detect the collision (collision detection fails)  -A is \u201chidden\u201d for C and vice versa  2.Near and Far Terminals: it's when a weak signal drowns out a strong signal. Terminals A and B send, C receives  -Signal strength decreases proportionally to square of distance  -Stronger signal of B therefore drowns out A\u2019s weaker signal  -C cannot receive A",
        "answer_feedback": "The response correctly explains the hidden and near and far terminal challenges.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d0ca8535cda14a3a9ee73730e5fb708b",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The Reverse Path Forwarding guarantees that the Packet used the best route when this packet arrived at the IS entry port. Reverse Path Broadcast is based on RPF  to suitable reduce of Overhead. RPF (for a packet arriving at an IS)  -Has this packet arrived at the IS entry port   over which the packets for this station/source are usually also sent?    Yes:  -Assumption:   Packet used the BEST route until now  -Action:     resend over all edges (not including the incoming one)     No:  -Assumption:   Packet did NOT use this route (it is NOT the best route)  -Action:     discard packet (most likely duplicate)  RPB: -Has this packet arrived at the IS entry port   over which the packets for this station/source are usually also sent?    Yes:  -Packet used the BEST route until now?    -YES: select the edge at which the packets arrived and from which they are then rerouted to source S (in reversed direction)  -NO: DO NOT send over all edges (without the incoming one),  i.e., not as in Reverse Path Forwarding (RPF)     No:  -discard packet (most likely duplicate)",
        "answer_feedback": "The response correctly explains RPF and RPB but it lacks the purpose. The purpose of both algorithms is to minimize the number of duplicate packets during broadcasting.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "e50cea44b9de4d4cb589aa238d077ee1",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding: Used for ensuring loop-free forwarding of multicast packets in multicast routing and to help prevent IP address spoofing in unicast routing.  Checks if the packet arrived at the IS entry port over which the packets for this station/source are usually sent. If packet is assumed taking the best route: resend over all edges (not including the incoming one). If packet is assumed not taking the best route: discard packet.  Reverse Path Broadcast: Used to check if the set of shortest paths to a node forms a tree that spans the network.  If the packet arrives at the IS entry over which the packets for this station/source are usually sent: Checks if Packet used the BEST route until now: if yes, select the edge at which the packets arrived and from which they are then rerouted to source. If no, do not send over all edges (without the incoming one). If the packet is not for this station/source: discard packet.",
        "answer_feedback": "The purpose of the RPB algorithm is missing in the response. Additionally, the purpose is not limited to unicast and multicast but instead used widely in broadcast too.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.9
    },
    {
        "id": "412d33d69aac410f8b3087780f682df9",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "-prevent routing loops/ cycles in the network   RPF: -each node has a route to every other node -each node only forwards a broadcast packet received from the same port used to send packets back towards the sender -so the packet is forwarded only if it comes from the same route that would be used to reply to the source   RPB: -improvement of RPF If the packet arrived at the IS entry over which the packets for this station/source S are usually also sent and packet used the best route until, then select the edge at which the packets arrived and from which they are then rerouted to source S in reversed direction, if it\u2019s not the best route then not send over all edges without the incoming one. -if not then discard packet.",
        "answer_feedback": "The response is partially correct as the explanation of RPF incorrectly states that \"each node has a route to every other node\" and also does not state over which links the packet are forwarded in RPF. The purpose and explanation for RPB are correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "19b8db883af147a093197307704ea30f",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Reverse Path Broadcasting are both algorithms used for loop-free multi- and broadcast communication and therefore aim to be more efficient than simple attempts like flooding or individual sending of packets to every destination in the network. The idea of both algorithms is the use of so called \u201cspanning trees\u201d for each individual node in the network, which contain routes to every subnode in every subnetwork. Since one intermediary system does only know his own spanning tree, but not the ones of the surrounding nodes, each router has to use its knowledge of optimal routing for certain destinations as a criterion for further transmission. For Reverse Path Forwarding this means, that each node has to check the link on which an incoming packet is received. If this link is the optimal one, over which packets for this station are also usually sent, then node can assume that the packet has taken an optimal way up to it. As a consequence, it then re-sends the packet over all of its edges, but not the one on which the packet was received. If the packet on the other hand does not come over such an \u201coptimal\u201d link it gets discarded.  Since re-sending over all edges seems not to be the most efficient way of multicast-routing, Reverse Path Broadcast introduces a further check for packets that arrive over the optimal link as well as a limited re-transmission of packets: If a packet has taken the optimal path until this station, the station reroutes the packet over the optimal incoming link to the Source in reversed direction. This means, that the receiving node looks up in its routing table over which link such a packet would normally be received. Exactly this link is then used for rerouting the packet to the source instead of using all links as it is done before in Reverse Path Forwarding. Otherwise (so to say if the packet is not received on an optimal link or received on an optimal link, but has not taken an optimal path so far), the packet is not resent. This mechanism is implemented to limit the number of duplicates in the network - while Reverse Path Forwarding retransmits over all edges (excluding the one on which the packet was received), Reverse Path Broadcasting chooses the most suitable link for retransmission.",
        "answer_feedback": "The response correctly explains RPF and RPB and their purpose.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "422aa9d84c3d4598bdfd10aa2e9f7a7f",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The PURPOSE of these two algorithms is the EFFICIENT (e.g. compared to flooding) execution of BROADCAST ROUTING, i.e. the sending of packets to all other hosts in the subnet. MODE OF OPERATION: Both are based on the idea of the Spanning Tree to use the shortest possible paths from the sender to the receivers. _RPF_ FORWARDS A BROADCAST PACKET arriving on one link over all other links ONLY IF IT ARRIVES OVER A LINK OVER WHICH, according to its own routing table, PACKETS ARE ALSO ROUTED TO THE SENDER OF THE BROADCAST (i.e. it is assumed that the packet arrived on the best path), OTHERWISE THE PACKET IS DISCARDED. _RPB_ differs from _RPF_ in the way it forwards packets: A PACKET IS ONLY FORWARDED TO A DIRECT NEIGHBOURING NODE IF THE FORWARDING NODE WOULD ALSO BE LOCATED ON A UNICAST PATH BETWEEN THE SENDER AND THAT NEIGHBOURING NODE. For example, if a node B receives a broadcast packet from A and B has links with C and D, but has previously learned that no packets are routed from A to C via B, but to D via B, then it will not forward the packet to C (unlike _RPF_), but only to D. This further reduces the number of messages compared to _RPF_.",
        "answer_feedback": "The response correctly answers the purpose and the explanation for both broadcast types.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9cc8281805254a9a96af9a6e26fa9d13",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "They are techniques to forward multicast packets in networks. They use information the IS has about the network structure (derived from normal unicast packets) to guess where to send the multicast packets. If a packet in RPF arrives over the \"usual\" path over which the sender sends, the IS will distribute(flood) the network with the packet. If the packet arrives not over the usual path, the packet will be dropped. In RPB, if the packet arrives over the \"usual\" path, the IS will send it over the path that unicast packets \"usually\" take and not flood the network. If the packet arrives not over the usual path, the packet will be dropped.",
        "answer_feedback": "The response is partially correct because it lacks the purpose of both algorithms which is to minimize the number of duplicate packets during broadcasting. In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.7
    },
    {
        "id": "553808e4ff0c4f1babd0dbeed2e75e52",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Reverse Path Broadcast are used in broadcasting, to enable loop-free by verifying the reachability of the destination. That way each IS will know its multicast tree RPF :  ALGORITHM : Check if the packet that arrived at the IS entry port over which the packets for this source are usually also sent. If yes, an assumption can be made that the packet used the best route. Then the packets will be resent over other edges (not including the incoming one) if no, assume the packet did not use the best route. Then this packet will be discarded. RPB : ALGORITHM :  Check if the packet that arrived at the IS entry port over which the packets fort this source are usually also sent. If yes, check if the packet used the best route. * If yes, select the edge at which the packets arrived and from which they are then rerouted to the source * If no, do not send over all the edges (without the incoming one) If no, the packet is discarded.",
        "answer_feedback": "The response correctly explains RPF and RPB and their purpose.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "24ed4eb8b0d14d6983dc6a812f63a919",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding: - Purpose: reduce traffic in broadcasting compared to flooding. In Reverse Path Forwarding, a sender only sends an incoming packet to all of its adjacent nodes if it has arrived over the edge that is considered to be part of the shortest path between that node and the source. Otherwise, the packet is ignored. Reverse Path Broadcasting: - Purpose: further reduce traffic compared to Reverse Path Forwarding. In Reverse Path Broadcasting, if a packet has arrived over the edge which is usually used for sending packets to the source, it is only forwarded to those neighbors, which usually route unicast messages to the sender via that node. So a router only spreads packets to a neighbor if it is on the shortest path between that neighbor and the source.",
        "answer_feedback": "The response correctly answers all three parts of the question. However, the purpose of minimizing the number of duplicate packets in the network is not explicitly stated in the response.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cdc11cf37050491db8db7f96c5470fe7",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding (RPF) and Reverse Path Broadcast (RPB) are algorithms that are used to distribute packets with more than one receiver in a network. Simple approaches are individual sending to every destination or flooding. These simple approaches aren\u2019t optimal for distributing packets to n receivers. In RPF each router has information which path it would use for unicast packets. If a router receives a package, it checks whether it received the package via the optimal route, and only forwards it to every other reachable router (except from the router it received the package from). In RPB however, packages are only forwarded according to the routing tables (via the best routes), thus reducing the load of the network.",
        "answer_feedback": "The response does not state why RPF and RPB are more optimal than flooding. The explanation for RPF is correct. The explanation for RPB is incomplete as the answer does not specify what \"according to the routing tables\" / \"via the best routes\" means.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "id": "df43125070a24576829ec9b68e51630b",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding is an algorithm to allow loop free forwarding of packets (especially for multicast). The source IP of an incoming packet is looked up in the routing table. If the packet would be send on this interface if the source IP would be the destination, the packet is forwarded on all edges but the incoming one. Packets that don't arrive via the shortest route may be ignored. Reverse Path Broadcast is also used for loop free forwarding and works similar to RPF. Though it does not send the packets out on all edged but selects those edges that are on the shortest path (in reverse) to the source of the multicast packet.",
        "answer_feedback": "The explanation of RPF is partially correct because the packets which did not use the best route will be discarded and not maybe discarded.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "55878e05a2bd4a05aa55f65155299bd2",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse path forwarding prevents multicast traffic from entering routing loops by looking up a table which holds all routers the multicast packet already visited. The packet is then forwarded to all routers that are not in the table. Reverse path broadcast is an extension of RPF: in this case packets are only forwarded to this interfaces, where the next router is on the shortest path to data origin.",
        "answer_feedback": "The response correctly identifies the purpose of RPF but the provided explanation is incorrect. In RPF, the packet used the unicast information stored in the routing table to check whether the broadcasted packet took the same route that it would have taken to send a unicast packet in the reverse direction. No explanation is provided as to what forms the shortest route in RPB.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.4
    },
    {
        "id": "db4955ce332b461187ac063a8e9234f6",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose is to reduce that overall network usage, and not produce unneeded traffic. You only send out the gotten packet if it came from a router you would route through to the sender, otherwise the packet gets droped as it can not be the optimal path.",
        "answer_feedback": "While both algorithms reduce traffic, the main purpose is to minimize duplicate packets during broadcasting. Also, it's unclear to which algorithm the given description is explaining.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "bc56a60a68e14da1bb5764a5d93edab5",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "In both,Packet used the BEST route with specific selection of the outgoing links. Reverse Path Forwarding:Packet used the best route and resend over all adjacent edges (not including the incoming one). Reverse Path Broadcast:Packet uses the best route and sends packet to adjacent nodes but select the edge at which the packets arrived and from which they are then rerouted to source in reversed direction and include the arrival node.",
        "answer_feedback": "The response explains the difference between the two correctly. The response does not state the purpose behind using the algorithms, namely to reduce duplicates in the broadcast. While both use the best route, how these routes are known is not explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.4
    },
    {
        "id": "1705402a8eb043c38b06e2765e60b390",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "PROPERTY: GLOBAL KNOWLEDGE OF THE MULTICAST GROUP\u2019S SPANNING TREE (MULTICAST TREE)  INITIALLY ONLY LOCAL KNOWLEDGE   ALL IS SEND LINK STATE PACKETS PERIODICALLY  -CONTAINING INFORMATION     DISTANCE TO NEIGHBORS     EXPANDED BY INFORMATION ON MULTICAST GROUPS -BY BROADCAST TO ALL THE OTHERS EACH IS CALCULATES A MULTICAST TREE -FROM THE NOW LOCALLY AVAILABLE AND COMPLETE STATE INFORMATION BASED ON THE INFORMATION ABOUT THE MULTICAST TREE - IS DETERMINES THE OUTGOING LINES - ON WHICH PACKETS HAVE TO BE TRANSMITTED",
        "answer_feedback": "The response is partially correct because it lacks the attractive property of spanning trees.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "a0df988fe6ab4e0c918aa3aa0ab0ddc8",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "A Spanning tree consists of a loop free topology including all nodes with minimum number of possible edges. It finds a minimal subnet and enables the network to minimize duplicates and reduce traffic. Modification of Link State Routing: The link state packet which contains information about the distance to neighbors can be enhanced by adding information on multicast groups. As the link state packets are broadcasted to all other nodes, every node is able to calculate a local multicast tree due to the fact that all nodes have the complete state information locally available.  Based on the multicast tree a node decides on which outgoing links a packet has to be forwarded.",
        "answer_feedback": "The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast and how the link-state algorithm is modified to construct a  multicast spanning tree.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "1cd35454bf2c417fa400fa017c12b825",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning Tree is known to the IS as, generates a minimum number of packet copies , that IS generates a copy of a packet for each required outgoing line and all spanning tree lines except incoming one have to be defined. It has to know the multicast basic principle, that all IS have to know the multicast tree. So all IS nodes send link state packets periodically.  The IS defines the outgoing lines and which packets have to be transmitted.",
        "answer_feedback": "While it is correct that a spanning-tree generates minimum copies of the message, it is not clear from the answer what the response meant by \"IS generates copy for each outgoing and spanning tree line\". The link-state modification for constructing spanning trees does not explain how each node shares its multicast information with others by adding it to the link state packet.  Each node then has the complete information to build a multicast spanning tree.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "f1c2ba021c854f7b8188648615453ff3",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "You use Reverse Path Forwarding with pruning. After the tree is set up the broadcast tree you know who belongs to the multicast. - If all child nodes aren't part of the multicast tree the parent knows it itself isn't part of the multicast tree. (Bottom Up) You can modify Link state routing by not only considering the \"distance\" between neighbors but also information on multicast groups.",
        "answer_feedback": "The desirable property is not because it makes Reverse Path forwarding possible. Instead, it is loopless and thereby reduces duplicates when broadcast- and multicasting. We do need to add the information to which group each IS belongs to in the link-state packet but it is not stated how it is propagated and used to construct the multicast spanning tree.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "8e71f33e7fc94aa794a4e049c8c33eba",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "the spanning tree algorithm determines the packets for the broad and multicasting While the link state packets will be sent, the containing information will be expanded by information on multicast groups - every IS calculate now its multicast tree",
        "answer_feedback": "Though it is correct that a spanning tree determines the path in multi-/broadcast, it does not answer why they are used. The reason is no loops in the spanning tree leading to reduced unnecessary duplicates.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "ec9d5d69e13e45b68381315abf9a8479",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "1. There is no loop",
        "answer_feedback": "The response lacks the explanation of the link state's modification to construct spanning trees. To calculate the spanning trees for multicasting, you also have to know which nodes belong to which groups. The link-state packets have to be expanded with multicast group information so other nodes can construct multicast trees by themselves.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "6668c253363c4fdda2cdaefcb2cb4048",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "The advantage of using a spanning tree for broad-/multicasting is that no duplicate messages are sent. This reduces network load while providing the exact same performance, only at the cost of lower reliability. If we want to construct a spanning tree using Link State Routing, each node, after having received the link state packets from all other nodes, calculates a spanning tree using the received information. The node will then use the connections from the calculated spanning tree to distribute multicast packets efficiently.",
        "answer_feedback": "The response correctly answers why a spanning-tree usage is ideal in multicast and broadcast. The explanation for modifying the link-state algorithm to construct a  multicast spanning tree for nodes does not state how a node gets to know about the other members of the multicast group and how this information is propagated to other IS.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "2a7786c9a8b44b709708c7f2cbfb0d70",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees have the following property, they connect all nodes in a graph with minimum possible edges. Since all nodes in the network are addressed by the source node that builds the spanning tree, an IS has to generate the minimum number of packet copies to broadcast or multicast to this sub-net. Each IS initially knows which multicast group it belongs to. This additional multicast information is added to the link state packets that are periodically sent out by the node. Once the complete state information is obtained, each IS calculates a spanning tree for multicast.",
        "answer_feedback": "The response correctly states the attractive spanning-tree property and explains how the link-state algorithm is modified to construct a  multicast spanning tree.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "69db1552796b4f099a632fa4f52fc3de",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Property: -no cycles / minimal path / connect only needed path for the transmitting   All IS send link state packets periodically, containing information about distance to neighbors and expanded by information on multicast groups and by broadcast to all the others. Each IS calculates a multicast tree from the now locally available and complete state information. Based on the information about the multicast tree IS determines the outgoing lines, on which packets have to be transmitted.",
        "answer_feedback": "The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast and how the link-state algorithm is modified to construct a  multicast spanning tree.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "97e3683c5df9481e984d95b5cd28587d",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Property: subnets of subnets can be displayed and addresses which enables more possibilities for multi-/broadcast for distribution of information\n\nmodification of link state routing for spanning tree multicast: \n\n- all IS have to know the multicast tree. \u2192which group belonging \n\n- Information distribution via link stated routing.\n\n- all IS send updates (link state packages) periodically\n\n\u2192calculate the own tree\n\n\u2192DETERMINE possibilities for transmission",
        "answer_feedback": "The desirable property is not correctly stated. Yes, a spanning tree is a subnet of the subnet, but what makes it unique is that it does not contain loops and thereby reduces unnecessary duplicates during multicast and broadcast. Additionally, the link-state packet needs to contain multicast group information so that each node can discover its fellow group member.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "3bb5caaf9fc845a788e22dacecf0e58b",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are located between header and payload. They can contain options or other information which extend the header. The main advantage of the extension headers in IPv6 compared to IPv4 is that they are optional. In IPv4 there is a part for the options reserved but in IPv6 when there are no options the space of the extension headers can be used for a longer payload. So the extension headers are more efficient.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The stated main advantage is incorrect as the option field in the IPv4 header is already optional, without any reserved space.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "ad0294d1afab4203af6120013a2a792f",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The header in IPv6 has a fixed length and is designed to be used for easy processing, it only contains information needed for routing. Any additional information is stored in extension headers. They carry optional information and can be found in between the fixed header and the playload. Since the whole IPv6 packet is only allowed a certain size, these additional extension headers take up space of the payload. The main advantage of extension headers is that they can be added optionally and help to overcome size limitation.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, the first stated main advantage is incorrect. The other advantage is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b5831bd49c404b738285de0805995658",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "- located between fixed header and payload\n- are optional, modularly including additional information, e.g. Routing information, Authentication, or Destination options\n- Ipv6 has a fixed sized header",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The IPv6 header has a fixed size but the response should state what advantage is concluded from this fact, so it is only partially correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1cc46e82e9e3414095585aabfa39689b",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers that can augment the main header. They are located between the main header and the payload. Their main advantages are that they cause less overhead since they can be omitted if not needed and that new headers can be added in the future.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional and could be 0 bits long, there is no added advantage over the IPv4 option field in terms of unnecessary overhead. The other advantage is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "031722f24ad4475d890ff8c7d7b60c93",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers in IPv6 are optional fields that cab specify additional options in an IP package. They are located between the actual IPv6 header abd the package payload. Compared to IPv4, they have the advantage of being more flexible as they are optional and can be used to add additional options to a package without being limited by the limited header size.",
        "answer_feedback": "The response correctly answers all three parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "af39577d2fad44869d982a2e538eaae6",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers are extensions for the normal header. You can support multiple addresses or specify more options for your header and packet, like e.g. authentication.\n\nThe Extension Headers are located between the normal header and the payload, they will be attached to the normal header. \n\nThe biggest advantage of Extension Headers is the possibility to use broadcasting.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The advantage is incorrect as IPv6 does not support broadcast.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "a2bbfe711a7344dbba4b91a06de1c86e",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers allow to append further options not covered by the fixed header and are located between the fixed header and the payload. \nIn contrast to IPv4 options the extension headers are entirely optional and can adapt to changing circumstances without touching the protocol itself.",
        "answer_feedback": "The response correctly answers all three parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "69948aa90551446ba6f6e0a19a44aedd",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are pieces of additional information that can be placed between the main header of an IP packet and the actual payload. They allow implementation of additional functionalities, for example predefining a static route through the network, but also information about fragmentation of larger packages. Therefore, they are the successor of the fragmentation- and options-field of an IPv4 header. In comparison, the new system with extension headers is much more flexible and adaptive to the wanted additions, because the extension headers are only optional, and also help to overcome the size limitations that were defined by the sizes of the fields in IPv4. What is more, the extension header idea is better for future developments, because new extension headers can be easily developed and attached to the existing system without changing the fixed header of an IPv6 packet.",
        "answer_feedback": "The response answers all three parts of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cd16a544e127457b96d6a4c5ab0bc218",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information.\n\nIf present, they are located between the fixed header and the payload data.\n\nThe main advantage of the extension headers compared to IPv4 is the simplicity and flexibility of their use: unlike IPv4, there is no limitation on the size of the option area (40 bytes) and in the future further extension headers can be specified without having to change anything in the IPv6 packet format. The extension headers allow, for example, to use the options from IPv4 packets (such as fragmentation) that are omitted in the fixed IPv6 header.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0b806031a4ac4cc5902f8313db52c68d",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional extensions to the fixed header. This provides flexibility to implement new features in the protocol. For such extensions, it is not necessary to change the fixed headers. Each header points to a next header and therefore this forms a header chain. The last header points to \u201cno next header\u201d and the payload, e.g. TCP or UDP, follows. Therefore, extension headers are located between the fixed header and the payload. Examples for extension headers are hop-by-hop options, destination options, encapsulating security payload (ESP) or mobility.\nAs a main advantage vs IPv4, optional extension headers provide a high degree of flexibility, overcome the size limitations of a highly predefined header in IPv4 and grant the potential for future extensions.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "594cfe7114424846a674304c793a9da8",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers in IPv6 are placed between fixed header and payload. The advantages compared to IPv4 is that these are optional, it helps to overcome size limitation and allow to append new options without changing the fixed header.",
        "answer_feedback": "The response answers the location and advantages of extension headers correctly and implicitly gives a description.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "746f2c892c2a4fc6ad7fb662b6d4e602",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers are additional information like Routing and Fragmentation for network device to decide how to process the IPv6 packet.\n\nExtension Headers are located between fixed header and payload.\n\nmain advantages:\n- optional\n- help to overcome size limitation\n- append new options without changing the fixed header",
        "answer_feedback": "The response answers all three parts of the question. However, the \"optional\" advantage is more a description of the extension header.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6f3aaf1a49454cf0ba12df7d36be88c8",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers located between the fixed header and payload. As they are optional, less data can be transferred by leaving them out. They also help overcome size limitations and allow appending new options without changing the fixed header in the future.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, there is no added advantage over the IPv4 option field in terms of the amount of transmittable data.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1353f030e8ca47b9acc95b482646ea99",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extensions Header in IPV6 contain supplementary information used by network devices (such as routers ,switches , and endpoint hosts) to decide how to direct or process an IPV6 packet and they are located between fixed Header and payload. The main advantage of extension headers compared  to IPV4 to allow to append new options without changing the fixed header.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "23b63cac4e474a86986d3d0db39aa0bd",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers carry optional header information which are important for IP routing.\nThey are placed between fixed headers and the payload in a packet.\nAdvantages compared to IPv4:\n-Extension headers help to overcome size limitations for options\n-Allow new options to be implemented without changing the header",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "891c052cb92f4220835e374f813586b8",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers contain of additional information like Routing and Fragmentation for the network device to decide how to process the IPv6 packet. Extension headers are placed between fixed header and payload. IPv6 options are placed in separate extension headers that are located between the IPv6 header and the transport-layer header in a packet. They help to overcome size limitation and allow to append new options without changing the fixed header. The main advantage is efficiency: Since there is no extra space for options between fixed header and payload there is a smaller header and therefore more space for payload. This way it is much faster.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The stated main advantage is incorrect as even the option field in the IPv4 header is optional. But as additionally stated in response it provides the flexibility to add extra options without changing the fixed header size. This is a correct advantage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "cde51d06259043508a447890068733d4",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers that store additional information.\nThey are located between the fixed header and the payload.\nA lot of the information that is stored in extension headers in IPv6 is stored the fixed header in IPv4. Because of that the 'space' for the information is always reserved in IPv4, even if you don't need it. In IPv6, because extension headers are optional, you don't have to reserve any space for this information, if you don't need it.",
        "answer_feedback": "The response is partially correct because the stated advantage is incorrect. After all, the options field in IPv4 is optional and also allowed to be 0 bit long, so there are no extra space benefits based on it.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "37d4ebacfeef4ffe9dbb139314bf59ec",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension headers are optional headers placed between the fixed headers and the payload. Using these, the header information is no longer limited in size (like in ipv4) and thus can be extended based on future requirements.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "07a237df6ae24853832056c9a102003b",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers allow to extend the new, simplified and fixed-size IPv6-header with additional options. These headers are located between the standard header and the payload (upper-level headers and user data). This approach allows to add several additional options without reserving space in the standard header for such optional data,allowing the standard header to be smaller.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. In IPv4, there is also no reservation of space for unused options. The main IPv6 header is most often larger than the IPv4 header in practice. Some benefits result from the main header having a fixed instead of a smaller size.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "c401425819c04cc89ea3d942c56a4a8e",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1. To support billions of end-systems\n2. To reduce routing tables\n3. To simplify protocol processing\n4. To increase security\n5. To support real time data traffic (quality of service)\n6. To provide multicasting\n7. To support mobility (roaming)\n8. To be open for a change\n9. To coexist with the existing protocol",
        "answer_feedback": "All objectives of IPv6 mentioned in the response are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4802d6cd6f1c4a2b9ae144b4ad3f8414",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "more/larger addresses; multicasting; mobility; better security; simplification of the protocol",
        "answer_feedback": "The response correctly answers objectives of IPv6.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5ca7b44fc77f45a586936e47d7636048",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Here are 5 objectives of the IPv6: \n\n-To support billions of End systems (2^128), much more than IPv4 (2^32).\n-To simplify the protocol. For example, the header of the IPv6 is much simpler than the header of IPv4. \n-To coexist with the older protocol. For example, thanks to tunneling, IPv6 can coexist with IPv4. \n-To add more security in the network. \n-To be open for eventual future evolutions (for example with the extension headers).",
        "answer_feedback": "The response is correct because all stated objectives of IPv 6 are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ba976438f48440799c252c67ce7abd1b",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "To support billions of end-systems\nTo reduce routing tables\nTo simplify protocol processing\nTo increase security",
        "answer_feedback": "All four IPv6 objectives in the response are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "02b00d7dd3c84c908d5978d23754de85",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "-support billions of end-systems (with specific addresses) \n-reduce routing tables\n-simplify protocol processing \n-increase security\n-support real time data traffic (QoS)\n-support mobility\n-be open for change in future with extension headers",
        "answer_feedback": "All the IPv6 objectives mentioned in the answer are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "068aa7ba40b64beaa24ba5ee36d0adcc",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "IPv6 is able to support billions of end-systems because it is using much longer adresses than IPv4.\nSince IPv4 is still very popular and the internet protocols version can't be switched instantly IPv6 have to coexist with other protocols.\nThe extension headers used by IPv6 enable changes in the future.\nTo simplify protocol processing IPv6 uses simplified headers.",
        "answer_feedback": "The response is correct as all the IPv6 objectives are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0e2da5ed945141d38a5383ea65a7180a",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1. Enlarge the available address pool:\n    By increasing the IP address length from 32 bits to 128 bits, a greater number of addresses can be assigned to end systems.\n2. Simplify protocol processing:\n    Any previous shortcomings in IPv4 can be removed and optimized in IPv6.\n3. Provide Multicasting:\n    Packets can now be sent to multiple destination addresses, which makes multicasting possible.\n4. Better Security:\n    Security means are already integrated in IPv6.",
        "answer_feedback": "The response correctly states four objectives of IPv6 with explanations.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "15fc35bc40b84666b36ec248279c3581",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "-Provide more adresses\n-simplify protocol processing\n-be usable while IPv4 is still in use\n-increase security",
        "answer_feedback": "All four IPv6 objectives mentioned in the response are completely correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "dd933fb12dca46a986b51b1b23109c0c",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter is reduced by the same factor, e.g. from 3km to 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d90988c14c5044588ba509f2752ca738",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter decreases by a factor of 10. That means the maximum distance between two locations on the network has to be 10 times smaller.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ddf912abac844e688f4c8626170572ba",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "this \u201c collision domain diameter\u201d will decrease by a factor of 10",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "988770907a0e4a7e8d1f07c3fb667189",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter will shrink with the same factor,so when the original speed is 10Mb/s and the collision domain diameter is 3km, an increase of the speed by the factor 10 to 100Mb/s will decrease the collision domain diameter to 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "497a3972819e4e7095afd2e7610ca1d0",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "the collision domain diameter have to shrink (divided by 10)",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0b6061ac8c204f589dd1d33c237d1aae",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The diameter gets smaller by the same factor.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "89e69798c61d4032b10920b2dfddbb66",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The maximum distance has to shrink by the factor of 10 and the LAN also has to get smaller which is not possible or at some point not feasible.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bfd96b84c1af4e5ea7d6d11d715d4b3c",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter decreases by the factor 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6f96bc68d1e74bccb0b93045762ffc24",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "For doing so you have to shrink the maximal distance between two locations.\nIn the given example the speed of the network should be increased by factor 10 from 10 Mb/s to 100 Mb/s. To achieve this without changing everything else, you have to reduce the collision domain diameter by factor 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4fd205cce98b41ff9fb2145b86e96211",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter will decrease by a factor of 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d9e9eafa5048492a9eeeb3d45c6650a9",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter will be reduced by the factor of 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "551f951cef7d43baaab337ad013ad07a",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter = 412m, i.e., ca. 300m instead of ca. 3000m",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "63475bedb7fa487ba24e8b839235cb7a",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The sender must still be able to detect collisions during simultaneous transmission and must also not exceed the maximum network extension. The collision domain diameter for 100 Mb/s is 10 times smaller than for 10 Mb/s if you use CSMA/CD => 3000m to 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9bfa0279d9904a8194c52908737f92d4",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "With a speed increased by a factor of 10, the collision domain diameter does decrease by a factor of 1/10 when using the same minimum packet size (e.g. 64 byte with Ethernet). This is because the send does finish much quicker (10 times as quick) while the time the electricity change needs to travel from sender to receiver and backwards remains the same.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "52814adc8a95436bbb0e548f4ecdb327",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "It is divided by the same factor of 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7793dec596f5428a916e39f47d81de37",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter will shrink, exactly by the same Factor Value we increase the speed of Transmission.\n\ni.e .. when we have with Transmission rate of 10 Mbps and a Distance of 3000m, then we gonna have by Transmission rate of 100Mbps just a distance of 300m between the locations the speed ist possible.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f4a8a5d7cc3946cf83736f228d7e1e6a",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting artificially increases the length of sent frames without adding meaningless padding by concatenating multiple frames, which are then sent together. Using frame bursting higher network speeds can be realized while maintaining the collision domain diameter that leads to better efficiency. \nOn the downside the latency for the frames is increased since they must wait for the next burst.",
        "answer_feedback": "The response answers all three parts of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7afa572127e1445da012acc8c5ddf5c9",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a transmission technique used at the data link layer of the OSI model, It can be effectively deployed in Gigabit Ethernets to increase network throughput. This is achieved by allowing a sender to transmit concatenated sequences of multiple frames in a single transmission.\nAdvantage: better efficiency than the carrier extension since multiple frames are sent in a single transmission. But carrier extension wastes bandwidth by sending a single frame at a time.\nDisadvantage: the end to end delay is increased because frames need to wait until the buffer is full or a timeout occurs and transmission happens. In contrast, carrier extension each frame would be transmitted alone without waiting for other frames.",
        "answer_feedback": "The response gives a correct definition of frame bursting, including its benefits and drawbacks.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b972f3409fa74d87af3f5e61155acbb7",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is concatenated sequence of multiple frames sent in single transmission. Advantage: When many frames are waiting at sender, it can be efficient. Disadvantage: When there are too less frames at sender, the sender keeps waiting too much before sending or at timeout adds too much padding data to send to receiver (inefficient).",
        "answer_feedback": "The response answers all the three parts of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "81770c906d204a5ab94cae777b4da652",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a feature of the shared broadcast mode. It allows the sender to transmit concatenated sequences of multiple frames in a single transmission. For example, we have eight packets and a buffer. We have to wait for all these eight packets to come and put them together on the line. If something is wrong, all of the eight packets have to be repeated.\n\nDisadvantage: If we only have to send one or two packets, we have to wait for eight packets to arrive, it takes a long time. So we artificially increase the end to end delay.\nAdvantage: It allows us to \"burst\" a sequence of packets resulting in higher throughput without abandoning the transmission medium.\n\nThere is a trade-off between efficiency and the end to end delay we are going to send. If we introduce end to end delay at the lower layer, the upper layer cannot do anything about it; it cannot speed it up anymore even if desired. So, thus, this is possible but not entirely ideal.",
        "answer_feedback": "The response correctly states the definition of frame bursting, including its pros and cons.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ec8c603b451f4ee097142d6abe1e760b",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is  a Transmission technique which is used to increase the Transmission rate of data Frames in the data link layer.\nAdvantage:the number of collision chances is reduced.\nDisadvantage:Frame Bursting does not address the primary goal of reducing the header Overhead.",
        "answer_feedback": "The response answer is incorrect as frame bursting is used to concatenate and transmit multiple frames in one single transmission to reduce the overhead of small frame transmission. Frame bursting offers higher efficiency in comparison to the carrier extension, but it increases end to end delay as the receiver may have to wait for frames.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "6927fec4250a4a14b25849257195974a",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a feature that allows for a higher throughput and efficiency. This works by allowing the sender to transmit a series of frames in succession in a single transmission without giving up control on the transmission medium. An advantage compared to carrier extension is that it has better efficency while a disadvantage is that it needs multiple frames waiting for transmission.",
        "answer_feedback": "The response answers the correct definition of frame bursting, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a600b2daa1194a6fa9426b622cf51050",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is one of IEEE 802.3z features of shared broadcast mode.It allows a sender to put several buffered frames together and transmit those concatenated frames in a single transmission to the receiver, without giving up the control over the transmission medium.\nThe advantage of frame bursting is the increased efficiency resulting from a higher throughput of individual data packets due to concatenation of single frames.\nThe disadvantage is, that this method can increase the waiting time of other senders (that are currently not sending) and the end-to-end delay.",
        "answer_feedback": "The response answers all three parts of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "18cff566fe384fa3a3b6782ab9c1ca63",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting allows the sender to transmit concatenated sequence of multiple frames in single transmission , this is a solution to the problem of the improvement of the speed and its consequences to the length of the cable.\nAdvantage: It is more efficient than the carrier extension.\nDisadvantage: with this you artificially increase the end to end delay, which can be a problem when the frame is critical.",
        "answer_feedback": "The response correctly answers all three parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c10fde93b7a745809fe7ce9e5e0b4250",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "The Frame bursting is one of the features of Shared Broadcast Mode. It consists in, on the sender's side, to concatenate a sequence of multiple frames in one single transmission. \n\nComparing to the carrier extension (the other feature of Shared Broadcast Mode), it needs frames to wait for transmission (disadvantage) but it has a better efficiency (advantage)\n\nThis will lead to higher among of collision or we should decrease the LAN size.",
        "answer_feedback": "The response is correct as it states the frame bursting definition, including its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fe374dfec2f649368d7cc34e57b23e7b",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "To be able to transmit data over lager distances at higher speed and still avoiding collisions you send bigger sequences by collecting several packets and sending them all together.\n\nAdvantage: You have a higher efficiency compared to carrier extension. You have a minimum of 70 percent (frame size 64 byte and user data 46 byte) user data compared to a minimum 9 percent of.\n\nDisadvantage: You have a delay in time while waiting for other packets until you have collected enough to send.",
        "answer_feedback": "The response is answering all three parts of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "378f95bc9e8d417999fa826387882927",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting allows the sender to concat frames in a single transmission to increase CSMA-CD distance.\nIt has a higher efficiency than Carrier extension (extending min. frame length),but frames are needed to wait for transmission. Also, carrier extension has a higher overhead.",
        "answer_feedback": "The response correctly answers all the parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "905d14b6f5aa416ebb3d170d4e38d8c6",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting allows the sender to send many frames concatenated.\nIf the station has several frames buffered and it has already sent a frame on the carrier, it can send the next frame directly after getting the acknowledgement from the receiver, without a dedicated time between frames.\n\nAdvantages:\n\"Frame bursting\": Higher speed, due to less waiting time between sending frames.\n\"Carrier extension\": Simple to implement\n\nDisadvantages:\n\"Frame bursting\": Requires many frames to be sent simultaneously\n\"Carrier extension\": Low efficiency",
        "answer_feedback": "The response is correct. However, to substitute carrier extension, frame bursting with frame aggregation has to be used. In this case, frames are directly concatenated without waiting for acknowledgments in between.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b51a0ae058d74107a8303b5730d76c52",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "The sender buffers a number of frames and concatenates them, so they can be sent in a single transmission.\nThe disadvantage is, that the end-to-end delay is increased, because the sender buffers the frames instead of sending them out as soon as they are created.\nThe advantage is that no \"rubbish\" data has to be sent, like with carrier extension, so the data efficiency is much higher.",
        "answer_feedback": "The response correctly answers all three parts, the frame bursting definition, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b6eda053c60a4a2495a70f742f0fd9da",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame Bursting allows sender to transmit concentrated Sequence of multiple Frames in single transmission.\n\nAdvantage:\nThe rate of efficiency is increasing in Transmission\n\nDisadvantage:\nFrames needs to wait for Transmission.",
        "answer_feedback": "The response gives a correct definition of frame bursting, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "11831c0a425b47e59edaf5f46a9d3c3d",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous: Each character is bounded by a start bit and a stop bit -> simple and  inexpensive, but low transmission rates\nSynchronous: several characters pooled to frames, defined by SYN or flag -> more complex, but higher transmission rates",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ddb9c673778b424fb0412be17d98afff",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "asynchronous: characters are bounded with a start and stop bit, the transmission rate is low with up to 200bit/sec\n\nsynchronous: characters are pooled in frames with a SYN or flag - this has a higher transmission rate and is also more complex",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "10adf6c3d77e48e0b26a01bb5e8e7121",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "In an asynchronous transmission each byte is sent separately and has a start and an end bit.\nIn a synchronous transmission data is sent in frames which can lead to higher transmission rates but becomes more complex.",
        "answer_feedback": "The response correctly answers the differences between synchronous and asynchronous transmission mode.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "06e1288db7514d54aa8525c221ed5146",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission sends single bytes which are bounded by a start bit and an end bit.\n\nIn comparison synchronous transmission is able to send a block of bytes (Frame). These blocks are defined by SYN or flag.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2c921d86adf443ecb6f1940b0c8246a0",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "In the asynchronous transmission mode, there is a start bit and a stop bit for every single character (byte) [ START | BYTE | STOP ].The synchronous transmission mode packs several bytes into a frame and a synchronisation flag is used to mark the begin and end of a new frame [ SYN | BYTE 1 | ... | BYTE n | SYN ].",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a7b574e1e7da48488f4614bd6d7c0daf",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous mode transmits characters separately and marks their boundary by using a start and stop bit, while synchronous mode groups multiple characters into frames where bounds are specified using control flags or a length field or invalid symbols of the physical layer.\nAsynchronous mode is simpler, but it's also slower than synchronous mode due to the increased overhead.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5949be86e48c404c82b27f4bbcaa9bdd",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "During asynchronous transmission each Byte of the transmission is bounded by a start and a stop bit. This makes it possible to transfer data at all time.\nWith synchronus transmission the sender has to wait for the reciever until he is ready, so a transmission has to start with SYN flags. After the SYN Flags all Bytes of the data can be transferred without being bounded with start and stop bits. This leads to a higher transmittion rate than with the asynchronus transmission.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f99bedf6dbc64850a4360a2e55a4f7ec",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "The difference is that in synchronous transmission mode the information which is to be transmitted is packed in frames, whereas in asynchronous transmission mode a single character represents a frame, which is bounded by a so-called start bit and a stop bit. So the asynchronous transmission have low transmission rates but is simple and inexpensive, in contrast the synchronous transmission is more complex but has higher transmission rates.",
        "answer_feedback": "The response correctly answers the differences between synchronous and asynchronous transmission mode.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c1c0383b4eba403696ac5c322c658dcd",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "While in asynchronous transmission every character is bounded by a start and a end bit, in synchronous transmission several character are bound to frames, these frames are bound by SYN or flag. The asynchronous transmission is simple and inexpensive, but has a low transmission rate, up to 200 bit/sec, while the synchronous transmission has a higher transmission rate, but is more complex.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cdc3b25e053b432ba57b3fb7e5139d38",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "On the asynchronous transmission, we transmit character independently from each other. Each byte is delimited by a start bit and a stop bit. \n\nOne the Synchronous transmission, whereas, several characters are regrouped in a \"frame\". \n\nThe Synchronous transmission is generally more complex but faster.",
        "answer_feedback": "The response is correct as it correctly answers the differences between synchronous and asynchronous transmission mode.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "63a39b1e3f974526b6fe2b51f54ee2d8",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Synchronous transmission sends data in a sort of blocks or frames.\n\nIn Unsynchronous transmission the data is sended in form of byte and character.\nStart and  Stop-Bits are added.",
        "answer_feedback": "The differences mentioned in the response are completely correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4c3208bb187f481397a34926f54e65c5",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "asynchronous: byte- and block-oriented\nsynchronous: character-, count- and bit-oriented",
        "answer_feedback": "The differences mentioned in the response are incorrect. The correct difference is: In asynchronous transmission, every character unit is surrounded by a start bit and a stop bit. In synchronous transmission, several characters are pooled into a frame.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "97c2cd368759432c89ba870284124eaf",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used on Internet Protocol networks whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on a network so they can communicate with other IP networks. It is used for simplified installation and configuration of end systems into network. Allows for manual or automatic assignment of IP addresses. May also provide additional configuration information like DNS server, netmask, default router, etc",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ceadc243bd3645e7b71489deabfd3ea9",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is used to manually/automatically assign IP addresses to physical devices inside of a network with the help of a DHCP server. A client with no IP address sends a broadcast DHCP discover packet to everyone on the network. The Server will respond with a DHCP offer, where he offers an IP address to the client. The Client will then respond with a DHCP request (telling the server he wants the IP address). The Server will assign the IP address to the Client for a certain time period (DHCP ACK). After this leasing time, the client has to renew the leasing of the IP address otherwise the DHCP Server will remove the IP address of the client again.",
        "answer_feedback": "The response is partially correct because the DHCP definition part is missing in the answer. There is an explanation about how DHCP works but that is not part of the question requirement.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "7ac51695bbfb40aca6be7d2fba653bc9",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a protocol to centrally manage the distribution of ip addresses in a network.\nThe DHCP simplifies the installation and configuration of end systems. Moreover, it allows for manual and automatic IP address assignment and may provide additional configuration information such as DNS server, netmask, default router, etc.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "da0cb1165e6643eba2946471fa8a85af",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The dhcp protocol is a protocol to configure systems that join a network.\nIt is used to assign ip addresses to systems within the network. \nIf a system joins the network it can ask the dhcp server for network configuration and an ip address that it should use in the future.",
        "answer_feedback": "The response is partially correct because it lacks a DHCP usage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "a0f00f0cd97a4e3d89cf3fa3937c3740",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used on\nInternet Protocol networks whereby a DHCP server dynamically assigns an IP address and other\nnetwork configuration parameters to each device on a network.\n\n\nDHCP simplifies installation and configuration of end systems, and allows for manual and\nautomatic IP address assignment.\n\nEnd systems can broadcast DHCP DISCOVER packets to retrieve their IP from the DHCP server.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f61d0dd942f64517b4b11ae1dfd2d350",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP has replaced RARP (and BOOTP) as it has Extended functionality. Its uses are: 1.Simplifies installation and configuration of end systems 2.Allows for manual and automatic IP address assignment 3.May provide additional configuration information like DNS server, netmask, default router, etc.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d503c59e2bee4093bf4a45fd7a8748e5",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a network management protocol which extends the functionality of RARP and BOOTP. DHCP simplifies installation and cofiguration of end systems. It also allows for manual and automatic IP address assignment.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "34ac6a4d78ce46a2a75e8e7d1ca49758",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol (DHCP) is Internet Protocol based on the special server that uses for manually or automatically IP addresses assignment and other network configuration parameters, such as subnet masks and default gateways, to each device on a network so they can communicate with other IP networks.\nThis server need not be on the same LAN as the requesting host. Since the DHCP server may not be reachable by broadcasting, a DHCP relay agent is needed on each LAN.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "542ffb3bc2074150a8ed1e8b08a9155f",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "It is used to assignIP addresses to hosts in a network.",
        "answer_feedback": "The response does not give a definition and does not specify how DHCP assigns the IP addresses, i.e. dynamically or automatically.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "85be48fe1fc74ffa8f8e08144e80248a",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a protocol that provides quick, automatic, and central management for the distribution of IP addresses within a network. It simplifies installation and configuration of end systems, also allowing for manual and automatic IP address assignment.\n\nIt is used for providing configuration information such as DNS server, netmask, default router, etc)",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f3ca8adfa8f94552af4bdba3ebf072b5",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a network management protocol which configures end devices on IP Networks (mostly LANs) by assigning them an IP address and other network configuration parameters.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The precise usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "c5f4942e4d10425b844db460a2edcd90",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "It\u2019s a network protocol used on IP networks to dynamically assign an IP address and other information to any device (host) on a network so they can communicate using IP.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The precise usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "cb53f4a90127434fa0bc19054c1ddef3",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP extends the functionality of RARP. It is used for automatic IP address assignment.",
        "answer_feedback": "The response only states the definition/description of DHCP correctly. The usage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "5863152a355e4f7da29ad1188586cc42",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol (DHCP) is a protocol designed as a replacement for the RARP and BOOTP protocols, with some additional functionality.\nAs such, it is is a protocol used for managing client IP addresses in a LAN.  \nWhen a client first joins a network, it sends out a so-called 'DHCP Discover packet', which is a way for the client to tell the DHCP server (usually the local router) that it needs a valid IP address.  \nThe DHCP server then responds, assigning an IP address and optionally some additional addresses (like the default netmask or router) to this host. The assigned address will be valid only for a certain duration specified by the host in the response.  \nThe client now has to renew its IP address (by sending out yet another DHCP Discover packet) before the assigned address expires.\n\nAs long as the address hasn't expired, it is safe for the client to assume that his current address is still valid, even after being disconnected from the network for some time.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8a0857f06e354ab6a16de2b9e999448f",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The DHCP protocol is used to simplify the configuration of ip adresses of end systems. It is used to dynamically assign ip adresses to the participants of a network, while still enabling administrators to configure ip adresses manually. Every end system is able to configure itself with the help of the dhcp-server. The end system sends a dhcp discovery broadcast, the dhcp identifies itself and afterwards they negotiate the ip adress, as well as other parameters like time server, name server, domain name and subnet mask.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "26fb0c1baccf4dd08561ae95c1cef731",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used on Internet Protocol networks whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on a network so they can communicate with other IP networks. A DHCP server enables computers to request IP addresses and networking parameters automatically from the Internet service provider (ISP), reducing the need for a network administrator or a user to manually assign IP addresses to all network devices.\n\nDHCP is used for:\nSimplifies installation and configuration of end systems\nAllows for manual and automatic IP address assignment\nMay provide additional configuration information (DNS server, netmask, default router, etc.)",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ce7fff8b7ce1467d8fb9178b30f18804",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The DHCP is used to add clients into a network by giving them the required information/addresses.",
        "answer_feedback": "The response partially describes DHCP as it lacks specifics, like what information is shared, to be considered complete and precise.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "b3933375086a42ab86b3aab0e85faa8d",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The requirement that has to be met that you can use the piggybacking extension to the sliding window protocol is, that we need the ACK field in the frame header that costs only a few bits. A seperate frame would need more costs: ACK, header and a checksum.",
        "answer_feedback": "The response is correct, a duplex connection is also required.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d224de1cd1e140b5a8ebdb899fcc48f8",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver. Therefore, the ACK for a data frame from a sender is sent in one frame with the next data frame issued by the receiver. Thus, the requirement for piggybacking is a duplex connection (and the need of sending an ACK).",
        "answer_feedback": "The response identifies the underlying requirement duplex connection correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8b92b2a5ffe7419a940b355abcecaed8",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Requirement: The interval of two adjacent frames, which are sent by sender, is short.\nSo that we can use piggybacking to response these two frames with one acknowledgement. \nThe communication has to be duplex (so the protocol must not be \"Utopia\").\nAnd the receiving buffer from the Sender must be ,so that it is able to store the ACK plus the additional data!",
        "answer_feedback": "The response contains a duplex connection as one of the requirements, but having to send two frames within short intervals is incorrect. Also, the same data and acknowledgments are tied together in piggybacking. Therefore, the total buffer space requirement should ideally remain almost the same as when they are sent separately.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "c0495c36eddd4ae0a7c5f0d424cc51ec",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "There must also be frames directed towards A (sender) in the transmission, so that B (receiver) sends frames back to A in a reasonable amount of time. In addition to that the amount of frames size of both parties must be similarly big, because the acknowledgement is added to frames directed at A. As a result there must be a certain balance of frames in both directions.",
        "answer_feedback": "The response correctly implies duplex operation. However, a balanced approach is difficult to achieve in real scenarios, so there are ways to overcome it, like a dedicated timer signaling a timeout in the absence of data to be sent.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "fc0c2f676d1d4dc4b608cae105153d16",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Need to know the initial sequence number, aswell as the next sequence number and acknowledgement.",
        "answer_feedback": "The response is incorrect as the above points are not specific to piggybacking but hold for the sliding window protocol in general.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "820606cc6ca44ec889e31a444b3fc1c7",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The connection has to be duplex and both sides have to have data to send (Otherwise the frame is 0 characters + the acknowledgement, which would just be a confirmation and no piggybacking).",
        "answer_feedback": "The response answers the underlying requirement correctly. Instead of sending the whole frame with no data, a separate acknowledgment can also be sent after a timeout.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "1c0cfab8391b493ea65d820e5844c218",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Both sides must send data to use piggybacking to be able to attach ACKs to data frames otherwise the sender will assume a frame loss.",
        "answer_feedback": "The response is incorrect as there will not always be data to send, in such cases a dedicated timer timeout is used to signal the absence of sufficient data and trigger the sending of separate acknowledgment.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "b30d3cc317514e79b068f0bf34429a76",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "To use the piggybacking extension to the sliding window protocol, we have to be in a duplex mode.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e79339cd4a2c489fa41275c9c89c16db",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "A receiver of a data frame has to send data frames the ACKs are piggybacked onto at a rate that is high enough so that the sender doesn't have to wait for too long for the ACKs to arrive. Otherwise a timeout might occur and the sender sends the frame again.",
        "answer_feedback": "The response states duplex communication indirectly but a dedicated timer timeout can also be on the receiver side to send acknowledgment separately when sufficient data is not present.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "a6a838beab144ea4a78fca00d53d7fee",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver cant differentiate between a new correct package or an \"old\" duplicated package, which leads to multiple data processing.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "769bf926ef0a4532bfe51092abe2d3fa",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "A receiver might not be able to distinguish a duplicate from a normal packet and so re-execute the given task.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "9bb4a26854b74f7b9549f74c7b8c046e",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The\u00a0duplicate packets\u00a0reduce effective tool bandwidth, waste tool processing power, and consume tool storage capacity, reducing their effectiveness.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "c56412c2ff7a44328d5f56be3b219a53",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver does not know which \"data\" it should use and will probably re-execute the transfer, which could lead to more network traffic.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "82d3f742a3d14a4cbb02021a425f0e9e",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Receiver might not be able to distinguish\u00a0between a real and a duplicate packet.",
        "answer_feedback": "The response is correct. The response can also state what will be the consequence in such a scenario.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "15c7bf2cd9bf4ea4a863ee5ec93e2922",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The duplicate packets is a a problem due to the reducing of the bandwidth and the decreasing of the efficiency.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "5e60964674ab47f4aab3611eb34496c4",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver cannot recognize the difference between correct data and duplicated data and would re-execute the transaction.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "38411c82398f408ca3a714f854f7b4da",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver may not be able to tell that the packet is a duplicate and re-executes an operation.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "6bf486834fb54ed69c2920b452253a60",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate packets can be false recognized as new data and then cause problems on higher network layers.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "1a0ef9584fb6475ba9c7e7067924453c",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver cannot differential between the correct date and the duplicated date.",
        "answer_feedback": "The response is correct. The response can also state what will be the consequence in such a scenario.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "08c7e79bf52c4b2a84167e1ed0cc0afc",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver receiving the duplicate might not distinguish it from a real packet and could perform a wrong (unwanted) action.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "032cfd7372b94561b21e08618bef14a4",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Yes because the receiver will think they are different and process them multiple times, which leads to unexpected result.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "86d18c40033d4b73b490ccc4a6ed9b95",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate packets in a network can result in a network overload if all senders would send the same packets multiple times.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "e01360ebc96748e499e02ef99ee129bb",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Besides increasing the traffic load duplicates can cause unwanted behavior when not handled properly.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "81ec790f64c84c958d6009daf4491083",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The\nreceiver cannot differentiate between a correct packet and a duplicated one and\nwould process it again.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "00f4bbf4e7c34628bb783fb468d1e0b9",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Network has varying transit times for packets, certain loss rate and storage capabilities, as well as\u00a0 packets can be manipulated, duplicated by flooding and resent by the original system after timeout.",
        "answer_feedback": "The response is not precise about the problem or consequences caused when duplicates are present.",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "b26aa057fbf3449a8e46f6c74fb0045d",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicates of packages could lead to the reinitiation of a transaction, which was already considered finished by one part of the communicating party.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "3c3311d464444e26a3a03199936995c6",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "duplicate packets will waste the bandwidth",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "67ed169fb3e4446fbe470b1c0e0eaf01",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "A receiver may not be able to differentiate between original and duplicated data and may so process wrong data.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "fc4af0675355436888dc0402a086eaa7",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Wenn der Empf\u00e4nger nicht in der Lage ist, zwischen g\u00fcltigen und duplizierten Paketen zu unterscheidenkann er auf dieselbe Information zweimal reagieren.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "387d91778a7e484f99e7be230b329b32",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicates (e.g delayed. from previous sessions) can't be distinguished from packets of the current transmission and may cause problems because they can't be handled properly.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "d38d98f3922f4d8a811ccd84e8e0a385",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The receiver cannot distinguish\u00a0duplicate packets from\u00a0correct packets so it would re-excute the transaction .",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "fd160d45b5c94dd78d12dbeec113c05f",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "If people doesn't use additional infomation to identify the packes, receiver cannot distinguish the first arrived packet and later pulicate packet, then it would re-execute the transaction.",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "a2d53b7213c643f99d773718d0b3c253",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Class A contains all IP addresses with first octet from 0 to 127 (IP address format is octet1.octet2.octet3.octet4)\n\nAccording to slide 45 of Internet Protocols:\nAddresses in range 127.0.0.0 to 127.255.255.255 are reserved, because they are used for loopback testing.\nAddresses in range 0.0.0.0 to 0.255.255.255 are reserved, because they refer to hosts of the current network.\n\n(So, excluding these 2 groups, each network with starting octet X, 0<X<127 of Class A has address X.0.0.0 for the network, X.255.255.255 for broadcasting and the rest addresses between these 2 for hosts).",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "27fef14816334f60884ba5b6655082e7",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "(0.0.0.0/8 Depends if 0 is countes as a class A network): Current Network\n10.0.0.0/8: Private networks\n127.0.0.0/8: Loopback\n\nQuelle: RFC 5735",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "7d6e69ce3bbc40feb3dffbdd9b8cfb81",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 to 0.255.255.255\n10.0.0.0 to\u00a010.255.255.255\n127.0.0.0 to 127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "19adc8612d374467bf857c5b8ebf07bd",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0/8: current network adresses\n127.0.0.0/8: loopback adresses",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "685a888ecb2543beb70144ecd3c41a24",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 - 10.255.255.255172.16.0.0 - 172.31.255.255127.0.0.0 - 127.255.255.255",
        "answer_feedback": "172.x.y.z is not in Class A anymore",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "09b65d0c0ece4515b1ffa27036ce3ca1",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 to 0.255.255.255: current network\n127.0.0.0 to 127.255.255.255: loopback adresses",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "d2e912872ef24e8a8f24b133361c8eaf",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "127.255.255.255 (Broadcast)\n1.0.0.0 to 126.0.0.0 (Network ID)",
        "answer_feedback": "Please watch your notation: 1.0.0.0 - 126.0.0.0 does not mean, only addresses with .0.0.0, but every address in this range, for example 13.8.255.4, tooBroadcast is x.255.255.255 with x between 0 and 127Missing: Loopback",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "7533404600ce43549f5e89a6e412effb",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "127.X.X.X (127.0.0.0\uff5e127.255.255.255)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0eded6d918f94287ae0a1da94abaed5b",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 bis 10.255.255.255 private netze",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "a430d8c276534b3c9732f5d7228c26af",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Network Addresses:\u00a0\u00a0\u00a0 [0-127].0.0.0\nBroadcast Addresses: [0-127].255.255.255",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "ec44176ed8854a2bbfcfb170a5413969",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Just to note: originally I understood the task to mean reserved addresses within the network (i.e. concerning the host part), but then there was an official statement in the forum that the network part was meant. If it is indeed the host part, then *.0.0.0 and *.255.255.255 would be reserved within each Class A network.\nReserved addresses in Class A networks:\n0.0.0.0 - 0.255.255.255 (host at this network)10.0.0.0 - 10.255.255.255 (Class A private network range)127.0.0.0 - 127.255.255.255 (local host)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "7678409f55594442991350d8cf6d149c",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "from 0.0.0.0 to 127.255.255.255",
        "answer_feedback": "Not all addresses in Class A are reserved",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "0f91d1bc40f44515abadfc1004207fe4",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "IP : 0.0.0.0\nIP Range: 127.0.0.1 to 127.255.255.255 are network testing addresses (also referred to as loop-back addresses)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "1758098f23c44449a54c33c026091622",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "IP Ranges:\n0.0.0.0 - 0.255.255.255\n127.0.0.0 - 127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "7e09091a6fd94bb4b0d8e2f282032e73",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "127.0.0.0~127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "c351b68c5a18403eaf08a5869bd891b1",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0-0.255.255.255\n10.0.0.0-10.255.255.255,\n100.64.0.0-100.127.255.255\n127.0.0.0-127.255.255.255,",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "ae8d3e3dba054d43a74497e83751e35e",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "All 0 -> 0-127.0.0.0:\u00a0 Network Address (or excluding 0.0.0.0 and 127.0.0.0 in respect to reserved addresses according the IETF Special-Purpose IP Address Registries from RFC 6890)\nAll 1 -> 0-127.255.255.255: Broadcast Address\u00a0(or excluding 0.255.255.255 and 127.255.255.255\u00a0in respect to reserved addresses according the IETF Special-Purpose IP Address Registries from RFC 6890)\nAdditionally the following Class A Network (Parts) are reserved according to RFC 6890 https://datatracker.ietf.org/doc/html/rfc6890#section-2.2.2\n127.0.0.0/8 loopback adresses\n10.0.0.0/8 Private-Use\n0.0.0.0/8\u00a0 \u00a0\"This host on this network\"\n100.64.0.0/10\u00a0Shared Address Space",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "efac355536c54fd5b24e27e6da35f850",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "For each of the 2^7 = 128 networks the first and the last address are reserved.\n- Network address (all zeros)\n- Broadcast address (all ones)",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "30cf1d8d5dc54360a3e64cac3bb35941",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "xxx.xxx.xxx.0\n\nxxx.xxx.xxx.255",
        "answer_feedback": "Missing: LoopbackNetwork and Broadcast are x.0.0.0 and x.255.255.255 with x between 0 and 127",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "e55dc52646b24861bf1f92634c6c9777",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.x.x.x -> network address\n127.x.x.x -> Loopback\n(x.0.0.0 -> Gateway in all Types of network)\n(x.255.255.255 -> Broadcast in all Types of network)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    }
]
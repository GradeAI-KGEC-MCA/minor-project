[
    {
        "id": "42a0f468c13e47bf86e5b4a7ccb0a07d",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Due to the expected channel load being high, there is not necessarily the bandwidth available to coordinate access to the channel. Thus a medium access procedure based on random access should be chosen. With random access and a high channel load also the problem for collision arises and handling of collision detection becomes necessary.\nI would recommend using CSMA/CD as it has (1) a low overhead and (2) utilizes the bandwith to its fullest when possible. One weakness of CSMA/CD is the edge case of possible long waiting times when short frames are transmitted of very long distances which renders the built-in collision detection essentially useless.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "46e8a9c5c80b420cb80b222df30db224",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would choose CSMA/CD. You can easily add up Stations, it is cost efficient and has practically no waiting time during low utilization. On the Downside, if distance increases, efficiency of CSMA decreases.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "1757f48dd39b4ba9acd91badebf23d07",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD saves time and bandwidth. Furthermore CSMA/CD is used for ethernet so it is really compatible to a lot of products. The usage of CSMA/CD with p-persistent CSMA would be nearly perfect, so the channel usage would be very high. A problem could be the maximum distance to a station.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "62fbc1e7f8ab4dfa8d3d470147b77a1d",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would use Token Ring, because it has good throughput even during high utilisation, which is to be expected at 20 systems and it can be expanded later, as it supports a maximum of 250 stations. But you need a central monitor.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws! Why is a central monitor a disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "id": "173c3af77e8247148277418d53926124",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "p-persistent CSMA.\nBecause it make full use of the channel's free time, it can relieve the pressure of the hardware. On the other hand, it has less collisions at high load.\nThe weakness is, it is difficult to set the p-parameter.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "fd66d3b8808d44759c9386c3e1d99281",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend TDMA with reservation, since the throughput is especially good in the contention-free period after sending the reservation requests because the data can be sent continuously without collisions and the channel is fully used. \n\nFurthermore because of the reservation concept the provisioning of the line is highly adjustable because only stations with a reservation request use the channel which is extremely good for a network which should be expandable. \n\nA potential weakness is that the stations need synchronized clocks which leads to an overhead.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "1af2de9481fa471ebf4b5280fcbddb8d",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend using CSMA/CD. Carrier Sense is useful when the channel has a high load since the necessary overhead gets really small in comparison to the amount of prevented collisions. CD is also useful since it reduces the wasted time after a collision which is important since high load means more collisions. The weakness of the procedure is that it scales badly with range because the maximum range is dependend on the frame size and vice versa.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "6b4ac0dc4d5a408eb6dcb804dabca637",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "My choice would be CSMA/CD since it enables the ability to add up stations easily and also has almost no waiting time in times of low utilization. \nOne downside of CSMA/CD is, for increasing distances, the efficiency is deacreasing.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "2f5e9125fc6642fcb1b9def6b1cae771",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "non-persistent CSMA should be used. Reason 1: It is good for the Networks with a high load medium. Reason 2: it offers a good throughput. Weakness: The delays are longer for each single station.",
        "answer_feedback": "What is the difference between reason 1 and reason 2?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "id": "f6589ab387a643a5841243e5cf794151",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "For this scenario I would recommend TDMA with reservation, since it can provide a high throughput in the contention-free period while offering a more flexible provisioning with the reservation.\nDue to the use of slot reservation, this MAC can also provide good service in an expending network.\nTDMA needs synchronized clocks on all systems for the time slots to work.\nEven though the reservation window is contention based possibly leading to collisions, this section makes up only a small fraction of the transmission process.\nHowever, if in the reservation phase to much collisions happen, the slots will stay unused meaning a low utilization.\nIn this case a p-persistent CSMA (/CD) would be a better fit, while p depends on the current load and number of stations and whether Collision Detection should be used, depends on the cable length.\n\nReasons against the other protocols:\nPolling would need a centralized controller.\nTDMA without reservation possibly wastes slots for systems, that don't have anything to transmit, and isn't as flexible to an increasing network size.\nA token ring is expensive and only provides low throughput.\nALOHA in both forms doesn't provide good throughput in a high load network, since many collisions will happen.\nIn CSMA networks with high load have also a poor throughput due to collisions.",
        "answer_feedback": "Then why do you not choose p-persistent if it is better? Correct though",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "ef6f8a313732462d9d5f4eef6cdb67ab",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "personally i would recommend token ring\npro:\n1. the token ring can guarantee a good throughput during high utilization.\n2. even all stations wants to send at the same time, the maximal waiting time is fixed.\ncontra:\n1.it required a central moniter to avoid lost token, and costs more when adding new stations.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "6ee88d77e8a84d1b8c8f7773af05cad5",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would choose a Token Ring MAC due to its great throughput during high net utilization and deterministic maximal waiting times. However, delays may occur while waiting for a token and it being a bit more expensive than CSMA.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "655e8c396328431eb8c629f90024a6ea",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would suggest using the non-persistent CSMA procedure. First of all, a procedure with contention should be used to provide the fastest data transmission for the possibly high amount of devices in the future even with a high channel load.\n\nWith the non-persistent CSMA, the hardware of the sender does not have to recheck the network continuously, which is good in order to reduce the load of the not-so-powerful device(according to the given info).\nSecond, the performance of the throughput is the second-highest even for a large number of attempts per packet, which is good for a highly loaded channel.\n\nProblem: There may occur longer delays for the single devices than necessary. In Example: the network was congested, the random timer is started and awaited, even though the network was free again immediately after the timer was started.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "af571dfe226a458e9cfabc89cf7f2622",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I recommend CSMA/CD. due to \n1.cost efficient\n2.practically no waiting time during low utilization\n\npotential weakness :if all 20  systems try to sending\uff0cthe utilization will be lower.  And as  the number of systems increases, the performance will get worse.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "5f4acfd78521424994091ab9343eadcb",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I recommend using the p-persistent CSMA procedure. As the channel load will be high, the company would profit from the high efficiency in terms of the overall throughput of this procedure. With a low p-value, the p-persistent CSMA could even reach perfect throughput efficiency. Also, adding additional systems to the network is simple, as the single participants in this procedure do not need to be aware of each other and it the procedure works completely decentralized. This increases the extendability and maintainability with extending scale. However, one drawback of this approach is that it introduces the p-value as an additional fine-tuning parameter that needs to be adjusted to fit the application scenario at hand, which introduces additional complexity.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "d2c4d4e3a6524f12a7fdd2e1b937cc5f",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend the company to set up their LAN using CSMA/CD. I see this medium access procedure as the most suitable for this scenario because it is cost efficient, which is great if the funding is tight, and also it is possible to connect new stations in the future without any downtime. A weakness of this procedure however is that the more the system is used, the more collisions occur and therefore, the throughput decreases. Accordingly, in addition to my recommendation I would tell the company to monitor the throughput as more stations join the system. In the future, they should consider changing to Token Ring at some point as the throughput with this method performs better with more stations.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "b094a822c2af48039f817adb3c30c37b",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD\nrecommand reason:\n1. the CSMA/CD can reach high speed to transfer the medium.\n2. CSMA/CD can deal with collision\n\npotential weakness:\ncan not avoid collision and need to listen the link while transferring the medium.",
        "answer_feedback": "Why is dealing with collisions an advantage? And why is listening to the link a disadvantage?",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "d0265aa347a848f48108d7769b18394c",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Tokenring wouldn't be impaired by the high channel load and can be easily expanded. Depending on the traffic and number of computers connected time between sends may be long though.",
        "answer_feedback": "Extendability might be a strong suit but it has its flaws and it is not easy to expand!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "id": "4acd9c8f1d254351bb04a5300a26774c",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend using either the non-persistent CSMA or the p-persistent CSMA with a low value for p (e.g. 0.1 or 0.05). First of all both are cost efficient (budget is tight) and using these specific CSMA procedures also guarantees a good throughput during high channel load. In addition to that adding new systems to the network is no problem since the network does not have to be shut down to connect additional stations to it. \nOne downside of this procedure would be the higher delay for the single stations.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "863243636908428389f895d8da993f31",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Using Token ring would be a good solution, as it is easy to implement (in terms of hardware) and it provides fairly distributed medium access even in high load situations. A weakness is the ring topology that requires a newly added system to be connected to two other systems.",
        "answer_feedback": "Well, there are other solutions, that would be easier to implement then token ring (Aloha for example)",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "19c305caf7934550b2a9211b47b62728",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I will use Token Ring, because it has random frame lengths and good throughput. That's why Token Ring is expandable.\nBut there is delay because of waiting for token.",
        "answer_feedback": "Random frame length and good throughput do not lead to extendability!\u00a0Extendability might be a strong suit but it has its flaws!",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.86
    },
    {
        "id": "98b2288ecc044472b0a553400e568c7e",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "Ich w\u00fcrde TDMA mit Reservierung vorschlagen,da die Auslastung bei dem konkreten Beispiel hoch ist und es weniger Kollisionen gibt, was insgesamt f\u00fcr einen h\u00f6heren Durchsatz sorgt. Au\u00dferdem soll, wenn mehr Systeme dazukommen, die Wartzeit einzelner Systeme durch ungenutzte Slots nicht zu hoch sein. Daher ist es von Vorteil den TDMA mit Reservierung zu benutzen. Ein Nachteil von TDMA mit Reservierung ist, dass durch die steigende Anzahl an Ger\u00e4ten die Anzahl der Contention Slots steigt, was zur Folge hat, dass das mehr Netzwerkkapazit\u00e4t den Contention Slots zuf\u00e4llt.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "08b2c295cb4242cc9b6c1658db1be22d",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "I would recommend non-persistent CSMA because it is expected that multiple stations want to send at the same time (high channel load) and it provides good throughput even during high utilization. One weakness of this MAC procedure is, that they are long delays for single stations.",
        "answer_feedback": "What is another reason? You only gave 1",
        "verification_feedback": "Partially correct",
        "max_score": 3.5,
        "normalized_score": 0.71
    },
    {
        "id": "2ea4471f0e7f4f21a96924552f71a5d6",
        "question": "A company is planning to set up a new LAN at one of their locations and is looking for an appropriate medium access procedure. However, the funding is tight so they expect the channel load to be high compared to the hardware they can provide. Currently, they would like to have 20 systems sharing the channel, but it should be expandable later on. Which of the MAC procedures introduced in the lecture (Polling, TDMA with or without Reservation, Token Ring, Pure or Slotted ALOHA, 1-persistent CSMA, p-persistent CSMA, non-persistent CSMA or CSMA/CD) would you recommend?Give 2 reasons for your decision and 1 potential weakness of your recommendation in 2-6 sentences.",
        "reference_answer": "0.5 P for a sensible choice out of: Token Ring, p-persistent or non-persistent CSMA, CSMA/CD or TDMA with reservation 1P for the drawback and 1P for an advantage.The following properties may be considered: Scalability, waiting time, extendability, cost effectiveness, constraints, hardware requirements, compatibility, throughput, overhead, complexity and prioritization capabilities",
        "provided_answer": "CSMA/CD:\n- Attached Systems, that are not sending for a long time, don't slow down the network. So the heavy load can be handled well, if only few systems at a time need a high transfer speed.\n- New systems can easily be added without altering the procedure of the router, like in Polling or Token Ring. \n\nCon: \n- If the systems need to have some real time requirements, the procedure can't ensure it. In this case you could implement a TDMA with reservation every given time only for this kind of data transfer.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 3.5,
        "normalized_score": 1.0
    },
    {
        "id": "fc9da8a78d4c454285d15da2741d6410",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "TCP should be used because it has a bidirectional bitstream. Furthermore it provides a flow control in order to handle too musch traffic.",
        "answer_feedback": "The response is not related to the theme of the encoding type.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "1b127dabe7044ce18629311d4b86613e",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Um die Bandbreite des \u00fcberf\u00fcllten Netzwerks perfekt auszunutzen, sollte Binary Encoding verwendet werden. Dieses ist einfach und g\u00fcnstig zu realisieren und erm\u00f6glicht 1 Bit per Baud. Voraussetzung ist dabei der Perfekte Takt der Nutzer, da Binary Encoding kein Self-Clocking-Feature besitzt. (Diffenrential) Manchester Encoding ist komplexer und erm\u00f6glicht nur 0,5 Bit per Baud.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "154d9f88a7d34233816d9b6d559ae946",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding due to the good utilization of the bandwidth",
        "answer_feedback": "The response does not provide the second reason behind using the binary encoding in the given scenario which is the lack of need of self-clocking making binary encoding a better option.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "03eb4af099764978934486bddfbdf35d",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding\n1. 3 users have perfect clocks. they  don't need a good \"self-clocking\" feature.\n\n2.Binary Encoding is simple, cheap and has good utilization of the bandwidth (1 bit per Baud)\uff0c all users generate more traffic than the link\u2019s capacities\uff0c so we need better  utilization of the bandwidth than Manchester Encoding and Differential Manchester Encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "9396e88542694cc38ef358db103c182d",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Because it is given that all users have pefect clocks, we can use a simple and cheap binary encoding technique. It will also be a very efficient use of the given bandwidth.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "65c8f882fb3349d8a6bd3003fb2aae47",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I think, binary encoding should be used in this network. The reasons are following:\n1. All users are interconnected and have perfect clocks, so we don't need to worry about synchronization problem between receiver and sender, i.e self-clocking feature is not necessary in this case.\n2. To mitigate network congestion causing by excessive traffic, we need to improve Bit Rate. Using Binary encoding can provide double wideband compared to Manchester encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "af1e8fbecb2f4169b6bb713b09cd2564",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Differential Manchester encoding should be used because it is overall more complex. \n\nSimilarly to Manchester encoding, DM encoding takes advantage of splitting the interval into two, where a voltage level shift takes place (either from high voltage to low voltage or vice versa). However, Manchester encoding is still too similar to binary encoding in that a 1 will always be equivalent to a voltage shift from high --> low and a 0 will always be low --> high voltage. Instead, if the voltage shift occurs between intervals (either high -> low or low -> high), a 0 will be encoded. Likewise, if there is no voltage shift between intervals then a 1 is encoded. By comparing the current interval to the previous interval's voltage level, a more accurate encoding technique can be realized.\n\nAnd unlike the binary encoding system, DM encoding does not rely on binary voltage levels to encode a bit stream. Instead, merely a change in voltage level between intervals encodes the bit stream. (Ex. A lack of voltage level change between the first and second interval would mean that the second bit in the stream is a 1) Therefore, because the voltages only need to differ between intervals, the bit stream is less susceptible to noise/error.",
        "answer_feedback": "The preference is always for a simple solution. Further self-clocking is not required here and manchester provides lower bandwidth utilization which can further complicate the congestion problem.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "fc515f16584c4e13af24aa9a4f456ea6",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "In the described scenario Binary endcoding should be used, because it provides a higher data throughput than Manchester encoding for a given Baudrate. The missing \"self-clocking\" feature is no problem, since all users have perfect clocks.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "51577070b985490781b5520cb368833e",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I would use a binary encoding as the bandwidth is limited. Binary encoding makes good use of the given bandwidth as it has 1 bit/baud, with both types of Manchester encoding having only 0.5 bits/baud. Additionally given a perfect clock the Manchester encodings are not needed, as their main advanteges are self clocking in unsynchronized networks.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "ab5bfe3f2e534314a0f91007f2c999d3",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "As there is no global clock, I would not suggest binary encoding. As there is much traffic I would choose Differential Manchester Encoding, rather than the normal one which is more susceptible to noise.",
        "answer_feedback": "As the perfect clocks are already provided, self-clocking is not required.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "e86cec25bb9d414abd21a43109bc89af",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding because the link's capacities is limited in this senario and differential/manchester encoding require more bandwidth (0.5 bit / Baud). Each user have the perfect clock, therefore encoding with binary encoding is possible in this case.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "b623b29816e44591b3f7a6318094ffe5",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "I think they should use binary encoding. Compare to other encoding techniques, binary encoding has good utilization of bandwidth. Also, the encoding is simple and cheap.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "47c055ab51684068b8ba8080108ddaff",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "For the given scenario Binary Encoding should be used.\nSince the users all have perfect clocks, the encoding does not need to to provide a self clocking feature.\nBecause the network is often congested and the links are more than saturated, a encoding with a high baud rate is preferable.\nHowever, if the network is prone to interference and noise, Manchester Encoding or Differential Encoding could be the better choice, even though it provides only half the baud rate.\nManchester Encoding could provide the benefit of \"built-in Integrity Codes\". This means you could identify overlapping transmissions, that interfere with each other.\nSince noise is not stated in the scenario and assuming, that a mechanism for medium access control is used to prohibit overlapping transmissions, my encoding scheme of choice would be Binary Encoding.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "d34687d61ceb4387b779a98e5aef98e4",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding:\n- Because the users have perfect synhronized clocks, they can aggree on a time window that is used to send one single Bit. So the self clocking of the manchester encoding is not needed.\n- Because we have only 3 interconnected users, we also don't need the high susceptibilty to noise, that would bring the differential manchester encoding",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "e238601a5c2a420f83e59ad3a0021c44",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "The binary encoding technique should be used. Because the chosen scenario doesn't need a self-clocking mechanism. The binary encoding also provides a good utilization in the network.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "de290ae492fb49749fa2d98ef2ae2288",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding should be used here. \nThere is no need for \"self-clocking\" when the users have perfect clocks.\nIt utilizes the full bandwith.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "fa216ddcd8064e5a84a7ddc46e9fb53b",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Because it is given that all users have pefect clocks, we can use a simple and cheap binary encoding technique. It will also be a very efficient use of the given bandwidth.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "45b4c6f085c549ad8db708b795496861",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding is the best option in my opinion as it offers a good utilization of the bandwidth which is especially useful as the network is often congested and furthermore because the local network has a perfect clock it doesn't need self clocking. Another reason would be that it's quite simple and cheap to implement.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "a3367e2cb8a84cdca16fd7a7b3addcc1",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary Encoding.\n1. Since all users have perfect clocks, self-clocking isnt needed.\n2. Good utilization of the bandwidth, so congestion is less of an issue.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "74e9c868295e4906916441af8c64ce34",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Since all users have perfect clocks. The binary encoding technique would be the most suitable one. On the strength of its efficient use of the Bandwidth, this type of encoding will be very practical in congested networks.\nFurthermore it is easier and simpler method to encode the bitstreams.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "3f4f44a5d2c64a3eadbeb013241ea832",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding. beacause it has a good utilization of the bandwidth.",
        "answer_feedback": "The response does not provide the second reason behind using the binary encoding in the given scenario which is the lack of need of self-clocking making binary encoding a better option.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "6a8024ab3bfa483abce8d5556167d5ea",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "The encoding technique is Binary Encoding. \nAll users have perfect clocks, so no need for \"self-clocking\". \nBinary Encoding has good utilization.",
        "answer_feedback": "Need to specify which utilization.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "c12ba353a4c5463b9d87681261701297",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary encoding should be used, because this has the highest baud rate in comparison to the other and so we can keep the congestion the lowest. Further we can implement with the bits on a higher layer a congestion control.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "609fddbf64834f31b2f0fe1a3be0c97e",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "CDMA\nIn CDMA, all the stations can transmit data simultaneously. It allows each station to transmit data over the entire frequency all the time. Multiple simultaneous transmissions are separated by unique code sequence. Each user is assigned with a unique code sequence. which means the rate of data is high.",
        "answer_feedback": "The question asks for the type of encoding to be used, not for access types.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "7684909cebef44928e0555f5309eee5a",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Binary coding. \nBecasue if the clocks are perfect in the local newtwork, there in no need to do self-clocking. And the binary coding has better utilization (almost 200%) to bandwidth than other techniques, which is important when the traffic is heavy.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "325d60e127f74e3e91aae6927621e096",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "Bianry Encoding, since it has good utilization of bandwidth which could solve the traffic problem. On the other hand, the 3 users have already perfect clocks, the no \"self-clocking\" feature of binary coding could be neglected.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "542484f03ec94359870ae7126f4360f9",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "The users can use both Manchester or differential Manchester encoding. The reason behind as fol:\n1. Both of them uses .5 bit per Baus so channel capacity is reduced.\n2. Less susceptible to noise due to interfearance.",
        "answer_feedback": "The binary encoding is the better option because of better bandwidth utilization and no self-clocking requirement.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "427d86b48a6e40b88ec629dfb3fdfe5e",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "DIfferential Manchester encoding should be used, since all users have perfect code and all frequency modulation is utilized (Users have access to selected frequencies at all times)",
        "answer_feedback": "The response is incorrect as under the given scenario binary encoding is better suited.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "b8b6a6bc5b254f1fbbd90143350ec4ee",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "With perfect clock we can use Binary Encoding as the problem with  long sequence of 0/1s wouldn't cause clock synchronization issue. Moreover, it's simpler and makes an efficient use of the bandwidth which could be helpful with heavy network traffic.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "2dc5bd0bef084067977d94901816a0be",
        "question": "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link\u2019s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.",
        "reference_answer": "Binary Encoding, as it is the most efficient in terms of bandwidth since you get a full bit per baud instead of only 0.5. Additionally, you do not have to deal with clock drift and various ticking rates as all clocks are perfect. Therefore, self-clocking / clock recovery is not as necessary. Simple and cheap is also acceptable as one of the reasons.",
        "provided_answer": "The Differential Manchester Encoding might be the best option here. Both Manchester and Differential Manchester Encoding have a self-clocking feature which enables users to know if a tansmission is happening at the moment (every clock cycle a change if there is an active transmission). The Differential variant has a low susceptibility to noise which might be beneficial in a multi-user environment because it might lower the chances of e.g. retransmissions.",
        "answer_feedback": "The binary encoding is the better option because of better bandwidth utilization and no self-clocking requirement.",
        "verification_feedback": "Incorrect",
        "max_score": 2.5,
        "normalized_score": 0.0
    },
    {
        "id": "fcdaabb02ec243e38aa7ac4a225de344",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "56.95 seconds.\nThis is M/M/1/N finite buffer case and N = 10. The question is asking about the time length that the system to be in a state in which there are less than 10 packets waiting in the queue, which also means that system is not full. Therefore, we should calculate the probability that system is full first, which is PB = pN = p10. Then, we can know the probability that less than 10 packets waiting in the queue which is 1-P10. In the case, we monitor exactly one minute after the system reaches equilibrium. With the probability (1-P10) we get, we would expect 56.95 seconds.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "86e535a3009f4c0180a1237f245a72c4",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Assuming 9 packets arrive each second and 10 are served. On average the buffer should never be full leading to a 60s time where there are less than 10 packets in the queue.",
        "answer_feedback": "The stated justification is incorrect as the given rates are not constant and can vary through time, so an average needs to be calculated for the given time. Therefore, the stated time is also incorrect.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "70b89f0848d44a3eab45851d472843c6",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "With the Poisson Process, we can calculate p0 to pN. If p0 = 0.5, this means, 50% of the time the system is empty. \nIn this exercise, we have a \u03bb=9, \u00b5=10 and N=10. The Blocking probability (the probability that the system is full) p10 is 0.051. So, 5.1% of the time, the buffer is full. The complementary probability (the buffer has less than 10 packets waiting) is 0.949. As a result, in one minute we expect that in 0.949*60s = 56,94s less than 10 packets are waiting in the queue.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4695d50b6a45458fb567dd42386176d0",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "First we note down some parameters: there is\na buffer size of N = 10,\nan arrival rate of \u03bb = 9 packets per second,\na serve rate of \u03bc = 10 packets per second,\na utilization of \u03c1 = \u03bb / \u03bc = 0.9\n\nNow we can use N and \u03c1 with the very last formula on slide 30 to calculate the probability p_n of the system being in a state where there are n packets waiting in the queue. We're interested in the sum of all probabilities except for the case n = 10, i.e. we can either calculate p_0 + p_1 + ... + p_9 or simply calculate 1 - p_10. Finally we have to multiply the result by 60 seconds to find out to how many seconds of a minute this percentage corresponds to. So the system spends\n(1 - p_10) * 60 seconds = circa 56.951 seconds\nin a state in which there are less than 10 packets waiting in the queue.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bcf8950636d34fbdb00219fa6a7e31b0",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "The given example is a finite buffer case, so a M/M/1/10 Queue. With the given numbers, the utilization rate is 9/10 = 0,9. So if on an average 9 packets arrive and 10 can be served, there should never be 10 packets waiting in the queue. However, by calculating the blocking probability (the probability that the system is full) with the given formula, Pb = 0,0508 which is ca. 5%. So there is a 5% probability, that the system is full and 10 packets are waiting in the queue. For 1 minute (60 seconds) 5% are 3 seconds. If I say that the system is in a state with 10 packets waiting in the queue for 3 seconds, I would expect the system to be in a state with less than 10 packets waiting for 57 seconds.\nIf I take a look at the system throughput and insert the values in the formula, the result is 8,5428\u202c. This means 9-8,5428 = 0,4572 packets don't make it per second and therefore 0,4572*60 = 27,432 packets are dropped per minute. If 9 packets arrive per second, 27,432/9 = 3,048 seconds in which the system is full. This leads to the same conclusion as above, that the system is for ca. 57 seconds in a state with less than 10 packets waiting.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation. The additional validation is also correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9a533bb0010d4bb5acbfbcdb10855002",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Less than 60 seconds, more than 50 seconds (50-60 seconds),\nbecause the buffer size has 10 and 9 packets go in 10 packets go out. The buffer size of 10 is never exceeded.\n\n- Steps involved :\n1.Arrival process: How customers/requests arrive?(time between requests ,inter-arrival time)\n2.Service process: How much demand do requests generate?(Service time of single requests)\n3.How many places in queue?\n4.How many service stations?\n5.How are queues processed?(First-come-first-serve (FCFS), shortest job first, priority queue, and so on)",
        "answer_feedback": "The required justification is missing and a broad time interval is provided but the precise time in seconds was expected.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "2edbd52ab533471589c930f7e62b1751",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "If the equilibrium has been already reached then assuming that the packets during this 1 minute are evenly distribute, the system would be most of the time In the state where less than 10 packets wait In the queue.\nwe calculate the probability that there are packets P(0) for 0 -9 using this (1-r)^n-1\nthen we",
        "answer_feedback": "The first step of calculating probabilities for 0, 1, \u2026, 9 packets to be in the queue is correct, but it is incomplete. Additionally, the final step of multiplying it with the time frame to receive the expected number of seconds is missing in the response.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "5010a9cca40d465897a50a64d667e477",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "Calculate the probability that there are 0, 1, 2, 3, 4 .. 9 packets in the queue.  Sum these probabilities and multiply that by 60 seconds.\n\nProbabilities can be calculated with these formulas where Pn is the probability the queue has n packets in it:\nP0 = (1-R)/(1-R^(N+1))\nPn = (1-R)R^n/(1-R^(N+1))\nR = 9/10 = 0.9\n\nProbability of 0 to 9 packets in the buffer = 0.9492\nSeconds = 56.96",
        "answer_feedback": "The response correctly explains how the number of expected seconds can be calculated. However, the number of expected seconds or the probability is rounded incorrectly. The correct value is 56.952 instead of 56.96 seconds.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "eca315578cfa4cb18d9837f814212966",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "p10 = ((1 - rho) * rho^n) / (1 - rho^(N+1)) = ((1 - 0.9) * 0.9^10) / (1 - 0.9^(11)) = 0.0508\n\nSeconds with 10 packets in queue = p10 * 60 = 3.048\n\nSeconds with less than 10 packets in queue = 60 - 3.048 = 56.952\n\nSo the system is expected to have 10 packets in it's queue for about 3 seconds and less than 10 packets for the remaining 57 seconds.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d572f3aab20c4ff98003d8347dfc4c08",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "From the problem we recognise that the number of packets arriving is lambda=9 pkts per second and number of packets being serviced is \u00b5=10 pkts per second. Dividing the lambda by \u00b5 gives us the utilization. In the case of an equilibrium in a finite buffer, the probability of having less than 10 packets can be calculated from the formula of the blocking probability (slide 31). As this formula would give us the probability of the system being full, we would then need to subtract it from 1. Finally, this would then be multiplied by 60 (as 1 minute contains 60 seconds) to give the number of seconds in which we expect the system to have less than 10 packets in the queue. The calculated blocking probability is 5,08% and therefore the probability of the system not being blocked 94,92%. 94,92% of 60 seconds is 56,952 seconds which is the estimated time of the system having less than 10 packets in the queue.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f5dd678d42324d8caa83e929c9b1a568",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "The likelihood of the less than 10 packets waiting in the queue is one minus the blocking probability, which is about 5,08%. \nGiven this information, 94,92% of the time, you would expect less than 10 packets to wait in the queue. \nIn one minute, this would be about 57 seconds.",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "06534fe07bfc4a6f815324c6cc291e6c",
        "question": "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.",
        "reference_answer": "Since we have a buffer size (N) of 10, we will always have less than 10 packets waiting in the queue unless there are exactly 10 packets in the queue. Therefore, we first calculate the probability of the system being full/having 10 packets in the queue. This is also called \u201cblocking probability\u201d or P_B.\nP_B = ((1 - utilization) * utilization^N) / 1 - utilization^(N+1) = ((1-0.9) * 0.9^10) / (1 - 0.9^11) = 0.0508\nAlternatively to the blocking probability, it is also valid to directly calculate P(X less than 10) = (1 -P_B) by summing up the probabilities for 0, 1, \u2026, 9 packets to be in the queue. To calculate the expected time where the system is not blocked in the 60-second observation time-frame, we simply multiply the time frame with P(X less than 10) or the complement of P_B (0.25p) = 60 * (1-P_B) = 56.9512 seconds",
        "provided_answer": "57s\nThe average arrival rate is 9 and the average service rate is 10. According to the limited buffer size 10 we can calculate PB, which means the probability that the system is full. So the probability that in the state there are less than 10 packets is 1-PB. Based on the 1 minute monitoring time we can get the result 57s",
        "answer_feedback": "The response correctly states the non-blocking time and provides a valid justification for the calculation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "91b04f793e6748bea48033599ec810b7",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "The routing could have poor performance if the traffic volume change over time. It could also have a high delay when the shortest path is high-loaded.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6a6ad94a71c74b318b094cba260cf616",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "If all possible paths have the same load in the beginning (like A->B, A->E, A->D), then it can not be determined which path it should take for sending the packet, since all paths have the same quality, so the process may fail already. Furthermore, one can argue with the fact, if both paths C->F and E->I have a current utilization of 0 packets/bytes, the packet would not even be sent to the other half of the network, which would lead to an infinite loop in the first half of the network. The destination, in our case G, therefore can't receive the packet at all.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "91d49b8219684495ba7516f6d5fd239f",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "On a network with strong load fluctuations, finding the best path can create a ping pong effect, when the load on a chosen path increases, maybe causing the routing to not terminate. This also can result in flipping paths. This is called oscillation.Additionally, utilization with sent packets/bytes is not relative to the link capacity, meaning, e.g., a half used 10Gbit/s Link would be less likely choosen to a fully used 1Gbit/s link.\nIn low latency scenarios with low bandwidth need, a connection with many hops but with low current load would be preferred, resulting in a probably higher latency.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cb168063f599450c95537c90b5b4cf25",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "There are multiple possible ways a package could take on its way to G. Problems occur because when a path is chosen it's utlilization is goes up makting this a less attractive option, so another path is chosen, repeating indefinately, causing heavy routing overhead.",
        "answer_feedback": "Why would this be repeated? If another path has less utilization, the path will be used and the packet will be transmitted to G.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "e9c1d8a2d7d6402489d535b9726993b4",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Using load can lead to oscillation, which means the path that seems best is always changing after the load is set to one path. \nExample: Path A and Path B have load 0, Path A is selected, its load goes up to 1. Now path B has the lower current load, so therefore the system would select path B and its load goes up ti 1. Now Path A is 0 again and the cycle continues.",
        "answer_feedback": "But why is oscillation a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "5ee3f0d7e4a64246b51b60e8fc3d37d0",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "This would lead to problems, since this metric would lead to an oscillation of the loadAdditionally, it would be a lot of computational overhead, since the paths have to be constantly evaluated",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5042db42a07d4bf5ab02c02a7260f714",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "It would cause problems because links / connections on the path could be overloaded or get disconnected / failed. This would result that packets could be lost or need a different path. Furthermore, the current load on a link could change after the best path was found.",
        "answer_feedback": "Why would a path get overloaded?\nWhen a current load on a link changes, the best path changes too, there is no problem with this.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "25ad584cda3c4db8bb9538cbb45b09a0",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "According to slide 78 current \"load\" should not be used as a metric because it may lead to an \"oscillation\" of the load (meaning sender flipping paths), which should be avoided. This would lead to frequently distribution of the local information all IS.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3d9cdc7019a1413ea0d67a42a532ba0c",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "This strategy is problematic, because the utilization can change over time, therfore A can assume paths which are over used at the moment. A update structure for the metric would be needed.",
        "answer_feedback": "To evaluate the path, the current\u00a0load is used: changes in the utilization are considered and A will not use paths which are overused at the moment.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "561f6744f9994955963f6e1477686c3e",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "The \"load-metric\": choose the path with the lowest utilization would lead to an oscillation between different path options. Everytime a path is choosen and used afterwards its utilization goes up and thus a differet path is chosen. This requires a lot of computational overhead since the path will be constantly evaluated.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d8653082083541d08f53c01af8bec4eb",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Multiple transmissions with load may lead to oscillation of the load. Another problem in applying the flooding procedure is the inconsistency, i.e. varying states are simultaneously available in the network.",
        "answer_feedback": "We do not apply flooding here!\nWhy is oscillation a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "50b96a3db7f440a4869531640d5aa5f9",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "The problem is in calculating the \"distance\" between nodes, which leads to \"oscillation\" of the load because of the links CF and EI.Furthermore each node has a CPU overhead because of the changing routing tables due to \"oscillation\".",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c10236b6914449bf807417f927976521",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Using current load could be not the lowest cost path.",
        "answer_feedback": "But this is no problem at the receiver end",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "72ee96afa41e48f5ad9b077527740dab",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "Yes, taking the current load into account can lead to route flipping, i.e. packets of the same connection take different paths to the same destination.\nIf such a flip occurs while nodes A and G are communicating with each other, a higher delay due to packet reordering might be the case or - if the receive window is too small - retransmissions may be necessary.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "18ca0dbe65a74c01909594c4fc73ba16",
        "question": "Consider the following network topology from the lecture:With routing, we want to find the best path for our packets. For this, we first need to define a metric to evaluate the quality of a path. One possible choice could be the current load (i.e. the current utilization in terms of sent packets/bytes) on this path. Assume that A wants to send data to G, could this routing strategy cause any problems at the receiver end? Please explain your answer in 1-2 sentences.",
        "reference_answer": "Yes, using the current load to find the best path can lead to fluctuations/oscillations when there is more than one path between any pair of end systems in the network (here: CF and EI). This can cause packet reorderings at the receiving side.",
        "provided_answer": "This strategy could cause problems:\nThe routing algorithm would constantly alternate between using link CF or EI, because when one of them is part of the current route, the other one seems like the better choice because it has lower utilization. So even if using only one of the links exclusively would make more sense because it may have higher bandwidth than the other one, this strategy would alternate between them.",
        "answer_feedback": "But why is alternating a problem?",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b88e7d542d204a59a81ef1534fa46e81",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,B,forward)\n(A,C,forward)\n(A,D,drop)<=D doesn't foward a message from A to F, because F doesn't receive unicast packets via D.\nHop 2:\n(B,E,forward)\n(C,F,drop)<=F doesn't foward a message from A to G, because G doesn't receive unicast packets via F.\nHop 3:\n(E,G,forward)\nHop 4:\n(G,H,drop)<=vertex H has only one neighbor from which it got the message, vertex H does not forward the message.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "a3ea5b5702084f52bd4b822a20256cc0",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop2:\n(B; C, drop) <= B knows that C does not receive unicast packets via B\n(B, E, forward)\n(C, B, drop)<= C knows that B does not receive unicast packets via C\n(C, D, drop)<= C knows that D  does not receive unicast packets via C\n(C, E, drop)<= C knows that E does not receive unicast packets via C\n(C, F, forward)\n(D, C, drop)<= D knows that C does not receive unicast packets via D\n(D, F, drop)<= D knows that F does not receive unicast packets via D\nHop3:\n(E, C, drop)<= E knows that C does not receive unicast packets via E\n(E, F, drop)<= E knows that F does not receive unicast packets via E\n(E, G, forward)\n(F, E, drop)<= F knows that E does not receive unicast packets via F\n(F, G, drop)<= F knows that G does not receive unicast packets via F\n(F, D, drop)<= F knows that D does not receive unicast packets via F\nHop4:\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB,\u00a0(A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "9882da4c583e439197513f6b843992bd",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2:\n(B, E, forward)\n(B, C, drop) <= A->C is shorter\n(C, B, drop) <= A->B is shorter\n(C, E, drop) <= A->B->E is shorter\n(C, F, forward)\n(C, D, drop) <= A->D is shorter\n(D, C, drop) <= A->C is shorter\n(D, F, drop)\u00a0 <= A->C->F is shorter\u00a0\n\nHop 3:\u00a0\n(E, C, drop) <= A->C shorter\n(E, F, drop) <= A -> C-> F is shorter\n(E, G, forward)\n(F, D, drop) => A->D is shorter\n(F, E, drop) => A -> B-> E is shorter\n(F, G, drop) => A -> B -> E -> G is shorter\n\nHop 4:\n(G, F, drop) => A->C->F is shorter\n(G, H, forward)",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "1a3e2f478ce54f5eb9f22cc7c2371a42",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop2:\n(B, E, forward)\n(B, C, drop) <= C doesn't send any Packets to A, over B\n(C, D, drop) <= D doesn't send any Packets to A, over C\n(C, E, drop) <= E doesn't send any Packets to A, over C\n(C, F, forward)\n(D, C, drop) <= C doesn't send any Packets to A, over D\n(D, F, drop) <= F doesn't send any Packets to A, over D\n\nHop 3:\n(F, E, forward)\n(F, G, forward)\n(E, F, drop) <= F doesn't send any Packets to A, over E\n(E, G, forward)\n\nHop 4:\n(G, H, drop) <= H doesn't have any adjacent nodes other than G",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "f143400596384782b18c3da7340198c4",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop2:\n(B, E, forward)(C, F, forward)\nHop3:(E, G, forward)Hop4:\n(G, H, forward)",
        "answer_feedback": "The reason also needs to be provided when a packet is not forwarded to other nodes i.e. dropped by the receiver as stated in the question \"list all the packets\u00a0which are sent together with the information whether they will be forwarded or dropped at the receiving nodes.\"\u00a0Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "id": "ff6757bb203743e3b75c5bd48c9d084f",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)(A, C, forward)(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, E, drop) not minimal spanning tree(C, F, forward)(D, F, drop) not minimal spanning tree(B, C, drop) not minimal spanning tree(C, B, drop) not minimal spanning tree(C, D, drop) not minimal spanning tree(D, C, drop) not minimal spanning tree\nHop 3:\n(E, C, drop) not minimal spanning tree(E, F, drop) not minimal spanning tree(E, G, forward)(F, D, drop) not minimal spanning tree(F, G, drop) not minimal spanning tree(F, E, drop) not minimal spanning tree\nHop 4:\n(G, H, forward)(G, F, drop) not minimal spanning tree",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "303709f1e1e94fff947619b67042c8ea",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop) <= C will forward to F because C has a shorter distance to F than D to F\nHop 2:\n(B, E, forward)\n(C, F, drop) <= E will forward to G because E has a shorter distance to G than F to G\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, drop) <= H can not forward anywhere",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "9017711037734175bc522de6d6fc5444",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, drop) <= D knows that either C nor F are routing to A over DHop 2:(B, E, forward)(C, F, drop) <= F knows that either C nor d or F are routing to A over FHop 3:(E, G, forward)Hop 4:(G, H, drop) <= H has no neigbors except G (no links except the incoming link)",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "7e4adfe9a03e4777ad0af25fea941e83",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1: \n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\n\nHop 2: \n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, drop) -> Packet drops because it only has one neighbour and node\u00a0 H does not forward the message.",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.5 for reasoning\u00a0(A,D, drop),\u00a0(C, F, drop)",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.8
    },
    {
        "id": "93d43578501f432a89db61b8c11d8280",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, drop) <= H has only one neighbor (G) from which it got the message",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop)\u00a0 will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "a4d1796dacc54d6eb882c460e33fc628",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,B,forward)\n(A,C,forward)\n(A,D,drop)<=because F does not forward a broadcast packet from A to D)\nHop 2:\n(B,E,forward)\n(C,F,drop)<=because G does not forward a broadcast packet from C to F)\nHop 3:\n(E,G,forward)\nHop 4:\n(G,H,drop)<=because there is no more receiver after H",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "daa510bd6d7d4472b3f69d6407be14c1",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, E, drop) <= because E never send a packet to A from C\n(C, F, forward)\n(D, F, drop)\u00a0<= because F never send a packet to A from D\nHop 3:\n(E, G, forward)\n(E, F, drop) <= because F never send a packet to A from E\n(F, E, drop)\u00a0<= because E never send a packet to A from F\n(F, G, drop)\u00a0<= because G never send a packet to A from F\nHop 4:\n(G, H, drop) <= H is the last node and there is no other link",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "3fa25da09a0b4262887534780a989956",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward);\u00a0(A, C, forward);\u00a0(A, D, forward).\nHop 2:\n(B, E,\u00a0forward);\u00a0\u00a0(B, C, drop) from A to\u00a0 C ,via B is not the\u00a0shortest;\u00a0\n(C, B, drop),from A to B ,via C is not the\u00a0shortest;\u00a0(C, D, drop),\u00a0from A to D ,via C is not the\u00a0shortest;\u00a0(C, E, drop),\u00a0from A to E ,via C is not the\u00a0shortest;\u00a0\n(C, F,\u00a0\u00a0forward).\u00a0\u00a0\n(D, C,\u00a0drop) ,from A to C ,via D is not the\u00a0shortest;\u00a0(D, F,\u00a0drop) ,from A to F ,via D is not the\u00a0shortest.\nHop 3:\n(E, C,\u00a0drop),\u00a0from A to C ,via E is not the\u00a0shortest;\u00a0(E, F,\u00a0drop),\u00a0from A to F ,via E is not the\u00a0shortest;\u00a0(E, G,\u00a0forward).",
        "answer_feedback": "The provided flow appears more similar to RPF than to RFB.\u00a0 In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "94f947b3d24d456f9d3f22bc0308f6d6",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,B,forward)\n(A,C,forward)\n(A,D,drop) D will not forward the packet, because A is the origin and D is not part of a unicast route a-c or a-f.\nHop 2:\n(B,E,drop) B is origin, routes g-a, f-a, c-a do not go through E(C,F,forward)\n\nHop 3:\n(F,G,forward)\n\nHop 4:\n(G,H,drop) G is origin of the packet, no further neighbors",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "f1f15a37b0234c36983da9084d747c2c",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n(B, E, forward)\n(C, F, forward)\nHop 3:\n(E, G, forward)\nHop 4:\n(G, H, forward)",
        "answer_feedback": "The reasoning behind which packets are dropped is not stated.\u00a0 Please go through the model solution. Packets will be considered drop if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop),\u00a0(C, F, drop) and (G, H, drop)",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "id": "9da42efa5a16426e8ce5358c02d51e99",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forw.)(A, C, forw.)(A, D, forw.)\nHop 2:\n(B, E, forw.)(C, F, forw.)\nHop 3:\n(E, G, forw.)\nHop 4:\n(G, H, forw.)",
        "answer_feedback": "Packets will be considered dropped if it is not forwarded further by the receiver node.(-0.75 for reasoning (A,D, drop), (C, F, drop) and (G, H, drop) ).",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.7
    },
    {
        "id": "0bf16cc8d48e44ef91604b4255f619c0",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "(sender, receiver, drop)\u00a0\nHop 1:\n(A, B, forward)(A, C, forward)\u00a0\n(A, D, forward)\nHop 2:\nNode B:\u00a0\n(B, C, drop) <= C is not on the best path to A\n(B, E, forward)\nNode C:\n(C, B, drop) <= B is not on the best path to A\n(C, D, drop) <= D is not on the best path to A\n(C, E, forward)\n(C, F, forward)\nNode D:\n(D, C, drop) <= C is not on the best path to A\n(D, F, forward)\nHop 3:\nNode E:\n(E, F, drop) <= F is not on the best path to A\n(E, G, forward)\nNode F:\n(F, E, drop) <= E is not on the best path to A\n(F, G, forward)\nHop 4:\n(F, H, forward)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "b6c76a90178442bc852ff088bee43f68",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, forward)\nHop 2:\n\n(B, E, forward)\n\nB would not forward the packet from S to C because B knows that from C to S the packet would never go through B\n(C, F, forward)\nSimilar reason, C would not forward the packet from S to D and S to B\nD would not forward the packet from S to C and S to F\nHop 3:\n\n(E, G, forward)\nE would not forward the packet from S to C and S to F\nF would not forward the packet from S to C and S to E\nHop 4:\n(G, H, forward)\nG would not forward the packet from S to F",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "46d803aa9ed1493596a31c0570a1e5ed",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop)\u00a0<= (C to F is shorter than D to F)\nHop 2:\n(B, E, forward),\n(C, F, drop)\u00a0<= (E to G is shorter than F to G)\nHop 3:\u00a0\n(E, G, forward)\nHop 4:\u00a0\n(G, H, drop) <= (H is the last node)",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "4297612a576c44d18edcd254ee74c4ca",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\nFROM A:\n(A, B, forward)\n(A, C, forward)\n(A, D, drop) <= Forward over B is not part of unicast A > C and A-> F\n\nHop 2:\nFROM B:\n(B, E, forward)\nFROM C:\n(C, F, drop)\u00a0<= Forward over C is not part of unicast A -> D, A->E and A-> G\n\nHop 3:\nFROM E:\n(E, G, forward)\n\nHop 4:\nFROM G:\n(G, H, drop) <= only link is the receiving link",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 2.5,
        "normalized_score": 1.0
    },
    {
        "id": "3c58ef0902df4ada94e313947ac38e6e",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 2:\u00a0\nFrom B:\u00a0\n(B, C, drop), (B, E, forward)\u00a0\nFrom C:\u00a0\n(C, B, drop),\u00a0\n(C, D, drop),\u00a0\n(C, E, drop),\u00a0\n(C, F, forward)\u00a0\nFrom D:\u00a0\n(D, C, drop),\u00a0\n(D, F, drop)\u00a0\nHop 3:\n\u00a0From E:\u00a0\n(E, C, drop),\u00a0\n(E, F, drop),\n(E, G, forward)\u00a0\nFrom F:\n(F, D, drop),\u00a0\n(F, E, drop),\u00a0\n(F, G, drop)\u00a0\nHop 4:\u00a0\nFrom G:\u00a0\n(G, F, drop),\u00a0\n(G, H, drop)\u00a0\nBecause vertex H has only one neighbor from which it got the message, vertex H does not forward the message.\nIn total 19 messages are sent during the broadcast.",
        "answer_feedback": "Hop 1 not given and\u00a0(C, F, drop) occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "2b83763bc7384254a7d5750713df4a06",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:\n(A,B,f)\n(A,C,f)\n(A,D,f)\n\nHop 2:\n(B,E,f)\n(B,C,d) <- A,C is faster, not minimal spanning tree\n(C,B,d) <- A,B is faster, not minimal spanning tree\n(C,E,d) <- B,E is faster, not minimal spanning tree\n(C,F,f)\n(C,D,d) <- A,D is faster, not minimal spanning tree\n(D,C,d) <- A,C is faster, not minimal spanning tree\n(D,F,d) <- A,C,F is faster, not minimal spanning tree\n\nHop 3:\n(E,C,d) -> A,C is faster, not minimal spanning tree\n(E,G,f)\n(E,F,d) -> A,C,F is faster, not minimal spanning tree\n(F,E,d) -> A,B,E is faster, not minimal spanning tree\n(F,G,d) -> A,B,E,G is faster, not minimal spanning tree\n(F,D,d) ->A,D,F is faster, not minimal spanning tree\n\nHop 4:\n(G,F,d) -> A,C,F is faster, not minimal spanning tree\n(G,H,f)",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) and (G, H, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.4
    },
    {
        "id": "99bbb413b72e48508460b21f1a3ffb20",
        "question": "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here",
        "reference_answer": "Hop 1 :(A, B, forward),(A, C, forward), (A, D, drop) <= reason: remaining neighbors C and F do not use D as the next hop to get to A Hop 2 :(B, E, forward),(C, F, drop), <= reason: remaining neighbors D, E, G do not use F as the next hop to get to A Hop 3 :(E, G, forward)Hop 4 :(G, H, drop) <= reason: H is only connected to G, packet is not forwarded to incoming link",
        "provided_answer": "Hop 1:(A, B, forward)(A, C, forward)(A, D, forward)Hop 2:\n(B, C, drop) <= C already has packet from A from Hop #1(B, E, forward)(C, B, drop) <= B already has packet from A from Hop #1(C, D, drop) <= D already has packet from A from Hop #1(C, E, drop) <= E has packet from B with better metric from route ABE (2+1=3) vs. ACE (2+2=4)(C, F, forward)(D, C, drop) <= C already has packet from A from Hop #1(D, F, drop) <= F has packet from C with better metric from route ACF (2+1=3) vs. ADF (2+3=5)Hop 3:\n(E, C, drop) <= C already has packet from A from Hop #1(E, F, drop) <= F already has packet from C from Hop #2(E, G, forward)(F, D, drop) <= D already has packet from A from Hop #1(F, E, drop) <= E already has packet from B from Hop #2(F, G, drop) <= G has packet from E with better metric from route ABEG (2+1+1=4) vs. ACFG (2+1+2=5)Hop 4:\n(G, F, drop) <= F already has packet from C from Hop #2(G, H, drop) <= H has only one neighbor (G) from which it got the message",
        "answer_feedback": "In\u00a0 RFB, (A,D, drop) and subsequent flow will change accordingly. Also (C, F, drop) will occur. Please consult the model solution.",
        "verification_feedback": "Partially correct",
        "max_score": 2.5,
        "normalized_score": 0.6
    },
    {
        "id": "0487cc3b88354e109632eb97f38e341d",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. TSAP only valid for one connection; some TSAP are well known\n2. identify each connection by SeqNo; Endsystem must store this information\n3. identify each PDU by SeqNo; higher usage of bandwidth and memory",
        "answer_feedback": "Few advantage and disadvantage are missing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.67
    },
    {
        "id": "1d34f12206124c58bd90602a41ab4f47",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Use temporarily valid TSAP \n+ Easy to implement (just need a new TSAP per connection that can be chosen during handshake)\n- Cant use temporarily valid TSAP because server uses \"well-known\" TSAPs that cant change\n- only very limited TSAP space \n\n2. Identify connections individually\n+ multiple individual (distinguishable) connections are possible in parallel over the same TSAP \n- endsystems must store the chosen sequence numbers for each connection for the duration of the whole connection\n- Only for connection-oriented systems\n\n3. Identify PDUsindividually\n+ high flexibility, due to fine-grained detection of duplicates independent of a connection or TSAP\n- High SeqNr space needed (Prerequisite: how big should the initial space even be? Is the connection alive for years or just seconds?) -> higher usage of bandwidth and memory",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "7592b9678e4e4297be9ab699c7c1d57c",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "Method 1: to use temporarily valid TSAPs\nGenerate unique (Transport) Service Access Point (TSAP) for each communication and they are valid for one connection only\nAdvantages:\nYou can always generate new TSAPs Disadvantages:\nsome TSAPs are standardized(\"well-known ports\") and cannot be usedMethod 2: to identify connections individually\neach connection is assigned a new Sequence number and endsystems story assigned Sequence number and remember them\n\nAdvantages:\nDuplicates from another connection with a\u00a0 Sequence number doesn't interact with other connection with a different sequence number.Disadvantages:\nOnly works with connection-oriented serviceMethod 3: to identify PDUs individually: individual sequential numbers for each PDU\n\nAdvantages:\nbetter usage of bandwidth and memory because you have individual sequence numbers for each packet and they almost never get resetDisadvantages:\nsequential number range depends on packet rate and packet probable \"lifetime\"",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "0e3bf1b60c08493184d946e8f759bd8a",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. use temporarily valid TSAPs\nAdvantage: Simple to implement almost no additional overhead\nDisadvantage:\u00a0\u201cwell-known\u201d TSAP's exist with which the server is addressed, so changing TSAP's is not possible in this situation\n\n2.\u00a0identify connections individually\nAdvantage: Distinction is made at connection level and not at packet level, so less overhead\nDisadvantage: The end system has to reliably keep a record of Connecion-SeqNo pair\n\n3.\u00a0identify PDUs individually\nAdvantage: Able to handle duplicates and ordering of packets\u00a0\nDisadvantage: The range of sequence numbers could be insufficient and cause duplicates if the packet rate is very high and/or the \"lifetime\" of packet is very long.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "af7ff5ad885e4c51947734d5716fcf24",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "There are two separate problems that need to be considered here: Duplicates within a single connection or duplicate connections. As duplicate packets within a connection are easily handled by seqeunce numbers and the focus of the lecture seemed to be on duplicate connection, I will focus on those.\n1. Different port(TSAP) for each connection: Kind of defeats the purpose of ports as multiple ports would be bound to a single thread. Also servers that communicate on a well known port cannot use this methode. Solves the problem of duplicate connections without using additional bandwidth\n2. Count prior connections: Requires endsystems to keep track of this counter. Low effort as this would only be required in the connection establishment stage\n3. Count prior packets: Requires a realtively high bandwidth and memory. Could replace the already used sequence number within the protocol to avoid for example out-of-order packets",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "5ca5c17fc74140fdb89a602217d83e4c",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Sequence numbers: + discards duplicates\n2. PAR: + doesn't block at loss of both frames and ACKs; - long wait possible\n3. NAK: + discards bad frame, can reduce additional traffic",
        "answer_feedback": "the problem of duplicate packets on the transport layer in a connection-oriented service needs to be explained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.17
    },
    {
        "id": "a2bfbb541ba74b7da2e05811f1ff2a3f",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "One method to resolve the problem of duplicate packets is to use temporarily valid TSAPs for only one connection. On paper, this reduces the problem of duplicates since each TSAP is only used once and therefore reduces the number of duplicates to the ones occuring in that one connection. However, a disadvantage is that some TSAPs always exist and are well-known. Hence, it is not possible to realize this approach and generate temporary TSAPs for each connection.\u00a0\n\nAnother method would be to make connections unique and identify them individually. An advantage is that duplicates from different connections do not occur anymore since each is unique. However, this is not possible with connection-less systems. Additional knowledge is needed to realize this approach since end systems need to remember the sequence numbers even after being turned off so that they remain unique for each connection.\u00a0\n\nThe third approach to solve duplicates is to identify each packet individually with unique sequence numbers. An advantage is that it easily allows the receiver to identify and discard duplicate packets since it can just compare sequence numbers to check if it already received a packet with that number. However, this approach requires that the range of sequence numbers are chosen wisely so that they do not repeat before each packet is delivered and processed. Furthermore, this approach requires more bandwidth/memory due to the overhead of the sequence numbers.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "2b33be1835f94e7baa6d562ae77e5ee1",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "One method to avoid duplicate packets is to make the time-out time of the packets larger. The advantage of that is that the sender has enough time to receive the acknowledgment of the packets. The Disadvantage is that if the time-out time is too large and the sender has only a certain window size to send unacknowledged packets, it could results that the sender is sending the data too slow and the receiver has to wait.\n\nAnother method is that the receiver can ignore/ discard the same packets by using sequence numbers. The advantage is that the receiver will not be full of duplicate packets and knows via sequence number which packet should arrive next. The disadvantage is that the packets send with sequence numbers are larger.\n\nAnother method is to use temporarily valid TSAPs. The advantage is that the TSAP is only valid for one connection only. A disadvantage is that server addressing is not possible because the server is reached via a designated/known TSAP.",
        "answer_feedback": "The problem of duplicate packets on the transport layer in a connection-oriented service needs to be resolved.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.67
    },
    {
        "id": "31c940ea71214a529c901d5cb8cac6a2",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "Method 1: to use temporarily valid T SAPs\nGenerate unique (Transport) service Access Point (TSAP) for each communication and they are valid for one connection only\nAdvantages:\n- You can always generate new TSAPs\nDisadvantages:\n- Some TSAPs are standardized(\"well-known ports\") and cannot be used\nMethod2: to identify connections individually\neach connection is assigned a new Sequence number and end systems story assigned Sequence number and remember them\n\nAdvantages:\n- Duplicates from another connection with a Sequence number does not interact with other connection with a different sequence number\nDisadvantages:\n- Only works with connection-orinted service\n\nMethod 3: to identify PDUs individually: individual sequential numbers for each PDU",
        "answer_feedback": "Disadvantage of third method not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.83
    },
    {
        "id": "a80eb0d153f842bca702924053e2e8cf",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "-1- To use temporarily valid TSAPs (unique ports).\nadvantage:\u00a0 it can eliminate the need for duplicate because we use unique port each time.\u00a0\u00a0\ndisadvantage: require large number of unique ports (names) for some period of time.\n\n-2- To identify connections individually.\nadvantage: git rid of duplicate between different connections by assigning each of them a sequence number.\u00a0 \u00a0\ndisadvantage:\u00a0end systems must be capable of storing\u00a0 sequence numbers (need storage for that ) as well as it is more complicated because if the connection is lost we need to remember what happened in the past (store information)\n\n-3- To identify PDUs individually:\nadvantage:\u00a0 assign a\u00a0SeqNo for each packet for certain time, after this time I can reuse this SeqNo and drop\u00a0 any duplicate packets.\u00a0\ndisadvantage:\u00a0higher usage of bandwidth and memory",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "eaccfaa209814d65b3c41ec947645b48",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. temporary valid TSAPs\nFor each new connection, a new, unique, temporary valid TSAP only for this connection is created and thrown away at disconnection and never used again.\nAdvantage: simple and effective (transport process just can't be reached by delayed duplicates)\nDisadvantage: does not work if the service should be reached via a designated, known TSAP.\n2. identify connections individually\nEach connection is assigned a unique identifier and identifiers that have already been used are remembered by the end systems, so that when the connection is initiated it can be checked whether the identifier has already been assigned before (= duplicate).\nAdvantage: designated TSAPs possible (e.g. for well-known services).\nDisadvantage: Storage requirements: end systems must permanently store the necessary information and have it available again, for example after a shutdown or crash.\n3. identify PDUs individually\nEach PDU is assigned an individual sequence number.Advantage: fixed TSAP possible, no persistent backup of information necessary.\nDisadvantage: More complex, lifetime of packet must be estimated well, because too small a value range for identifiers (sequence number) leads to duplicates not being recognized.",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "10a979564d4a4f53a43702382f7df8fc",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1.\u00a0to use temporarily valid TSAPs:\n-need large numbers of names because it should be unique -> not usable in reality\n+very potent, because TSAP is valid for one connection only\n\n2. to identify connections individually:\n-endsystems must be capable of storing this information\n+if there is a duplicate from another connection they don't interact with each other\n\n3. to identify PDUs individually:\n-perfect time has to be known by everybody\n+one can reuse the SeqNo after a certain time",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "c5a3b94bfcaf42a49038567b8f29b62f",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Using temporarily valid TSAPs:\nthis method uses TSAP that are valid for only one connection which makes it easy to identify duplicates. This method is not always possible, e.g. with process server addressing the server needs a designated/known TSAP\n\n2. Identify connections individually:\neach connection is assigned a new SeqNo and end-systems remember already assigned SeqNo. It does not work with connectionless oriented systems and end-systems have to be capable of storing the already assigned SeqNo. It's easy to implement this method.\n\n3. Individual sequential numbers for each PDU:\neach PDU gets a SeqNo assigned which results in higher usage of bandwidth and memory. They are enough SeqNo so that they basically never get reset.",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "00739c93a61043f48a56c2dfcd6b0941",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. to use temporarily valid TSAPs\n\u00a0 \u00a0+ easiest realization\n\u00a0 \u00a0- too many TSAPs will quickly use up all the limited port numbers\n2. to identify connections individually\n\u00a0 \u00a0+ each individual connection has a individual sequence number\n\u00a0 \u00a0- assigned sequence number are saved in endsystems. If endsystem switched off, the information disappear\n3. to identify PDUs individually\n\u00a0 \u00a0+ sequence numbers basically never gets rest\n\u00a0 \u00a0- higher usage of bandwidtrh and memory",
        "answer_feedback": "The response is correct",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "d4aa496f5e024dcd908145800762d269",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. Use temporary valid TSAPs: +TSAP valid for one connection only -not always applicable because of process server addressing method\n2. identify connections individually with unique Sequence Numbers: +unique identification of connections remembered by ESs -ESs must be capable of reliably storing this information\n3. identify PDUs individually with individual sequential numbers for each PDU: +SeqNo basically never gets reset -higher usage of bandwidth and memory\n(f.23, 24)",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "ba53cbe552f74bc9acfa6fcb0b83596d",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "1. use temporarily valid TSAPs\n+ easy and quick\n- does not identify the unique packets inside a connection.\n\nFolie 23 / 10 Transport Layer",
        "answer_feedback": "Only one method mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.5,
        "normalized_score": 0.33
    },
    {
        "id": "887704ed04b8485db2f0c3b83355561a",
        "question": "Discuss 3 methods (each with at least one advantage and disadvantage) that address the problem of duplicate packets on the transport layer in a connection-oriented service.",
        "reference_answer": "1. to use temporarily valid TSAPs -method: -TSAP valid for one connection only -generate always new TSAPs -evaluation -in general not always applicable:-process server addressing method not possible, because -server is reached via\u3000a designated/known TSAP - some TSAPs always exist as \"well-known\" 2. to identify connections individually -method: -each individual connection is assigned a new SeqNo and -endsystems remember already assigned SeqNo -evaluation -endsystems must be capable of storing this information -prerequisite: -connection oriented system (what if connection-less?) -endsystems, however, will be switched off and it is necessary that the information is reliably available whenever needed 3. to identify PDUs individually: individual sequential numbers for each PDU -method: -SeqNo basically never gets reset -e.g. 48 bit at 1000 msg/sec: reiteration after 8000 years -evaluation -higher usage of bandwidth and memory -sensible choice of the sequential number range depends on -the packet rate -a packet's probable \"lifetime\" within the network",
        "provided_answer": "One possibilty to adress the problem of duplicate packets is to use temporarily valid TSAPs. These TSAP are valid for one connection only and are unique for this connection. Some ports are relied to as well-known TSAPs, which can cause duplicates again.\nAnother possibility is to identify connections individually with a SeqNo. So duplicates are avoided by assigning a SeqNo to every connection. A disadvantage is the need to store this information in the endsystems even if they are switched of.\nAnother possibility is to identify the different packets individually. So a sequential number is assigned to each PDU individually. This makes it easy to identify duplicates. A disadvantage is the higher usage of memory and bandwidth.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.5,
        "normalized_score": 1.0
    },
    {
        "id": "5524bccd01b0409099658569a35aef49",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start (getting to equilibrium) \u201cWant to find this extremely fast and wasting time\u201d. Phase 2: Congestion Avoidance \u201cAdditive increase - gradually probing for additional bandwidth. Multiplicative decrease - decreasing cwnd upon loss/timeout\u201d. Congestion Window (cwnd): Initial value is 1 MSS (=maximum segment size), and counted as bytes. Slow-start threshold Value (ss_thresh): Initial value is advertised window size.\n\t Phase 1: Slow start (cwnd is less than ss_thresh) => After initialize (cwnd =1), each time a segment is acknowledged increment cwnd by one (cwnd++). Then continue until reach ss_thresh (window size) or packet loss. Phase 2: Congestion avoidance (cwnd >= ss_thresh) => When Timeout, that means there is a congestion. And in each time congestion occurs ss_thresh is set to 50% of the current size of the congestion window (ss_thresh = cwnd/2), cwnd is reset to one (cwnd = 1), and slow-start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase is missing details about how ss_thresh changes when a packet is lost. Also, the congestion avoidance phase's explanation lacks how the cwnd is incremented.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "id": "27e7c6c794334644b8dfff754f21c383",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The 2 phases are slow start and congestion avoidance. cwnd indicates the number of segments that are send. Each time this sent segment(s) are acknowledged, cwnd and the send segments double (=increments by one per ack) \u2192 send a sement, receive ack, increment cwnd, send 2 segments, receive ack for both, increment cwnd by 2 (because of 2 received ack), send 4 segments\u2026 exponential growth.\nThis continues until ss_thresh is reached (cwnd >= ss_thresh), or a packet is lost. If a packet is lost, then cwnd falls back to initial size 1 and ss_thresh is set to current cwnd/2. If no packet is lost but threshold reached, then don\u2019t double amount of send segments each RTT (add 1 segment per received ack) but only add 1 segment to each incremented cwnd (linear growth).",
        "answer_feedback": "The response is partially correct because if congestion occurs, ss_thresh is set to half of the current size of the congestion window and the congestion window is reset to one, in both phases. This, therefore, also happens in phase 2.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "3b0348cc84ce445da6cd24cc4e5255e0",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: cwnd is increased by one with each each acknowledgement, which effectively means doubling the cwnd each RTT. When cwnd reaches ss_thresh congestion control goes to Congestion Avoidance phase.\n2. Congestion Avoidance: cwnd is increased by one each RTT.\n\nEach time congestion occurs, ss_thresh is set to cwnd/2, cwnd is reset to 1 and slow-start is entered again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6f29d81d90dc44649bfda1ffdc8a14c2",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The first phase is called \"slow start\".\nAfter the initialisation, the sender starts sending segments, and waits for the receiver to acknowledge them all. This number will double every Round trip time (RTT) until the advertised window size is reached. If a timeout happens beforehand, phase one is restarted immediately.\n\nWhen ss_thresh is reached, phase two - \"congestion avoidance\" - is entered, when the \nThe RTT will be increased linearily until a timeout occurs. When this occurs, phase one is initialized again.\nThese two phases will be repeated over and over again, the sending rate will never be constant with TCP.",
        "answer_feedback": "The response is correct except that in both the phases when the congestion occurs,  ss_thresh is set to half of the current size of the congestion window and the congestion window is reset to one, which is not the same as the initialized value where cwnd = 1 and ss_thresh = advertised window.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b0c53e81a0944c4aab8d133fdb6be77a",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases of congestion control are:\n1. Slow Start\n2. Congestion Avoidance\n\nIn the Slow Start phase, the sender sends as many segments as the size of the cwnd. Every time a segment is acknowledge the cwnd increases by one. So, in the slow start phase the number of sent packets increases exponentially. When the cwnd reaches the ss_thresh or there is a packet loss, the system changes to Congestion Avoidance phase.\nIn the Congestion Avoidance phase, each time congestion occurs ss_thresh becomes cwnd/2 and cwnd is reset to 1. Then, the Slow Start phase restarts again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f973e386eb414e06bf1ad307200edccf",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "In congestion control with TCP there are the phases Slow Start and Congestion Avoidance. During the slow start phase the number of segments sent is doubled (cwnd *= 2) starting from cnwd = 1 packet until either a timeout is detected or the ss_tresh value is reached. When the ss_tresh value is reached the phase changes from the Slow Start Phase to the Congestion Avoidance Phase in which the number of sent packets is iteratively increased by 1 (cwnd += 1) until a timeout occurs. If a timeout occurs in one of the two phases the ss_tresh value is set to half of the current cwnd value and the packet transmission starts again with the slow start phase with cwnd = 1.",
        "answer_feedback": "The response is correct and complete as it provides the phases' names and changes in the value of the congestion window and threshold correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2dd5b850976a40c295279ec6826c80f1",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "Phase 1: Slow start\nThe basic idea behind \"slow start\" is to send packets as much as the network can accept. It starts to transmit 1 packet and if that packet is transmitted successfully and receives an ACK, it increases its window size to 2, and after receiving 2 ACKs it increases its window size to 4, and then 8, and so on.. \"slow start\" increases its window size exponentially until the slow-start threshold is reached.\n\nPhase 2: Congestion Avoidance\nAfterwards, the congestion window is only incremented by one unit if all packets from the window have been successfully transmitted. It therefore only grows linearly per roundtrip time. This phase is called the Congestion Avoidance Phase. If a timeout occurs, the congestion window is reset to 1 and the slow-start threshold is reduced to half of the congestion window. The phase of exponential growth is thus shortened, so that the window grows only slowly in case of frequent packet losses.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1306c9f41b99434992775ff024018ca3",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are Slow start(getting to equilibrium) and Congestion avoidance.\nThe slow start is to discover the correct sending rate where initial congestion window size is 1 and each time the segment is acknowledged, then increasing it by one this is done until the slow start threshold is reached or ends with packet loss.\nIn case of congestion avoidance, when the congestion occurs then we reduce the value of slow start threshold to half the value of the congestion window and then resetting back the value of congestion window to 1 and starting the slow start phase again.",
        "answer_feedback": "TThe response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "591a014b909142f7b06e1989d3d4f8b8",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "cwnd indicates how much data can currently be sent at once before waiting for acknowledgements.  cwnd is doubled every time all remaining acknowledgements are received and as long as cwnd is lower than ss_thresh. Once a packet times out (no acknowledgement received), cwnd is reset to 1 and ss_thresh is set to half its previous value (phase one - slow start).  \n\t After surpassing ss_thresh, cwnd will only increase linearly (phase two - congestion avoidance) and will revert to slow start after a timeout.",
        "answer_feedback": "The response correctly states the name of the two phases. The explanation of the slow start phase and congestion avoidance phase is correct except that  ss_thresh is set to half its current congestion window value, not half of its previous threshold value. When the timeout occurs in phase 2, the congestion window is reset to 1, and ss_thresh is set to half its current congestion window value and then reenter into the slow-start phase.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "c418cfbf7e6a4722bbbc7dfb3127b1ef",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are called Slow start and congestion avoidance. In the slow start phase the cwnd is less than ss_thresh which mean we send less data than the advertised window. In the congestion avoidance cwnd is greater or equal to the advertised window which means we send more or exactly enough to saturate the receiver, since we started slowly, we know that the network is very likely to handle the traffic.",
        "answer_feedback": "The response correctly states the name of the two phases. The response does not state the condition, nature, and degree of change in the congestion window and slow start threshold. Further, the answer is missing what happens when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "007743572f9d4067b8ce0f1f130b9fe3",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "At the beginning (during phase 1, also called slow start), the Congestion Window (cwnd) is set to one and duplicates with each cycle until ss_thresh is reached. Once cwnd >= ss_thresh, phase 2 starts, also called congestion avoidance. Here, cwnd is only incremented by one until a congestion occurs. Then, the process starts again with ss_thresh = cwnd/2 and cwnd=1.",
        "answer_feedback": "The response is partially correct because it is unclear what is meant by the cwnd duplicating every \"cycle\" in the slow start phase. It is also unclear when cwnd increments in the congestion avoidance phase. The slow start phase description is missing details about how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "e1b72f91834e4b45a7027d22f9150a60",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "The two phases are \"slow start\" and \"congestion avoidance\". In phase 1 cwnd less than  ss_thresh. The congestion window increases exponentially until cwnd >= ss_thresh (so cwnd=1, then 2, then 4 etc.). After the threshold is reached, phase 2 is entered where we have an additive increase and a multiplicative decrease. This means, that the cogestion window now is always increased by 1 every roundtrip time and when a timeout (=congestion) occurs, the ss_thresh is set to 50% of the current size of the congestion window (ss_thresh=cwnd/2), the congestion window is reset to 1 and the slow start (phase 1) is entered again.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "bf7698c484c3475a8f00e77db36bf311",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "1. Slow start: Each time a segment is acknowledged cwnd is incremented by one. Continues until cwnd reaches ss_thresh or a packet gets lost. 2. Congestion Avoidance If congestions occurs ss_thresh is set to 50% of the current cwnd an the new cwnd is set to one. Then the slow start is entered.",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. The explanation of the congestion avoidance phase is also partially correct as it does not mention how the congestion window increases in this phase, exponentially or linearly.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "1e021994a2384738a5f799aa7178d395",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "2 Phases: Slow Start, Congestion Avoidance At first the cwnd size is set to 1 segment. After each acknowledged segment it will be doubled until it reaches a certain ss_thresh. (Slow Start)\n\t If the threshold is reached the cwnd will only increase linearly by 1 segment until a timout occurs. (Congestion Avoidance) The timeout causes the cwnd to reset to one, the ss_thresh to become cwnd / 2 and to enter the slow-start phase again.",
        "answer_feedback": "The Slow start phase's explanation is partially correct as it does not mention what happens when a packet is lost before ss_thresh is reached. Here the slow start threshold also becomes half of the congestion window, and the congestion window becomes 1. The explanation of the congestion avoidance phase is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "969e03004d514f038f9ba4f9393132ad",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences .",
        "reference_answer": "Slow start (cwnd less than ss_thresh): \nIn the slow start phase, cwnd is incremented by one every time a segment is acknowledged. This results in an exponential growth as cwnd is essentially doubled after each Round Trip Time (RTT). This is done until either a packet is lost or ss_thresh is reached. When cwnd >= ss_thresh, the congestion avoidance phase is entered. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1. \n\nCongestion Avoidance (cwnd >= ss_thresh):: \nIn the congestion avoidance phase, cwnd is incremented more slowly. There are different incrementation strategies, but they usually grow linearly, e.g. only increment cwnd by 1 after all sent segments have been acknowledged. This is done until a packet is lost. Typically, this means that cwnd less than ss_thresh and the slow start phase is entered again. \nAfter a packet is lost / congestion the following adaption is made in both phases: ss_thresh = cwnd / 2. Then cwnd is reset to 1.",
        "provided_answer": "phase 1: Slow start\n\t \nthe cwnd starts with one and increases (with factor 2^x until a specific value then it grows linearly with +1) until the threshold is reached or packet loss occurred\n\n\nphase 2: congestion avoidance\nif cwnd >= ss_thresh or package loss occurred, the ss_thresh is set to 50% of the cwnd value.\nThe cwnd is reset to 1 and increased again until ss_thresh is reached or package loss occurs (new slow start)",
        "answer_feedback": "The response is partially correct because the slow start phase's description is missing how ss_thresh changes when a packet is lost. Also, the congestion avoidance description does not state how the cwnd increases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "c62baa80e1c94df38d7f6a0447802d32",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Initial value of cwnd is one and increases by one in phase 1 slow start, when a segment is acknowledged. This phase continues until value of cwnd reaches ss_thresh which equals to advertised window size or data packet loss occurs.\nCwnd increases by only one in each roundtrip time in phase 2 congestion avoidance until congestion occurs. When congestion occurs, ss_thresh is set to 50% of the current cwnd and then cwnd is reset to one and slow-start is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "2bec0e954d5a411caa5e5d3d5e55f0ca",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "There are two phases: 1) slow start and 2) congestion avoidance.\nDuring the slow start (cwnd <= ss_thresh) each ACK generates two packets, hence the data rate grows exponentially. In the meantime cwnd grows for each ACK and when cwnd > ss_thresh, we enter congestion avoidance phase.\nDuring congestion avoidance phase, the data rate doesn't grow exponentially anymore and each ACK only generate one new packet. cwnd only grows until we get some timeout/congestion.\nWhenever that happens, ss_thresh is set to half of cwnd and cwnd gets resetted (ss_thresh = 0.5*cwnd; cwnd = 1).",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "f5c948e8db1f4b2bb40636f0c730bf59",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Conjestion control with TCP has 2 phases: 1) Slow start and 2) Conjestion Avoidance\n\nSlow start -> After initialization, in each step, cwnd is increased exponentially (2^x), until it reaches(or surpasses) ss_thresh.\n Conjestion Avoidance -> Then, it starts increasing only by 1 in each step.\nIf we receive a timeout (congestion), the process starts again, but this time we set new ss_thresh = cwnd / 2 and cwnd = 1.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "03552f08b6cf48fb958205772dbb9728",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phases of congestion control:\n1) Slow Start\n2) Congestion Avoidance\nCwnd is incremented by one when a segment is acknowledged and\nIncreasing (exponentially bc more and more segments are sent in same time interval) until it reaches ss_thres or until a packet is lost. \nWhen ss_thresh is reached only one segment is sent per Roundtrip time-> cwnd increses not as fast as before.\nWhen congestion occurs ss_thres is reduced by 50%of cwnd and cwnd is reset to one and slow start is repeated until proper sending rate is discovered.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "64641c804fd54719afd0cf896b33b4f1",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The 2 phases of congestion control with TCP:\n1)\u00a0Slow start\u00a0\n2)\u00a0Congestion Avoidance\n\nAt 1): The Congestion Window (cwnd)\u00a0becomes incremented each time a segment is acknowledged until a package is lost or\u00a0ss_thresh reached.\u00a0When cwnd >= ss_thresh, TCP slows down the increasing of cwnd by adjusting tge transmission rate .\u00a0\nAt 2): Each time a congestion occurs ss_thresh is\u00a0 set to half of the size of cwnd (ss_thresh = cwnd / 2) and cwnd is set to 1. Then the slow start phase begins again.",
        "answer_feedback": "\" When cwnd >= ss_thresh, TCP slows down the increasing of cwnd by adjusting tge transmission rate . \" should be in phase 2. Also congestion canoccur in phase 1 and change in the cwnd needs to be mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "e9773928214748d794f2e73e71c2b829",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase1 Slow start:\nEvery received Ack increases the cwnd by one until the threshhold of ss_thresh.\nPhase 2 Congestion control:\nWhile avoiding congestion, every RTT increases the cwnd by one. When a congestion occurs the ss_thresh is set to half the current window size. (cwnd) and cwnd are set to 1 and we move back to phase 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "5926686169154beebd5efc0dd26a8abe",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow Start\nPhase 2: Congestion Avoidance\n\nDuring phase 1 each acknowledged segment increases cwnd by 1, which ends up doubling it every round trip.\nDuring phase 1 ss_thresh doesn't change.\nAfter cwnd reaches ss_thresh phase 2 begins, in which cwnd is increased by 1 every round trip until a timeout occurs.\nWhen a timeout occurs ss_thresh is set to cwnd/2 and cwnd is set to 1.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "49210fb2d0dd4a8086c20ac2792335e5",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The two phases are\nSlow start (getting to equilibrium) andCongestion Avoidance.In the first phase cwnd is increased each time a packet has been acknowledged until ss_thresh is reached or a package is lost. When ss_thresh is reached, the increasing of cwnd (until now doubled each RTT) is slowed down.If an timeout occures in the second phase, ss_threshold is set to cwnd/2, cwnd is reset to 1 and slow start procedure is executed again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "30c4cf271dd44a3faef21c346005ec4a",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1 is Slow Start. Here the sender tries to send from a small amount of packets to as much packets as possible. For each send the sender increase the amount of packets exponential until the receiver no longer acknowledge the packages. After that the\u00a0Phase 2 is Congestion Avoidance starts where the sender increase the amount of packets linear instead of exponential. This happens until the receiver no longer acknowdledge and then the Slow Start Threshold is set to 1/2 of the Congestion Window size.",
        "answer_feedback": "Phase 2 starts when cwnd>=ss_thresh, not when congestion occurs.\u00a0\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "44e236965aa346c88a2a4c117685734f",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "First Pahse is the slow start phase. Second phase is the congestion avoidance.\nFirst Phase:\nFor every acknowledged packet the cwnd increases by the Maximum Segment Size\nss_thresh is not changing in this phase\nSecond Phase\ncwnd increases with each acknowledgment received linearly. 1/cwnd is the increase\nss_thresh is set to half the congestion window size on timeout",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached(ss_thresh is set to half the congestion window).",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "81c15d7c1f02474cb102897d8fa54565",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phases of congestion control: \n- 1. Phase: Slow Start\n- 2. Phase: Congestion Avoidance\nFirst, in the slow start phase, where the cwnd is smaller than the ss_tresh, the cwnd is initialized with cwnd = 1. Now the sender can send one segment in the initial set-up. For each acknowledgment, the sender receives the cwnd increases by 1, which means that cwnd doubles his size every RTT. When the cwnd is greater or equal the ss_thresh (cwnd >= ss_thresh) the second phase congestion avoidance begins where the whole cwnd increase by 1 every RTT. ss_thresh doesn't change until congestion occurs.\nEach time congestion occurs the ss_tresh is set 50% of the current size, the congestion wind is set to cwnd = 1, and slow-start phase begins again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "5d08997f0bf94fecaea3eac706c75c59",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1. slow start: cwnd starts smaller then ss_thresh (cwnd < ss_thresh). cwnd gets increased incrementally until it reaches ss_thresh.\u00a0\n2. congestion avoidance (when congestions occurs: decrease ss_thresh = cwnd/2 and set cwnd = 1)",
        "answer_feedback": "In the slow start phase, the case when the packet is lost before the threshold is reached is not covered.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "69dafdb3216d4d8990754beecf32b17a",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Congestion control consists of the slow start phase, where cwnd<ss_thresh applies and cwnd is increased exponentially until packet loss or reaching ss_thresh, and the congestion avoidance phase with cwnd >=ss_thresh when ss_thresh is set to cwnd/2 and cwnd to 1 after every congestion and the slow start phase restarts.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "34938aa459614a47a619b783efb1783f",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The name of first phase in slow start(getting to equilibrium) and the name of the second phase is congestion avoidance. For slow start, ss_thresh is a constant value and each time a segment is acknowledged, we increment cwnd by one everytime until it reaches the threshold ss_thresh(cwnd >= ss_thresh) for which it slows down the increase of cwnd. For congestion avoidance, ss_thresh is set to 50% of the size of congestion window(ss_thresh = cwnd/2) and cwnd is set to 1(cwnd = 1)",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "b8029999e0e34c52944bb08f9aea49b9",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Phase 1: Slow start\nPhase 2: Congestion Avoidance\nAfter initialization of cwnd and ss_thresh, cwnd is continuously incremented, increasing the sending rate, as long as segments are acknowledged. This happens until cwnd reaches ss_thresh upon which TCP slows down the increase of cwnd. Every time congestion occurs, ss_thres is set to cwnd / 2 and cwnd reset to 1.",
        "answer_feedback": "The response needs to be specific about the rate in both phases.i.e exponential increase in phase 1 and linear increase in phase 2.\nIn the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "id": "8c17fd95d0b54b5aad7d8e9277928a56",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "In the first phase the cwnd starts with very small packet sizes and starts doubling the package size after each acknoledged transmission.\nAfter reaching the treshhold value of package size, the second phase - congestion avoidance is active.\nNow the package size only increases linear. If a transmission fails / times out, the new slow start threshold is set to the half of the last succesfull package size.\nThe first phase is now active again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "e3fbd9b2d6ab4ecaa681397b61d71746",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "After the initialization, the \"Slow Start\" phase is entered. The cwnd is increased for every acknowledged packet, leading to an exponential increase. When cwnd >= ss_thresh, the \"Congestion Avoidance\" phase is entered in which the cwnd is only increased by one in every (error-free) roundtrip time. Whenever a packet is lost, the ss_thresh is set to cwnd/2, cwnd to 1 and the \"Slow Start\" is entered again.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "85c3ef3ae5ee4fb1b16aa96c0dcdd995",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "The first phase is called slow start and there cwnd is initialized to 1 and increases exponentially fast (for each acknowledged segment: cwnd++) as long as\u00a0 the congestion window is below the threshold and the acknowledgements arrive before their timeouts. Once the window size exceeds the threshold, the congestion avoidance phase starts and there the congestion window grows lineary (incremented by one). In both cases, in case of a timeout, the threshold is set to half the value of the current congestion window and cwnd = 1.",
        "answer_feedback": "Correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7c2a0aad153147429670bb8b67c6e3e2",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "first, we set our ss_thresh to a number. Then we are sending one packet until its ackknowledged. After the packet is ackknowledge we increase the cwnd by one. We repeat those steps until we reached our ss_thresh. After we reached ss_tresh, TCP slows down increasing cwnd.",
        "answer_feedback": "Phases Name not mentioned. What happens when congestion occur in either of the phases.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.63
    },
    {
        "id": "0835bb94ffb14e98ae3e703659876303",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "1. Slow start (getting to equilibrium)\nEach time a segement is acknowledged increment cwnd by one.\nContinue until reach ss_thresh or packet loss.\n2.Congestion avoidance:\nIf ss_thresh is set to 50% of the current size of the congestion window (ss_thresh = cwnd/ 2),\ncwnd is reset to 1 and slow start is entered.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.In the congestion avoidance phase, the cwnd is increased linearly before congestion occur is also not mentioned.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "0c93f7255cb740fbb51b10850c59417e",
        "question": "In the lecture you have learned about congestion control with TCP. Name the 2 phases of congestion control and explain how the Congestion Window (cwnd) and the Slow Start Threshold (ss_thresh) change in each phase (after initialization, where cwnd = 1 and ss_thresh = advertised window size) in 1-4 sentences total.",
        "reference_answer": "Slow start: ss_thresh is constant, increment cwnd by one every time a segment is acknowledged until ss_tresh is reached, then slowed increase of cwnd Congestion Avoidance: cwnd is reset to 1 after adjusting ss_tresh = cwnd / 2",
        "provided_answer": "Slow start: cwnd starts at 1\u00a0and doubles after every roundtrip\u00a0till ss_thresh or packet loss is reached.\nCongestion Avoidance: After this Congestion Avoidance is reached and cwnd grows by 1 every round trip till congestion occurs. Than it gets reset to 1 and\u00a0ss_thresh = cwnd /2. At this point slow start is used again.",
        "answer_feedback": "In the slow start phase, what happens to the cwnd and threshold when the packet is lost before the threshold is reached.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "76721a698a8b48aaa23b358a581e33c7",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "least probable -> B -> C -> A -> most probable\n\nEvent B is a fixed sequence, so there are no variations possible to increase the likelihood of the event happening. It is therefore the least probable of the three events.  \nEvent A has to be more probable than event C, because it includes all results of event C, and more.  \nC's sample space is a subset of A's sample space, and B's sample space is a subset of C's sample space, meaning that A is the most likely event and B the least likely.\nWith events A and C being more probable than event B, and event A being more probable than event C, the order is clear.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "63e22be75c7c496cb9ab06e3d369e10d",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "least probable -> B -> C -> A -> most probable\nB: There is only one case of all possible sequences where event B occur. \nCalculation: P(B) = 0.6^3*0.4^3 = 0.014\nC: If event B occurs, event C occurs too. Additionally event C includes several more cases (e.g. HTTHHT)\nCalculation: P(C) = 6 over 3 * P(B) = 0.276 \nA: Event A includes every case of event C. Additionally event A includes several more cases (e.g. HHHHTT)\nCalculation: P(A) = sum_x=3_N=6 (6 over x * 0.6^x * 0.4^(1-x)) = 0.821\nThe exact probability of H is not relevant in this case (except probability of H is 0 or 1 where the probabilities of some or all events becomes equal).",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "becc5f5e196f40238a88c56d6130983e",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "You can calculate the probabilities according to the binomial distribution.Event B: p = 0,013\nEvent C: p = 0,2765\nEvent A: p = 0,82",
        "answer_feedback": "The response does not explicitly state the order of the events but it contains the correct probabilities of all events which are sufficient to identify the correct order.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "87d77b04bef64bdf829d397bbf89a35e",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "least probable to most probable \nB -> C -> A \n\nB: 0.6^3*0.4^3 = 0.013824\nC: (6 nCr 3) * (0.6^3*0.4^3) = 0.27648\nA: P(X>=3)= (6 nCr 3) * (0.6^3*0.4^3)+ (6 nCr 4) (0.6^4*0.4^2)+ (6 nCr 5) * (0.6^5*0.4^1)+(0.6^6*0.4^0) = 0.8208",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ba1e1b0f3b8d4a29a578a285f1f09adf",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B \u2192 C \u2192 A\n    B includes only the sequence HHHTTT.\n    C includes all sequences, where there are 3 H\u2019s. In particular the\n    sequence contained in B.\n    A includes all sequences, where there are at least 3 H\u2019s. So in\n    particular also all sequences contained in C.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d9d28267be104404a93bc2b56eebf732",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Least probable --> most probable\nB,C,A\n\nP(B) = 1 * 0,6^3 * 0,4^3 = 0,0138\nP(C) = (6 over 3) * 0,6^3 * 0,4^3 = 0,2765\nP(A) = P(k=3)+P(k=4)+P(k=5)+P(k=6) = 0,8208",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3387e8c89272421c9101bede5032b4e5",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "B -> C -> A\nB is the least likely, as there is only 1 combination of all 2^6 combinations possible which is exactly this combination.\nC is more likely than B as there are more than 1 combination possible (e.g. HHHTTT, TTTHHH) (10 total).\nA is more likely than C as all combinations that are valid under C are also valid under A plus for example the combination (HHHHTT) which makes it a strict supergroup to the group of combinations valid under C.",
        "answer_feedback": "The response correctly states the sequence of the three given events. C's justification is partially correct as the total number of possible combinations for C is 20, not 10. The justification provided for A is also partially correct as C is a more strict subgroup than A.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b3ed66e744ee47d4848f081e44147029",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event B -> Event C -> Event A. If we were to draw a decision tree for the six throws, we could count the number of paths that fulfill each condition. Condition B would only have a single path, because it has a precise outcome attached to each of the throws. If we compare events A and C, we will notice that C describes a subset of A, thus having fewer paths to fulfill the condition. Every path with exactly three H\u2019s is included in event A, but the paths that result in 4, 5 or 6 H\u2019s are neglected for event C.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b028875e329744858f1baf88dbf9a8b5",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event C, Event A and Event B",
        "answer_feedback": "The stated order of events is incorrect. The correct order is B, C, A. Additionally, the response lacks a justification regarding all event probabilities.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "6acb4cedd5174798979bc452456a9329",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Event A:\n- The number of H\u2019s follows a Binomial distribution with parameters N=6 and p=0.6\n- P[Y>=3] = P[Y=3] + P[Y=4] + P[Y=5] + P[Y=6]\n- P[Y>=3] can be calculated as follows: ((6 choose 3)*0.6^3*(1-0.6)^(6-3))+((6 choose 4)*0.6^4*(1-0.6)^(6-4))+((6 choose 5)*0.6^5*(1-0.6)^(6-5))+((6 choose 6)*0.6^6*(1-0.6)^(6-6))\n- The probability for event A is 0.8208\n\nEvent B:\n- The probability that the coin flip results have a specific order is just the product of the probabilities of each desired result\n- P[HHHTTT] = 0.6^3*(1-0.6)^3\n- The probability for event B is 0.013824\n\nEvent C:\n- P[Y=3] = (6 choose 3)*0.6^3*(1-0.6)^(6-3)\n- The probability for event C is 0.27648\n\nHence, the order of the events is B,C,A. This makes sense because B is the most specific event and A the most general.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "80c0b3b38933498ea301efa5a1dc7d26",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "least probable: B (likelihood = 0.6^3 * 0.4^3)\nsecond least: C ( we don't care about order, so there are 6!/ (3!*3!) possibilities)\nmost probably: A since it includes C but also the probability for 4, 5 or 6 H's",
        "answer_feedback": "The response is partially correct because the event C also includes the event P(B). Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "084816e6bcee4574b40f6cc9cb9f640b",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "Previous defninition: \" ** \" represents power calculation, e.g. 2**2 == 4. (n\u00a6k) means that pick up k elements from n elements, (n\u00a6k) here is a symbol of combination.\n\nEvent A: the converse-negative proposition(CGP) of event A is \"you see at most two H's\". With 1 subtractes the probility of CGP, we are able to get the result of event A.\nCalculation of Event A: 1 - [ (6\u00a60) * (0.6 ** 0) * (0.4 ** 6) + (6\u00a61) * (0.6 ** 1) * (0.4 ** 5) + (6\u00a62) * (0.6 ** 2) * (0.4 ** 4)] = 0.7968\n\nEvent B: since the proposition is \"you see the sequence HHHTTT\", there is only one solution out of all possibilities.\nCalculation of Event B: (1 / 6!) * (0.6 ** 3) * (0.4 ** 3) = 0.0000192\n\nEvent C: you see exactly three H's, the position of each H needs to be taken into account.\nCalculation of Event C: (6\u00a63) * (0.6 ** 3) * (3\u00a63) * (0.4 ** 3) = 0.27468\n\nthe likelihood in increasing order, i.e. least probable \u2192 most probable is\nEvent B \u2192 Event C \u2192 Event A",
        "answer_feedback": "The response is partially correct because it contains the correct order of events, but the calculation part for all events is incorrect.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "9b00eddbeb3645a6abf0f639fe5f4579",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "P(Event B) is  less than P(Event C)and P(Event C) less than  P(Event A)\nEvent A is the most generic event, whereby event B and C are subsets of the probability set A. Therefore, if event B or C occur, A also occurs. This makes A the most probable event. The same logic applies to C and B, whereby C is a more generic (and therefore probable) event than B.\nIn detail: B occurs if the sequence HHHTTT happens. This event has exactly three H's (and three T's). Therefore, this event also includes event C (\"exactly three H\u2019s\"). But, the set of valid sequences for C is bigger than C, i.e. with sequences like HTHTHT. The sequence of the H's doesn't matter in C. Furthermore, event A is also always true if C is true. But the set of A is bigger, since events like HHHHHT are only in A and not in C.",
        "answer_feedback": "The response correctly answers the order of the events with appropriate justification.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2d0efff211264eb18767653fd9c5345d",
        "question": "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable \u2192 most probable): \u25cf Event A: you see at least three H\u2019s \u25cf Event B: you see the sequence HHHTTT \u25cf Event C: you see exactly three H\u2019s Include justification in your answer headers.",
        "reference_answer": "The correct sequence is BCA , where B is the least probable. One of the following justifications should be given:\n\t\t\u25cf Event B is a subset/part/special case of C, and C is a subset/part/special case of A\n\t\t\u25cf Event B is more specific/strict than C and C is more specific/strict than A\n\t\t\u25cf An actual, correct calculation of the probabilities:\n\t\t\u25cb P(B) = 0.6 * 0.6 * 0.6 * 0.4 * 0.4 *0.4 = 0.6^3 *0.4^3 = 0.013824 \n\t\t\u25cb P(C) = (6 choose 3) * P(B) = 0.27648 \n\t\t\u25cb P(A) = P(C) + P(Y=4) + P(Y=5) + P(Y=6) = 1 - P(Y=0) - P(Y=1) - P(Y=2)\n\t\t= (6 choose 3) * 0.6^3 * 0.4^3 + (6 choose 4) * 0.6^4 * 0.4^2 +(6 choose 5) *\n\t\t0.6^5 * 0.4 + (6 choose 6) * 0.6^6\n\t\t= 0.27648 + 15 * 0.020736 + 6 * 0.031104 + 1 * 0.046656\n\t\t= 0.8208 \n\t\t\u25cb The answer may be rounded to up to 2 decimal places, e.g. P(B) = 0.01 or\n\t\tP(B) = 0.014\n\t\t\u25cb It is also sufficient to give a formula without the actual calculation, if it is apparent that P(B) less than P(C) less than P(A), e.g. by stating P(C) = 20 * P(B)",
        "provided_answer": "p(B) less than p(C) and p(C) less than p(A)\n\np(A) must be smaller then p(C) because cases whith more then 3 H's are invalid. p(C) must be smaller then p(B) because the H's and T's must be sorted.",
        "answer_feedback": "The response correctly states the sequence of the three given events. However, the justification states the opposite which is incorrect.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "5f294cda98324fda84e0ca390670124d",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service:\n-This service has no flow control and the data unit does not get acknowledged from receiver. So there are no Timeouts.\nConfirmed Connectionless Service:\n-This service has no flow control and the data unit does get acknowledged from receiver. So there could be Timeouts and retransmits.\n\nConnection-Oriented Service:\n-This service has flow control. Is implemented in a 3-phased communication.\n1. Connection\n2. Data Transfer\n3. Disconnection",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5735037e9a4e476ab99117dade0cb730",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "-unconfirmed connection less service\n-confirmed connection less service\n-connection-oriented service \n\nFor unconfirmed data is send by data request (Data.req) and receiver gets the data stating an indication.\nThe data is send immediately on the contrary to the connection-oriented service. In unconfirmed conn. less service, you have to expect that \nthe sent packet arrives but you have no feedback Acknowledge-signal (ACK) whether the packet arrived. \n\nFor confirmed conn. less service, when data is send via data request and arrived (data indication), the receiver answers to the sender by an \nacknowledgement (ACK) , so there is a feedback signal stating that data arrived successfully. \n\nSo far (for unconfirmed and confirmed connection less service), data is send and acknowledged (for confirmed conn. less) immediately. \nThere is no difference for connection, data or disconnection phases. By using connection-oriented service, you have requests and indications for connection\nestablishment, data transfer and the disconnection. Instead of just sending data, you send a request first for establishing a connection, then\nthere is an indication (=able to receive) on the receiver and the receiver sends a response back, after that the sender has a confirmation.\nThere is the same principle for the phases 'data transfer' and 'disconnection'. \nWith connection-oriented service the system is able to send more than 1 bit bidirectional\nbetween sender and receiver and vice versa.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c8eb34c321b1449cbb4190c417bef0ba",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service (=Frames are transmitted as independent units --> Data can be lost)\nConfirmed Connectionless Service (=Frames still transmitted independently but with acknowledgement)\nConnection oriented Service (=3 Phases: Connection --> Data Transfer --> Disconnection)",
        "answer_feedback": "The response answers the services' names and differences correctly.  But there is no common theme of the differences between them. The last point should also discuss the presence or absence of acknowledgment.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "811b7a820069414bb4157178acbfb298",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "1.\tUnconfirmed Connection Less Service: Has no flow control, connect or disconnect; Sender\u2019s Frames are transmitted as it is without taking in account possible loss during transmission with no acknowledgement from Receiver. Suitable with communication channels with very low error rate.\n2.\t Confirmed Connection Less Service: Sender\u2019s transmitted frames get acknowledged by receiver, may result in duplicate and sequence errors; Still no flow control, connect or disconnect. Better reliability than Unconfirmed Connection Less Service.\n3.\tConfirmed Connection Oriented Service: Starts with connect, then data transfer and end with disconnect. Uses flow control to synchronize frame sequence and prevent loss due to speed mismatch. Reliable compared to 2 types of connection less services.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "57a03a5534194184804ca573c0628408",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "The 3 service classes of the Data Link Layer are the unconfirmed connectionless service, the confirmed connectionless service and the connection-oriented service.\nThe connection-oriented service has a 3-phased communication. First the entities have to connect, then they can transmit data and after this they disconnect. There is a data flow in both directions. The sender gets an acknowledgement if the receiver receives the data, so there is no loss of data. This service offers also flow control and prevents duplication or sequencing error. \nThe other services have no connect or disconnect and no flow control. \nThe confirmed connectionless service can only ensure that there will be no loss of data because the receiver sends an acknowledgement. \nThe unconfirmed connectionless service cannot because only the sender can send data and the receiver sends nothing back.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ea1fd487c58d46a3b5c771c8ce16f72f",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service: Transmission of isolated, independent units which are called frames. Data loss is possible. L2 doesn\u2019t correct this and only transmit correct frames.\n\nConfirmed Connectionless Service: No data loss, because each frame is acknowledged. Timeout and retransmit is possible when a sender doesn\u2019t receive an acknowledgement within a certain time frame.\n\nConnection-Oriented Service: Connection over error free channel. Theres no loss, duplication or sequencing errors. It also features Flow Control.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "818ab13b29984849a1240284a5f4787b",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "The Data Link Layer offers these :\n\n-unconfirmed conn.less service ->  no flow control , no connect or disconnect , sender will never get an acknowledgement from receiver (Radio/Broadcast).\n\n-confirmed conn.less service -> no flow control, no connect or disconnect , Duplicates and sequence errors may happen due to \u201cretransmit\u201d\n\n-the connection oriented service -> No loss, no duplication, no sequencing error ,Flow control  (3Phased communication : connection, Data transfer and disconnection)",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0674617659e94f2e9ef5acae1248a870",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "- Unconfirmed Connectionless Service\n- Confirmed Connectionless Service\n- Connection-Oriented Service\n\nThe first class offers no flow control or feedback if a frame arrived to its destination. The second class offers no flow control, but the sender resends the frame, if an error occured, which is noticable due to implicit acknowledments. The third class resends the frames, too, but detects additionally duplicates and offers flow control due to \"states\" of sender and receiver and 3-phased communication.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "73e94f5e24c2455da27100b560a8f19c",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed Connectionless Service:\nIn this service, the data that is sent by the sender is expected to arrive at the receiver, however, there is no feedback from the receiver's side. Due to a lack of acknowledgement from the receiver side, there exists a possibility of data units getting lost. Furthermore, in this service there is no flow control taking place and neither is there a possibility of connect/disconnect. This form of service is best suited for L1 communication channels with a very low error rate.\n\nConfirmed Connectionless Service:\nThis service is a form of bi-directional communication, where upon sending information the receiver responds with an acknowledgment of whether the information has been received or not. It is particularly used in applications where a high error rate is expected, such as for moving devices. Similar to the unconfirmed connectionless service, no flow control or a connect/disconnect phase takes place. Due to the additional acknowledgement here, no data loss is possible, however, duplication and sequencing errors are still possible\n\nConnection-Oriented Service:\nAs opposed to the connectionless services, the connection-oriented service operates with flow control, achieved by a 3-phase bi-directional communication (connect, transfer, disconnect). This prevents data loss, duplication of data or sequencing errors. This form of service requires some sort of connection management as both the sides agree to perform the communication.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "adab1a2e9d244302af340fd101cfe9b3",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "unconfirmed connectionless service:\n  -  units are transmitted isolated and independently. Errors are detected by upper layers.\n\nconfirmed connectionless service:\n  -  the receiver acknowledges received data with a receipt for data units\n  -  in comparison to unconfirmed c. s. data units can be retransmitted if they are missing/not acknowledged. Retransmitted packages may lead to duplications and sequence errors\n\nconnection-oriented service\n  -  no package loss, no duplication, no error sequences due to flow control\n  -  the sender sends connection request, to make sure the receiver is ready to get data\n -  Data is transferred and the connection is closed after that",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f27f52883c194ad59e4166166aa6da5d",
        "question": "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.",
        "reference_answer": "1.unconfirmed connectionless - no ACK, loss of data possible, no flow control, no connect or disconnect.\n2.confirmed connectionless - with ACK, no loss of data (timeout and retransmit instead\u2192 duplicates and sequence errors possible), no flow control, no connect or disconnect.\n3.connection-oriented - no data loss, duplication or sequencing errors. Instead a 3 phased communication with connect and disconnect, and flow control",
        "provided_answer": "Unconfirmed connection-less service, confirmed connection-less service, and connection-oriented service.\nAn unconfirmed connection-less service offers no feedback, if the sent frame was received. A confirmed connection-less service offers this feedback with a simple acknowledgment per frame, therefore no loss occurs. But the acknowledgment can also lead to duplicates because the sender may have not received the ACK yet and retransmits the frame. In contrast to other two, a connection-oriented service provides a connection without duplication or sequencing errors and provides flow control, because the connection is setup (exchange of parameters, i.e. sequence number) and teared down afterwards.",
        "answer_feedback": "The response answers the services' names and differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fff2da8d44e347959a1f27516b5dee7a",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "DQDB has a fairness problem. The likelihood to access the medium (reserve a slot on other bus) is not the same for all stations because it depends on the location. That means that a station that is wired at the beginning (or end) of the bus, sometimes has an advantage and sometimes a disadvantage compared to other stations at different locations. This is not fair.",
        "answer_feedback": "The response correctly identifies the fairness problem in DQDB and also provides an appropriate reason for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7b88ac884d6743a7ba508c5140c9be5a",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The mentioned problem with Distributed Queue Dual Bus is fairness. Each node when it wants to send something, it has to make \u201creservation\u201d on one side of a bus and then when allocated at the other side of other bus, it is able to send something. For this reason, some nodes which is located near either one or two ends of the two buses might have too much advantage and disadvantage when it comes to transmission reservation (near and far ends). Only nodes which are in between may have average fairness to reach both ends of the two bus.",
        "answer_feedback": "The response correctly identifies the fairness issue in DQDB and also provides an appropriate reason for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7ce6c42b8aab47dcb42ff916e315af97",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "DQDB describes a MAN structure, where every node (mostly LANs) is connected with two unidirectional buses, which have a frame generator at the end. A node can reserve on one bus and then send one the other one. The problem is the reservation is depending on the location of the node and therefore you sometimes have an advantage and sometimes a disadvantage and so it is not fair.",
        "answer_feedback": "The response correctly identifies the fairness issue in DQDB and provides an appropriate explanation for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "85cceca5ac3d4bbcb77b5536095a5e52",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem is the fairness, because it can depend on the location of the station if you have an advantage or disadvantage in getting access to the bus and send data. This is, because to send data a station has to make a reservation on one bus and after the reservation arrives on the other bus, the station can send its data on that other bus (so reservation on one and sending on the other). Therefore the location of a station has influence on the fairness because for example if the station is at the beginning it could be more likely to get a reservation to send some data than it is when the station is at the end of the bus.",
        "answer_feedback": "The response answer is correct because it identifies the correct problem in DQDB including an appropriate explanation.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8f80ae2c67b74a8399292637d61374af",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The main problem with DQDB is fairness. While stations in the middle might have a chance of 50% to send data, stations at the beginning or at the end can have advantages/disadvantages depending on the situation.",
        "answer_feedback": "The response correctly identifies the fairness problem in DQDB which is due to the station location.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bb791aa1baf445d7b0bbc0cb254a9663",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with \"Distributed Queue Dual Buses\" is the fairness problem. The Question that comes up here is how it can be fair, that everybody gets the same access to the data, or respectively depending on the location does it makes a difference in terms of fairness.",
        "answer_feedback": "The response correctly answers the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2bb8dbc21dbf4f659fa263d5ff86dfe9",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "A node A close to the frame generator can pick up a \u201cfree\u201d token and transmit data with a higher probability than a node B far away from the frame generator if the network is highly loaded. Therefore, Node A has a preference when trying to send a packet compared with node B which is a fairness problem. Research to improve the DQDB protocol includes bandwidth balancing schemes to increase the fairness of bandwidth allocation.",
        "answer_feedback": "The response correctly states the fairness issue in DQDB and provides an appropriate reason for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c99af755b29246b99f222838c503c6dd",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "To build a Metropolitan Area Network (MAN), which can combine asynchronous data traffic(IEEE 802.x MAC) and isochrounous traffic (ATM-B-ISDN), the DQDB (IEEE 802.6) was designed.\nTherefore, two unidirectional buses are used. One bus is in the opposite direction of the other. A node is connected to both buses, on one bus data can be requested, on the other one data is sent.\nThe main problem with this architecture is, that a node can request more data than others depending on its position in the network.  So fairness is the issue with DQDB. \nTo sum up:\n\"For a light-to-medium load, DQDB is somewhat unfair.\", Fairness issues of the DQDB protocol,",
        "answer_feedback": "The response states the correct problem in DQDB architecture, including the proper explanation for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d3599bdb681643958fc2527811694bd7",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "the main problem is the \"fairness\" - the nodes in the beginning of the bus can reserve way more data than in the end, to send it to the other bus. this is only based of their position, which is unfair to the other nodes",
        "answer_feedback": "The response correctly identifies the fairness issue in DQDB which depends on the station location in bus.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7201777374034a4d93b8f6a86c7366bd",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Depending on the location a station may not be able to make a reservation. The further a station is at the bus-head end, the less reservating frames is possible due to FIFO - First in first out scheduling. The main issue is fairness as the stations do not have the same chance to access the bus.",
        "answer_feedback": "The response correctly identifies the fairness issue in DQDB and also provides an explanation for it.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "023c9f799ee34803a8369b55db3ca0d0",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "Depending on where a node is connected to the two busses at some spots it is more likely to be able to reserve a time slot and send something than at other spots. Making fairness the biggest problem of this solution for connecting subnetworks since the probability to be able to send depends on the position in the queue.",
        "answer_feedback": "The response correctly answers the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "dd5031ebe19448cf8e3bb86639fa8070",
        "question": "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.",
        "reference_answer": "Depending on your position in the bus station have a disadvantage/advantage when reserving transmission rights.",
        "provided_answer": "The problem with this system was the fairness. Some nodes had better chanes to reserve bandwith for themselves than other nodes.",
        "answer_feedback": "The response is partially correct as it states the issue in DQDB but lacks an explanation of why some nodes have better chances of reservation rights. The possibility to reserve bandwidth depends on the distance between a station and the Frame Generator or the Slave Frame Generator.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "574abc2d63e742d6817ede8cefd56797",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "1. TCP has a flag for indicating that finished\n2. UDP has no sequence number\n3. UDP has no acknowledgement number\n4. TCP has a flag, if the data is urgent",
        "answer_feedback": "There are different types of flags available in the TCP header, but they are all within the flag header field. Therefore, points 1 and 4 are similar and count as one.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "d299fe3447c64ce59ca95fe6d47a65e1",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "While both (UDP and TCP headers) have source and destination port fields, every other part of their headers differ. For UDP only a packet length and a checksum field follows the two previously mentioned fields. TCP needs more information. So after the source and destination port the header is followed by a sequence number field as well as a field for the ack number. The TCP header also stores information on HL/RESV/Flags, Window size, checksum (as in UDP), Urgent Pointer and Options.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8c63e9d168d44078afeda29dbd478471",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "The TCP header has some additional data fields for: Acknowledgment Number, Flags, Advertised Win. or Urgent Pointers.\nIt needs more information in the header to include more Features than UDP.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "1adf9b0d7a094b499c8e4c2525b0a123",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "TCP Header: 1.Reliable bidirectional in-order byte stream 2.Connections established and torn down 3.Multiplexing/ demultiplexing 4.Ports at both ends 5.End-to-end flow control 6.Congestion avoidance UDP 1.UDP is a simple transport protocol 2.Unreliable 3.Connectionless 4.Message-oriented 5. no flow control 6. no error control",
        "answer_feedback": "The question requirement is to identify differences between TCP and UDP headers instead of general differences.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "0ede84ff4ca24cc39870d784c61a3aa7",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "-    Since UDP is a simple protocol that actually sends IP packets with limited header additions to the receiver where the packet is forwarded to the application directly, i.e. w/o reordering, etc. The UDP header consists only of \n     essential needs for data transmission, i.e. a sender and receiver port, packet length, and an optional checksum.\n-    In difference to that TCP it is more complicated, since the goal is to receive exactly the same data as transmitted by the sender, i.e. fully complete and in the right order of the packets. To achieve a reliable connection some \n     additional parameters vs. UDP have to be added in the header:\n      o\tSequence Number: to get the right order of the packets\n      o\tAcknowledgment Number: Needed together with Sequence Number for connection setup to get the starting sequence number (3-Way handshake)\n\t        o\tVarious Flags, e.g. SYN-Flag for 3-Way handshake\n      o\tAdvertised Win. or WIN: needed for Flow Control",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ab0d0297edc84c20927cdec5f9f8ea96",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "1. TCP header size is 20 bytes, but UDP Header size is 8 bytes.\n2. TCP does Flow Control, but UDP does not have an option for flow control.\n3. TCP does error checking and error recovery, but UDP does error checking but simply discards erroneous packets.\n4. TCP has sequence number field, but UDP does not.",
        "answer_feedback": "The first point is partially correct as the TCP header length varies from 20 to 60 bytes and does not have a fixed size. The second and third points are incorrect as there is no context provided for the header field related to them. The fourth point is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.38
    },
    {
        "id": "d855beddaabd4087a4d2eaca1ca455ca",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "Sequence number, Acknowledged number, Urgent pointer, Advertised Window",
        "answer_feedback": "The response is partially correct as it does not state whether these fields exist in UDP or TCP.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "845aa6d5bd0c4cf7a5a154053612a70e",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "The UDP header contains four parts, the sender port, the receiver port, the packet length and the checksum. The sender port is optional and the checksum also. The packet length minimum is 8 byte.\nThe TCP header also contains a source and destination port and a checksum but has some other contents too. So four fields which are different from the UDP header are the sequence number, the acknowledgment number, the HL/RESV/Flags and the advertised window. Additional there is an urgent pointer field and some space for options. The TCP header is also larger than the UDP header.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers. However, the abbreviations, such as HL and RESV should be properly named.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f899415feb6242bb9c6664cd11be53e5",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "1. UDP is connectionless and the TCP is connection oriented.\n2. UDP does not control the errors, this means that if there was a with the order of the packets or one of them got lost, the UDP won\u00b4t correct anything and will send it exactly the same to the application layer. Instead, the TCP corrects all the errors and transmit the packets reliably.\n3. The UDP needs few resources. The TCP instead has higher resource requirements for buffering, status information and timer usage.\n4. The UDP transmission is fast because it is connectionless. The TCP instead needs to wait for the connection establishment and the disconnection to send any message.",
        "answer_feedback": "The response is incorrect as the question asked for the differences between headers, but the answer enumerates differences between TCP and UDP protocol instead.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "d78c7fc4813547f99646c731ded74f34",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "UDP is missing the Sequence Number, the Acknowledgement Number, the HL/RESV/Flags, and the Urgent Pointer fields. TCP has this fields.",
        "answer_feedback": "The response correctly states four differences between TCP and UDP headers. However, the terms HL and RESV should be properly named.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "42694ac849ef4f4db912d4910fa48ae8",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "- TCP has no Packet Length header, which UDP does. This is not needed for TCP, because it is connection-based and once all packets have been acknowledged by the receiver, the connection is terminated. - TCP has a Sequence Number header, which UDP does not. It is used to identify the current packet and ensure the correct order when reassembling the data at the receiver - TCP has an Acknowledgement Number header, which UDP does not. It is used to confirm packet reception and connection setup and termination confirmation - TCP has an Advertised Window header, which UDP does not. It is used by the receiver for flow control, indicating how much data the receiver can currently receive",
        "answer_feedback": "The response correctly states four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "1d5a4b22bbda4683877aff3f2514e8b5",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "There are the differences between TCP and UDP headers:\n1. TCP has a at least 20 bytes variable length header while UDP has an 8 bytes fixed length header.\n2. TCP provides extensive error checking mechanisms since it uses checksum but also provides flow control and acknowledgement of data while UDP has only basic error checking mechanisms using checksum. In TCP, Erroneous packets are retransmitted from the source to the destination.\n3. TCP header contains the sequence number of data but UDP does not contain the sequence number. There is no sequencing of data in UDP.\n4. TCP header has urgent pointer to show receiver where urgent data ends but UDP does not have this urgent pointer.\n5. TCP contains information for not only compulsory but also optional functions while UDP has only the information for compulsory function.\n6. UDP is comparatively faster, simpler and more efficient than TCP. However, TCP is reliable since it guarantees delivery of data to the destination.\n7. TCP is heavy-weight, since TCP requires three packets to set up a socket connection, before any user data can be sent. TCP handles reliability and congestion control. However, UDP is lightweight. There is no ordering of messages, no tracking connections, etc.\n8. UDP is connectionless, but TCP is connection-oriented. Therefore, TCP header has fields used for connection setup and maintenance, specifically the flags. UDP does not.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers. The response contains extra points, but they are general differences between the TCP and UDP protocol.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "895fa634de6140e8a23cd5c85c300d5e",
        "question": "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.",
        "reference_answer": "Possible Differences :\nThe UPD header (8 bytes) is much shorter than the TCP header (20-60 bytes)\nThe UDP header has a fixed length while the TCP header has a variable length\nFields contained in the TCP header and not the UDP header :\n-Sequence number\n-Acknowledgment number\n-Reserved\n-Flags/Control bits\n-Advertised window\n-Urgent Pointer\n-Options + Padding if the options are\nUDP includes the packet length (data + header) while TCP has the header length/data offset (just header) field instead\nThe sender port field is optional in UDP, while the source port in TCP is necessary to establish the connection",
        "provided_answer": "- The TCP header has a sequence number to enable reliable communication, the UDP header doesn't.\n- The TCP header has a acknowledgment number, which is used in the 3-way handshake. It is also necessary for the reliable communication. UDP doesn't need this field, because it is a connectionless protocol.\n- The TCP header has some flags, which are used for connection management. For Example SYN for connection establishment and FIN for connection release.\n- The TCP header has a field for the advertised window, it is used for flow control. UDP doesn't have any flow or congestion control.",
        "answer_feedback": "The response correctly identifies four differences between TCP and UDP headers.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a8c1154f40654d49a8b259446fd79fa3",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption mostly does not hold for real internet traffic as packets are not sent independently and in a steady way. Rather, internet traffic is very bursty. Data and messages are not sent via individual and independent packets so a single packet most likely is part of a larger message with which it is sent together.",
        "answer_feedback": "It is correct that the assumption does not hold because of bursty traffic. However, the explanation for why the traffic is bursty only refers to packet fragmentation which is only a small reason for bursty traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "2ae50d00ea364c1f933d0f6b3a67b6cc",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No it does not hold for real internet traffic. Take video traffic as an example. Client fetches data from server and fill them in its buffer. Data could come more and more if the previous of them has been transmitted and filled successfully. If not successfully, the retransmission can happen. Therefore each time interval are dependent on others, i.e. they are not independent.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9c41490c7ccf47bfb7914755eb5a05e4",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "This assumption is not realistic for real internet traffic. It is different between day and night. For example in video traffic, a window buffer may be on to fetch the next segments of video, then off to deplete it, and then on again to fetch the next further segments of video.",
        "answer_feedback": "One can use a function instead of a constant to model the arrival rate to reflect such large-scale, day-night behavioral patterns. The arrivals would not depend on previous arrivals then, only on the time of the day, which is known. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "09f72b0867b949da9738b12062e5747d",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No the assumptions does not hold, since the arrivals depend on each other. For example one request to a server leads to multiple packets being sent back. Also the answer of the server then might lead to multiple requests afterwards. Thus the packets come in stacks with time in between, since the server and the client need to process the packets.",
        "answer_feedback": "The response is correct except that it attributes the bursty nature of the internet traffic to the request/response model which is not always the case.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "451bd0e0ee4942cfb9de5158be4fa341",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "After all, this is not less and not more than a simple model. The details of internet traffic including bursty traffic situations, streams like video (e.g. YouTube, Netflix) or audio (e.g. internet telephony) or the traffic depending on daytime or season cannot be modelled accurately using a Poisson process. There is some derivatives of the Poisson process to better model the bustiness, e.g. the compound Poisson process.\nHowever, the Poisson process has some nice mathematical features allowing a simpler math and therefore allowing to analytically describe queueing systems. Therefore it is widely used for a first analysis or for more mathematical modelling. If a system needs to be analyzed in more detail, simulations tools (e.g. Opnet) can be used. This does not lead to a mathematical formula but delivers more precise numerical results and enables the modelling of a wide variety of data source models.",
        "answer_feedback": "The question asks whether it is true that the arrivals at a node depend on previous arrivals for real internet traffic instead of whether the Poisson process is a realistic model. The reasoning is correct for both questions, but the definitive answer to arrival independence is missing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "d996cc3e109a4b56967621458b111446",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No this assumption does not holds for the real internet traffic because in real scenarios whenever the packet arrives then it is expected that much more packets will be received and if pick a time duration for that such as the night or day then there will be a variance in the packets arrival as we will be checking for packets during the day and which will be more also similarly for a video buffering application, the interval at which we request the packets will be different and infrequent.",
        "answer_feedback": "The response is partially correct because the arrival process' parameters can be time-dependent. In this way, the arrival rate wouldn't depend on the previous arrivals, but instead on the time of the day.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "818672b5e7994da38c1a65c64b7a04cd",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, this assumption does not hold for real internet traffic. If somebody for example streams videos on the internet, the probability that after the first packet another packet will arrive is much higher than the probability for the first packet. If you increase the interval \u0394t there is the possibility the assumption become true again, but it\u2019s not a realistic case.",
        "answer_feedback": "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "66742f9eb5224f138c7de8e166cdcd4a",
        "question": "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval \u0394t are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. ",
        "reference_answer": "No. Real internet traffic often comes in bursts. Therefore, arrivals are not independent because the probability of an arrival happening at a node is influenced by previous arrivals at the node. For example, on-demand video streams selectively load the next video segments when needed. This means the actual network utilization depends on the current playback state on the client-side. The packet arrivals are not independent, as it is likely that there is a low utilization directly after the next segments have been loaded.",
        "provided_answer": "No, because sometimes many users want to access the server at the same time, while at other times, only few request the server. For example a livestream of a football match: everybody sends requests to the server at kickoff, but only few do after the game (to watch the highlights). That means that the arrivals are not independent. They can depend on other events.",
        "answer_feedback": "While the response explains the event point of view, internet traffic is generally bursty, independently of specific events. This makes the packet arrival at a node depend on the previous arrivals.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f1e47215ce794d0898d2143f3133e552",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table bridge holds information of the following: table: station \u2192 LAN. This means it stores information on how to reach a certain source address and over which LAN it can be reached. This table is updated with backward learning. The bridge works in promiscuous mode and receives any frame on any of its LANs. The bridge receives frames with source address Q on LAN L. This means Q can be reached over L. This information is then stored in the table. In the forwarding process this can be used to forward it to the next bridge which then can forward it according to its own table. The benefit of that is that an urgent packet can be directly forwarded without any routing required.s",
        "answer_feedback": "The response incorrectly explains how this information is used in the selective forwarding and the stated benefit is also incorrect. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "2d7c7c39159e4290b4663745c2344c4b",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds information about which device/system can be reached on which LAN. The table initially is empty and during backwards learning, the bridge monitors each frame, checks its source address and creates a new entry in the table with the source address and the LAN from which the frame arrived (if necessary).  When receiving a frame, the bridge checks its table to determine if the receiver is on the same LAN as the sender. If that is the case, the frame is dropped by the bridge, because bridging in not needed. If the receiver is on a different LAN, the bridge forwards the frame to the correct LAN, if the bridge doesnt know which LAN the receiver is on, it tries to find that out by using FLOODING.",
        "answer_feedback": "The response does not mention the benefit of using the bridge table in selective forwarding. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "e4456ca9eda64759ad25194818e0313b",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Initially, the bridge table, containing fields for the sender, the receive timestamp and the used LAN, is empty and as a result the bridge will simply use flooding to reach an unknown destination. During the backwards learning phase the bridge works in promiscuous mode - receives all frames from all its LANs - and uses the information of those frames in order to build its table. The information from the table then allows the bridge to forward incoming packets to the correct LAN in order for it to reach its destination. The benefit of this method is that the bridge learns and adapts over time and does not have to rely on flooding, therefore optimizing bandwidth usage.",
        "answer_feedback": "The response does not state which information is learned from the received packet and how it is used while selectively forwarding packets. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "06ad8bd350334abab4216fb0a9aee15d",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "A bridge connects some different LANs the bridge table contains the information which LAN needs to be accessed to reach a certain destination address. At the beginning the bridge does not know the topology and uses flooding to forward packets to the right destination. Once a correct route is found a new table entry with this new information is added to the table. This process of slowly getting to know the topology is called backwards learning. When a packet arrives at the bridge and its destination address is already in the bridge table there is no need for flooding the packet can be forwarded directly according to the table entry.",
        "answer_feedback": "The response does not mention what is learned and interpreted on receiving a packet from source S over link L, i.e. S can be reached over L. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6b77ad565c9b43d2856b79860ae62950",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "-The table holds the next hop, i.e. the next LAN (output line), for each station. Is the station not known or the table is initialized flooding will be used.\n-\tBackward learning: Bridge can learn about the other LANs with every received frame from each LAN it is connected to.\n-\tThe decision procedure is as follows: If the \n      o\tsource and destination LANs are identical, the frame is dropped, if the\n      o\tsource and destination LANs differ, the frame is rerouted to destination LAN and if the\n      o\tdestination is unknown, flooding is applied.\n-\tBenefit: The network is transparent and adaptive.",
        "answer_feedback": "The response correctly states what information the bridge table contains and how the selective forwarding uses this information. In the backward learning process, the bridge learns about the mapping between outgoing LANs and stations, not just about connected LANs. The stated benefit is not related to the selective forwarding process.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "cb150378592c4c2d8336ae0568f121f3",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds the information to what LAN/other bridge a package should be send to in order to reach its destination. \n\nIf a frame is received and\n- its destination lies in, or is routed over the over the same LAN from where it is received it is dropped.\n- the destination is unknown the network is flooded\n- otherwise the package is rerouted to the next LAN according to the table.\n\nThis makes the Bridge self sufficient and other components of the network does not need to know about the bridge. \n\nWhenever a frame is received the Bridge knows that the source can be reached over the incoming LAN and creates a table entry or updates its table accordingly. \nTo update for changes in the topology a timestamp is used for each entry and refreshed for each corresponding received frame. If a timestamp gets to old it is assumed that the entry is valid anymore and flooding is used the next time a frame with a corresponding destination is received.",
        "answer_feedback": "The response does not mention the benefit of using the bridge table in forwarding packets, i.e. fewer duplicates. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ed948069470e4c2e94bbe808037d1ac5",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table holds the information which output lines/LANs to use to get to a certain station. The bridge inspects the traffic (backward learning) and when the bridge receives frames with source address Q on LAN L it learns that Q can be reached over L and creates a table entry accordingly so it can adapt to changes in topology. When the bridge gets a frame with the identical source and destination LANs in its table, the frame can be immediately dropped since it was already on the right LAN. When the source and destination LANs differ, the frame is rerouted to the destination LAN. The frame is only flooded when the destination is unknown; with this decision procedure unnecessary flooding or forwarding is prevented.",
        "answer_feedback": "The response answers all the four requirements of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5c493749c9af4b7bb95e6650709aab42",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table assigns each adress the bridge knows about to a network. When the bridge is asked to forward a packet to a receiver, it looks at the table to see to which network it has to send the packet. That way packets that have the same source and destination network can be ignored by the bridge and packets with a different source network can be sent on the direct path to that network. Initially the table is empty, but the bridges listens to all packets of the networks it is connected to and  whenever a packet is sent, the bridges wirtes an entry into its forwarding table containing the sender adress and the its network. If the bridge is asked to send a packet to an adress it has no entry for, it floods the packets in all other connected networks.",
        "answer_feedback": "The response does not mention the benefit. Additionally, the table contains station, outgoing LAN, and the timestamp which is not clear from the first line. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "804ce5f9fd98426996e348ae44ffa96d",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Transparent bridges maintain a table that maps each station to an output line or LAN the bridge is connected to. Initially the table is empty, the bridge reads packets from all its output lines and learns where the stations are located. This phase is called the backwards learning phase. During the forwarding process, the bridge uses the table to make the following decisions. If the source and destination are known and are from the same LAN, the packet is dropped. If the source and destination are known and are from different LANs, the packet is forwarded to the correct output line. If the destination is not known, then the packet is flooded on all its lines. Benefits of such a forwarding process is, only frames that need to cross the bridge are forwarded. They also reduce collisions by creating a separate collision domain on each line of the bridge.",
        "answer_feedback": "By observing packets, the bridge learns through which LAN a station can be reached, not where the station is located. This information is used in selective forwarding. Apart from this, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "32df26c8533744bdbf877f2551ace2f2",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Each entry in the bridge table holds: a destination address S, which LAN (link) L is to be used to reach S, and a timestamp T of when last the frame from S arrived.\n\nWith backward learning, the bridge learns from an incoming frame and the LAN connected to the link through which the frame was received that the frame's source S can be reached via LAN L (directly or through other bridges) and stores/updates this information (S, L and frame arrival time T) on any arrival of a frame with S as the source address (and purge it by another process if it is too old, that's why T is stored).\n\nThe forwarding process uses the bridge table by looking up the entry for a given destination address S to decide what to do with an incoming frame addressed for S: if an entry is found then the frame is either rerouted via the destination link L if that link is different from the incoming link or discarded (as it should not leave the LAN), else flooding is used as a fallback (destination LAN unknown).\n\nThe benefit is a reduced network load (compared to flooding the network) if an entry exists for a destination because then only the relevant outgoing link is used and no needless copies of the frame are created and flooded into the network.",
        "answer_feedback": "The response answers all the four requirements of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "ea3e296eb7af4e1f94af12cce838aea2",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "Transparent bridges store destination addresses and start with an empty forwarding table.This table is populated by using backward learning (once the bridge receives a frame from an unknown source over router x, it stores the address information of the source). Whenever a frame is sent to an unknown destination (no table entry), that frame is then flooded and the destination responds to the bridge to create a table entry. Transparent bridges are invisible to other components during the forwarding process, thus simplifying other components.",
        "answer_feedback": "The response has the following errors a) Transparent bridges store not just the destination but also the corresponding outgoing LAN along with the timestamp. b) the stated backward learning process is incorrect. c)Yes, the Transparent bridges are invisible but that is not the benefit derived from using selective forwarding instead of flooding.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "0a1f610a742642bda082343309a95528",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table holds the infromation which Source can be reached over which LAN. During the backwards learning process the bridge creates table entries, when it recieved a frame. When it gets a frame from source X over LAN Y, a new entry will be created that X can be reacher over Y. During the forwarding process the bridge deciedes based on the information table over which LAN the frame should be routed (In case the source an the destination LAN are the same, the packet can be dropped). If the table does not hold any information about the destination point, it will be flooded. Since the bridge keeps track of which LAN contains which station (MAC address) it usually only has to forward a frame to the destination LAN containing this station without having to send it to all connected LANs.",
        "answer_feedback": "The response answers all the four requirements of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "922fbaffa4074aa0a330867faac7c11c",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The table possesses information about the stations and in which  LAN they are located.The bridge works in promiscuous mode, that means that it receives any frame on any of its LANs and with the information stored on these frames of how the stations can be reached, it will create a table entry. In the forwarding process if the source and destination LANs differ the frame will be rerouted to the destination LAN (the information of where this destination is found is on the table).",
        "answer_feedback": "The response does not mention that if a packet arrived over link L from source S, it would use the same link L to forward packets destined for S. The response does not state the benefit of using the bridge table information. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "033f83a632ab41edb47c0e99949aa0fd",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table contains information, which source addresses can be reached via which output line. \nWhenever a bridge receives a frame from one of its LANs, it extracts the source address and updates its entry for this address with the corresponding LAN and time stamp. \nAfter some amount of time all entries, which have not been renewed are purged to prevent outdated information from being used. \n\nKnowing via which output line to forward a frame, the bridge does not have to resort to flooding thus reducing the total network load. \n\nAn advantage of transparent bridges is that the bridge is invisible to the network which simplifies other components.",
        "answer_feedback": "The response does not mention how a packet from source S received over LAN L can be interpreted as \"destination S can be reached over L\" which forms the base for backward learning. Also, a benefit of the transparent bridges in general is stated which was not required. Apart from that, The response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ee957e507005499cb4011f1e35a70791",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The forwarding table maps stations to LANs and has a timestamp for each of its entries.\nWhen the bridge receives a frame with the source address Q on LAN L in the backward learning phase, it adds a table entry Q can be reached over L. If this entry already exists, the timestamp is updated, the timestamps are used to regularly delete old entries.\nWhen the bridge receives a frame in the forwarding process, it looks up the LAN of the destination. If the source LAN and the destination LAN differ it reroutes the frame to the destination LAN. If the destination address is not available in the forwarding table the packet is flooded.\nThe benefit of this is that the bridge is not visible to other components in the network, this simplifies the other components.",
        "answer_feedback": "The stated benefit is related to transparent bridges in general, but the question asked for the benefit of using bridge table information during forwarding, which is reducing duplicates. Additionally, there is one more condition during forwarding where the packet will be dropped when the source LAN = destination LAN. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "ed05839fdb8e42d39bf484d7b16fa92e",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table holds information about which station is attached on which LAN. It includes a timestamp, adress of station and LAN. \nWhenever the bridge receives a frame from a station, it can determine in which LAN the station is attached. From this information the bridge builds an entry for the station in the bridging table. The advantage from backwards learning is, that the bridge does not have to flood the frames, therefore decreasing the network traffic. Additionally, the bridge can drop frames, that are adressed for a receiver in the same LAN as the sender station. \nThe entries are deleted, if they haven\u2018t been updated for a set time (usually several minutes).",
        "answer_feedback": "The response answers all the four requirements of the question correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d1cba65dc11d44f99147b215f0cc2c1d",
        "question": "Transparent bridges manage a bridge table for the forwarding process. Please describe what information the table holds and how it is modified during the backwards learning phase. How is the table used in the forwarding process and what is one benefit of that? Please answer the question in 2-5 sentences. ",
        "reference_answer": "1.A mapping between destinations/stations (MACs) and outgoing LAN interfaces.\n2.This table is initially empty and received packages are flooded on every line. When a bridge receives a frame (as the bridge runs in promiscuous mode it listens in on all the traffic arriving at its LAN links) with source address Q on LAN L, it adds the timestamped entry \u201cQ can be reached over L\u201d to the table and the next time a packet with destination Q arrives, it is forwarded on link L. The time-stamp is used to update or purge old entries. Therefore, it learns by observing the incoming traffic.\n3.To do selective forwarding instead of flooding. \n4.i)less duplication/unnecessary flooding is prevented.\nii)less congestion.\niii)better bandwidth usage than flooding.",
        "provided_answer": "The bridge table of transparent bridges holds the frame with address on every connected LAN and the frame arrival timestamp, including sender and receiver.\nDuring the backwards learning phase because of learning in the promiscuous mode the bridge can know any frame from any connected LAN and can trace back to the source address on the LAN of the received frame to create a table entry in the bridge table. In the forwarding process if the source and destination LANs are identical, the frame will be dropped, otherwise(if they are different), the frame will be forwarded. If the destination LAN is unknown, the frame will be flooded. \nThe benefit is forwarding frames without considering types of LANs and without changing the configuration tables to achieve transparency.",
        "answer_feedback": "The bridge table does not contain both sender and receiver information; it only maps the destination station to the incoming LAN. The stated benefit is not correct as the response mentions the benefit of transparent bridges in general, not of using transparent bridge table information in forwarding. Apart from that, the response is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "7db19e90e7ee46eb9fdb83d2014cc8a5",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Network Topology and Access Points- Network topology and Access Points changes faster when the devices are moving around which is not the case in fixed network. Security - As the network could be available even inside and outside the building, it possesses a threat to security. User Mobility: The users can be moving from one network to another and want to communicate anytime with anyone without losing the network.  Device Portability: The device can change in a period of time but having the portability with the other device to connect anytime and anywhere to the network.",
        "answer_feedback": "The response is partially correct because the device portability is not an actual challenge beyond what is already stated in the previous challenges.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "38475cc21db4449482e03d397d3782d5",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminal: If multiple nodes are hidden from each other, the transmission to a common node of them results in a collision at the common receiver. Two nodes are hidden from each other, when they cannot sense each other (distance > detection range). Near and Far Terminals: Stronger signals drown weaker signals. That means that the distance of the nodes can influence the communication behavior because the signal strength depends on the distance to the sender.",
        "answer_feedback": "The response correctly states and describes the hidden terminal problem. The near and far terminal challenge does not specify the relation between distance and signal strength.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "37ebd6d7efea458db6b8524eb96dc62e",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "ONE CHALLENGE IS THE USER MOBILITY. IN CONTRAST TO FIXED AND WIRED NETWORKS, USERS CAN MOVE AROUND WHILE COMMUNICATING WIRELESSLY ANYTIME, ANYWHERE AND WITH ANYONE. THIS BRINGS ADDITIONAL COMPLEXITY TO ROUTING MECHANISMS SINCE THEY HAVE TO TAKE INTO ACCOUNT THAT OPTIMAL PATHS MAY CHANGE FROM TIME TO TIME AND HAVE TO BE RECALCULATED. Another challenge is the device portability which is a requirement for the ability of devices to connect anytime and anywhere to the network. Properties like power consumption or space play a more significant role when compared to fixed and wired networks.",
        "answer_feedback": "The response is partially correct because portability is not a challenge itself. However, the description mentions the power consumption of mobile devices which is an important challenge in mobile routing.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "b4ddfa2c03ee4f4eb1ad0729d9bdf65a",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Hidden Terminal Problem: Having three nodes A, B and C where A and C are in range of B, but not in range of each other, if A and C send at the same time to B, there will be interference at B, but they dont know because they are out of range of each other. Exposed Terminal Problem: The constellation of the Exposed Terminal Problem are two sender-receiver pairs where the senders are in range of each other, but not the receivers. If S1 sends data to R1, S2 will assume that the channel is occupied and cannot send data, while in theory it could because R2 is out of range of S1.",
        "answer_feedback": "The response correctly states and describes the hidden and exposed terminal problems in wireless networks.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fbd3f1061e444cafabdd1a4289c70e96",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Challenges: Hidden Terminals, Exposed Terminals, Near and Far Terminals Hidden Terminals:  Each terminal has its own maximal detection or transmission range. Nodes A and C cannot hear each other, and the transmissions of nodes A and C may collide at node B, then nodes A and C are hidden from each other. Hidden terminals can cause more collisions, waste of resources, etc. Exposed Terminals: Assumed terminal B is able to hear A and C, C can hear B and D. If B sent to A, C wants to send to D is not A or B. At this time C must wait, indicating that a medium is being used, but A is outside the radio range of C, then C is \"exposed\" to B. There is a problem of underutilization of channel.",
        "answer_feedback": "The exposed terminal description part is partially correct because the reason behind the busy medium is incorrect. C wants to send data to D but senses the medium busy and waits. The wait is unnecessary because D is outside the range of B. Therefore, C and B are exposed to each other.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "eab91bf79f2640e3ad08ed9fadbc11b0",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "There are a multitude of challenges facing mobile routing. From the speed the mobile hosts have, to security, scalability etc.  Two challenges of mobile routing are hidden terminals and exposed terminals. Hidden Terminals:  The hidden terminal problem occurs when multiple nodes can access one IS, but not each other. This leads to issues when both nodes try to send data packets to the IS in the middle simulatenously. As the nodes that are not in each others reach are \"hidden\" from each other, they cannot detect the collision at the IS in the middle.  Exposed Terminals:  The exposed terminal problem occurs when multiple nodes are in close vicinity and in their neighboring radio range. A node (lets call it B) has to wait for an adjacent one (C) to finish with their data transmission (to a node D) before starting their own, even if their destination node (lets call it A) would be a node that is not in the radio range of C. The sender node that has to wait (B) is \"exposed\" to the traffic from the IS next to it (C). This leads to underutilization of the channel and low efficiency.",
        "answer_feedback": "The response correctly states and describes the hidden and exposed terminal problems in wireless networks.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c6b126b160dd48c88c07c97e3860b222",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "In mobile communication, nodes can move, i.e. are not placed on a fixed location, and are transmitting over the air, i.e. they use wireless communication. This stands in big contrast to fixed and wired networks.\n\nTo adapt to these new factors, challenges on many different layers occur. Whether the Discovery of Service in the Application Layer, the specific needs for routing in the Network Layer, or the control of power used for transmission in the Physical Layer and even more have to be taken into account to set up such a mobile network.\n\n             -  For mobile routing, the following challenges exist:\n\n                         o   Hosts can move and change the attachment to a certain network segment (handover). This handover mechanisms occur in cellular networks and in complex WLAN infrastructures as well. For the  example of WLAN, handover can be handled at the link layer if both ace points are in the same networks. If there are different networks, a network layer handoff occurs and needs to be considered to implement routing for a seamless service.\n\n                         o   In mobile ad-hoc networks, nodes can disappear due to the radio system and channel characteristics. Therefore the network topology may change rapidly and pre-calculated, pro-active routing may fail. In this case, reactive routing maybe of preference, as the route is calculated using the actual network topology.\n\n             - Moreover, the general challenges in mobile communication include:\n\n                        o   Hidden terminals\n\n                                \u00a7  Since in mobile communication no wire is used, the nodes can only listen for a certain distance defined by the radio parameters and the channel characteristics, which makes the usage of CSMA/CD not practicable as the concept of Carrier Detection (CD) does not work.\n\n                                    An already happening transmission to a certain destination node could not be detected by an intermediate system (IS) if the node communicating with the destination node is out of the \u201clistening\u201d-range of the IS. To solve this problem the collision will not be \u201cdetected\u201d but \u201cavoided\u201d using certain ACKs (CSMA/CA, RTS/CTS).\n\n                       o   Exposed terminals\n\n                                 \u00a7  When a node wants to send a packet and detects an already existing transmission in its transmission area, it waits until the existing transmission is finished and only thereafter sends its packet to avoid collisions.\n                                     This can cause underutilization of a channel if the receiving nodes are far enough away from each other, i.e. beyond the radio range, and therefore both transmissions would not affect each other and could have successfully be completed in parallel.\n\n                       o  Near and Far terminals\n\n                                  \u00a7  If multiple packets arrive at the receiver, they will typically arrive with different signal strengths. Depending on the modulation used, the receiver will detect the \u201cstronger\u201d packet and ignore the \u201cweaker\u201d packets if the power difference is large enough. However, fading and multipath may lead to changing power levels and add complexity to the problem.\n\n                                  \u00a7  Using a multipath receiver design, both packets can be detected as different paths and the receiver may fail to detect these packets. The near/far problem is difficult to solve in direct sequence spread spectrum systems (rake receivers). In OFDM based system, this problem is less severe.",
        "answer_feedback": "The response correctly states the challenges faced in the wireless network.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b2ee36747d6c48d3985a600ef492f1f5",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "The challenges are: * Security:the packet of data is not much secure.Through neighbor authentication, a user can know it neighboring users.For security of packet we must have to use technique of data encryption. * QoS * Scalability * Heterogeneity * Adaptation:Network has to adapt to Dynamic positioning of nodes.This is necessary and nodes may join the network or may leave the network dynamically. * Dependability\t-",
        "answer_feedback": "The response names 5 requirements but describes only two. Further, the description of security is partially correct as it does not clarify how security is a challenge in a wireless network as encryption is common in both wired and wireless communication.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.62
    },
    {
        "id": "ef57f8cf69f043908406328de833e8a0",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. Hidden Terminals  Exposed Terminals\uff0c Near and Far Terminals 2.Hidden Terminals\uff1a There are maybe 3 nodes. Because of the maximal range. Nodes A and C cannot  hear each other. They can only hear B. The transmissions of A and C may collide at B. So they are hidden from each other. This is a waste of resources and it may lead more collisions.\n\t Exposed Terminals:  There are 4 nodes. B can hear A and C. C can hear B and D. When B sends to  A and C sends to D, C must wait because there is signal that a medium is being used. But because A is out of the range of C . So this wait is unnecessary. C  is exposed to B . This leads to the problem of underutilization of channel and lower effective throughput.",
        "answer_feedback": "C does not need a signal that the medium is busy because it can use its carrier sense to detect B's transmission.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "a6513af0684f4b2a81a2c33b86215b00",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "One challenge is the hidden terminal problem. CSMA doesn't work in mobile networks because a node cant see the whole transmission medium. They can't detect if there is some other node sending,to the same destination, at the moment so the send to and cause a collision. An other challenge is the exposed terminal problem. It can happen that a node tells all other nodes in range not to send, to avoid collisions at their receiver. But some nodes then need to be silent even though they aren't even in range with the receiver of the other transmission and therefore can't create a collision. So the wait unnecessary and Utilization is lower than it could be.",
        "answer_feedback": "The description of the exposed terminal problem is partially correct. It states that a node tells all other nodes not to send. But instead, the node wanting to send senses the medium is busy and waits until it is free again.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "d63c271c0a144dd683e2a77299982ada",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "When we have mobile routing some problems begin to appear. One of them is called \u201chidden terminals\u201d and this is caused because the nodes are not within each others transmission rate, causing that they are invisible to each other and they have to communicate through a third node ,which is within range of BOTH of this nodes, the issue with this case is that because they can\u00b4t know at first if the other node is sending something, collisions may occur. Another challenge is the near and far terminals , in which if two nodes are sending signals at the same time, the stronger signal will drown out the weaker signal, making the receiver not being able to receiver the weaker signal.",
        "answer_feedback": "The response states the hidden terminal challenge correctly except that nodes communicate 'to' the common node, not 'through' the common node. The near and far terminal challenge description is incomplete because it does not mention the signal's relation with increasing distance.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "50748f8bbd0c49d7bb01706c25e9810b",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. Hidden terminals: For given nodes A, B and C, nodes A and C cannot hear each other if their transmissions collide at node B. In this way, nodes A and C remain hidden from each other. 2. Exposed terminals: For given nodes A, B, C and D, B sends to A and C wants to send to another terminal like D, but not A or B. C has to wait and signals a medium in use. But A is outside the radio range of C, therefore waiting is not necessary. In this way, C is now \"exposed\" to B.",
        "answer_feedback": "The response correctly states and describes the exposed terminal problem. But in the hidden terminal problem, not only the collision but also the transmission of the other sender remains undetected.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.88
    },
    {
        "id": "ecd34f9671014025964c4f46ac582f65",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "1. Hidden Terminals: for example there are 3 nodes A, B, C. A cannot hear C and vice versa, and B is located between A and C. B can hear A and C. When A sends to B, C still thinks that B is free, thus start sending to B as well. This results in a collision in B. 2. Near and Far Terminals: assume there are 3 nodes A, B, C. B and C are located near to each other, while A is located farther from them. When both A and B send to C, B's stronger signal drowns A's weaker signal, thus making C cannot hear/receive A.",
        "answer_feedback": "The response correctly states two challenges of mobile routing.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3cc97149924e40a8b122b8a6f346142c",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "One challenge are \"hidden terminals\". In this case, there are 3 nodes A, B and C and A and C cannot hear each other but A and B and B and C can. So when A sends to B due to the fact that C can't receive A C senses B as \"free\" medium and also sends to B. Then at B a collision occurs which A can't detect. A is therefore \"hidden\" for C and C for A. Another challenge are \"exposed terminals\". There we have 4 nodes A, B, C and D. A can reach B, B can reach A and C, C can reach B and D and D can reach C. Now B sends to A and C wants to send to D. But because C signals a medium in use it has to wait, even if the medium in use is  B sending to A and A is outside the radio range of C and the waiting is not necessary. So C is \"exposed\" to B. The third challenge are \"near and far terminals\" which is about 3 terminals A, B and C where A and B are sending and C is receiving. Due to the decrease of the signal strength proportionally to the square of distance the stronger signal (e.g. B's) drowns out the weaker signal (e.g. A's). The result is that C cannot receive A.",
        "answer_feedback": "In the exposed terminal description, C senses the busy channel rather than signaling it. Apart from that, the answer is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "12e6aeea55ad4924869e5b5f2bc48f30",
        "question": "WHAT are the challenges of Mobile Routing compared to routing in fixed and wired networks? Please NAME and DESCRIBE two challenges.",
        "reference_answer": "Possible Challenges:\n1.Adaptation: The network has to handle the dynamic positioning of the nodes/topology changes. Additionally, nodes can leave or join the network anywhere (within signal range) at any time.\n2.Security: Interception of packets or injection of faulty packages is easily possible in wireless networks. This may necessitate encryption and authentication.\n3.Medium Access Control: Wireless networks feature new possibilities for inference and collisions of the transmitted signals. See the following correct example challenges:\ni)Hidden Terminal: Nodes\u2019 transmissions may collide at other nodes without them noticing, because they are out of detection range of each other (A and C in this example) and therefore sense the medium to be free even though there is an overlap of the transmission ranges. \nii)Exposed Terminal: Nodes (C in this example) may not realize they could send collision-free, because a node in their detection range (B) is sending and they, therefore, detect the medium as busy, but the sending node (B) is out of the detection range of the destination node (D) so that no collision would occur.\niii)Near and Far Terminals: Signal strength decreases proportionally to the square of distance, so closer nodes may drown out the weaker signals of nodes farther away.\n4.Quality of Service (QoS): Due to the rapidly changing network topology, imprecise network information, and resource constraints of participating nodes, it is challenging to provide the desired QoS. Additionally, signal quality may also decline due to noise and occlusion.\n5.Scalability: Since it is not possible to know the number of participating nodes beforehand, it is vital that routing protocols, etc. are capable of dealing with increasing network sizes and workloads. \n6.Heterogeneity: Nodes may have different capabilities, responsibilities, and constraints, e.g. processing capacity, transmission ranges\n7.Dependability: Providing consistent, performant, and reliable routing behavior that higher-level services can trust is challenging in such a dynamic environment. \n8.Power Consumption: As most mobile devices are battery-powered, power consumption becomes an important optimization factor in routing, etc.",
        "provided_answer": "Energy Efficiency: Not having infinite power leads to the problem of having to safe power as to not empty the mobile instantly.\nAdaption of routing protocols: in a constantly changing network you have to use other routing algorithms, than in a normal network",
        "answer_feedback": "The response states and describes both the challenges correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "4384e60a7ada4d80984c77b3b3bf6dc0",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Revers Path Broadcast are used to forward multicast packages without using loops (and therefore not creating duplicates on the way). In Reverse Path Forwarding every node has its own spanning tree. When a package is received by an intermediate station, the station checks if it would send packages to the sender over the used link. If that is the case, then it should be the best route and the intermediate station forwars the package to all connected nodes (except the incoming edge). If it is not the case, then it gets discarded, because it is very likely a duplicate, from another node. Reverse Path Broadcast is an improvement on reverse path forwarding. Instead of forwarding packages to every node (if conditions for forwarding are met) it only forwars it to the node, from where packages would normally arrive from.",
        "answer_feedback": "The purpose of Reverse Path Forwarding and Reverse path Broadcast is not limited to the multicast but also used in broadcast. In Reverse Path Forwarding, only the sender needs to know the spanning tree, and it makes use of unicast information to forward the broadcast package. The explanation of RPB is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "26a002ad89bc42d2abcf9bcb604424f5",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Purpose of RPF and RPB: Avoid receiving duplicate packets * RPF: Use unicast routing information to decide if an incoming packet is dropped or sent further via the outgoing links: * send further if incoming packet used path in unicast routing info * drop if incoming packet did not use path in unicast routing info * RPB: Select outgoing edges: * Select edge if a packet coming from the connected node to the sender would use this edge. * Sent incoming packet via selected edges (not including the incoming edge).",
        "answer_feedback": "The response correctly answers the purpose and the explanation for both broadcast types. The purpose is to avoid forwarding duplicate packets not receiving but it is acceptable.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f898735b8bf444c3b10f97085cdd2c7e",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse path forwarding (RPF) and reverse path broadcast (RPB) are techniques used in routers. They enable loop-free forwarding of multicast packets in multicast routing. REVERSE PATH FORWARDING:  In this procedure, each node i checks whether an incoming packet from source q has been received on the connection on which node i also transmits its packets to q. If this is the case, then the packet is assumed to have been transmitted on the shortest route and it is forwarded on all other lines. If, on the other hand, a packet was received on a line other than the one on which data is transmitted to the sender, then it is assumed that it is a duplicate that did not take the shortest route. This duplicate is then discarded instead of being forwarded. REVERSE PATH BROADCAST:  RPB is an improvement on RPF. RPB not only evaluates the shortest path with respect to the interface where the multicast packets are received, but also influences the forwarding of data to the interface of the router. As a result, multicast packets are only forwarded to the interfaces where the next router is located in the reverse direction on the shortest path to the data source. RPB specifically checks whether the incoming packet arrives at the IS through which the packets for this station/source are normally also sent.  If not, the packet is discarded directly. If yes, it will be further checked if the packet has taken the best path so far. If yes \u2192 select the edge at which the packets arrived and from which they are then rerouted to source S (in reversed direction) If not \u2192 do not send over all edges (without the incoming one)",
        "answer_feedback": "The purpose of Reverse Path Forwarding and Reverse path Broadcast is not limited to the multicast but also used in broadcast. The explanation of RPF and RPB is correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d641379a69a34431b09b2b668efc9219",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Reverse Path Broadcast are both broadcast algorithms that attempt to minimize the number of duplicates packets in the network compared to other algorithms like the simple Flooding algorithm. The idea of Reverse Path Forwarding is that each sender/node has an own spanning tree, but the other ISs do not need to know them. The algorithm is based on a simple condition for each node. The node has to decide, whether the received packet sent over the best route, i.e. it used the edge the node would use to send it back to the sender/author or not. If this is true, the node resends this packet over all other edges, i.e. excluding the incoming one, otherwise, the packet is most likely a duplicate and it will be discarded. The Reverse Path Broadcast goes a little further and the IS sends the packet from the best route only to the nodes it is responsible for/the sender used before. If the IS is on the best path between the sender and his neighbor node, i.e. it is responsible for it, it learns over time.",
        "answer_feedback": "The response correctly explains RPF and RPB and their purpose.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b474fb4274e241558ba8fb4ace40443e",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose is to reduce packet duplicates due to flooding. * REVERSE PATH FORWARDING: The receiving router checks, if the incoming packet used the usual (proboably best) path to it. If it is the case, the packet will be forwarded to all links except the one at which the packet arrived. If a packet arrives at an unusual path (probably not the best path and a duplicate), the incoming packet will be ignored (discarded) and not forwarded any farther. * REVERSE PATH BROADCAST: Similiar to _Reverse Path Forwarding except _that a node gains information about the best paths between neighbouring nodes due to observation. That is, a packet will not be forwarded via every link but only via those links, which are unlikely to create more duplicates. For example a node A will not forward a packet to X coming from a node Z, if it knows (due to obeservation), that there exists a best path between X and Z for that packet.",
        "answer_feedback": "The response identifies the purpose of RPF and RPB correctly. The RPF explanation is partially complete because it is unclear what the \"usual path\" means and how it is determined. If a node X receives a broadcast packet from Source S, node X checks its routing table to see if it would have used the same route to send a unicast packet to S, if yes the incoming packet followed the best route. The RPB explanation is also not complete as it also does not explain how nodes learn the best path between nodes, namely through the unicast routing algorithm (e.g. link state) or observing previous unicast traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "id": "b7431ec697184fdcb76ed7dfee7e1b48",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "The purpose of Reverse Path Forwarding and Reverse Path Broadcast is to send (loop-free) multicast packets in multicast routing. Reverse Path Forwarding is more reliable (it can compensate if one router of the network has an error). Reverse Path Broadcast is more efficient, it relieves links, which are not the best path and therefore not necessary. Reverse Path Forwarding: If the router gets a packet, it looks where the packet comes from. If it is from a link, which its (unicast) routing table would also suggest to send via this link, this link is the best path and the router distributes the packet. Otherwise the packet is discarded. Reverse Path Broadcast: If the router B gets a packet, it also looks in the routing table, if the packet comes from the best path. If it is the best path (from router A), the router looks in the routing table whether a packet is ever send from C to A. If this is not the case, the link to C is not the best path and B doesn\u2019t send it to C. So not necessary links are relieved.",
        "answer_feedback": "The response is partially correct because both RPF and RPB explanations didn't clearly explain how the packets are forwarded in a network. Additionally, the IS would also look whether packets are sent from A to C in Reverse Path Broadcast. The algorithms avoid loops not only in multicast but also in broadcast.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.6
    },
    {
        "id": "02ca776df06148dc99dbf1b84b0ddc76",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding and Reverse Path Broadcast are used for Broadcast Routing (a Sender send a message to all Recievers(1:all communication)). The sender has its own Spanning tree to calculate the routes. The receivers however do not. So they have to deciede how to handle the received packets. So they check if the received packet was received through the edge, which is usually used by packets from the sender. If this is not the case, the packet will be discarded. However if it is the case, in Reverse Path Forwarding, the packet will be resend over all edges.  With Broadcast Routing on the other hand the receiver checks if the received packet used the best route until this point and only forwards it over those edges which belong to the best routes,",
        "answer_feedback": "The response correctly explains RPF and RPB. However, the response lacks the purpose which is to minimize the number of duplicate packets during broadcasting. In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.7
    },
    {
        "id": "db6ff07f10c14ba293ee0c290202767a",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Using Reverse Path forwarding ensures loop-free forwarding of multicast packets. The idea behind this algorithm is: If a packet station X arrives at an IS over an entry point over which the packets for station X are usually sent, this might probably be the correct and fastest route. Therefore only if this is the case packets distributed over all edges. If the packet is received over another entry point it will be discarded. Reverse Path Broadcast is a refined version of this algorithm. It differs from Reverse Path Forwarding by the fact, that if the packets have taken the best route until their arrival at a certain node they will be forwarded to the best next edge taken from the routing table. If not, they are NOT sent over all edges. This is achieved by the knowledge, which other nodes usually receive their unicast packets via this note.",
        "answer_feedback": "The answer is partially correct as the purpose of RPB is not explicitly mentioned. In RPF, the optimal/best route may or may not be the \"fastest\".  In both algorithms, the packet is also not forwarded to the edge from which it was received.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "c865fb83194f4482a2a382e968409992",
        "question": "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?",
        "reference_answer": "Purpose: Both implement a more efficient kind (in terms of duplicate packets) of broadcasting than flooding / to minimize the number of duplicate packets in the network when broadcasting.\n\nReverse Path Forwarding:  Each node has a routing table stemming from unicast routing algorithms. When receiving a packet of sender S from neighbor N at node X, check the routing table and only forward the packet to all adjacent nodes (except for N) when you would send packets to S over N \u2192 the assumption is that the packet used the BEST route until now.\nReverse Path Broadcast: Nodes monitor unicast traffic to learn which paths in the network they are a part of, that is node X knows that is on the best path between two nodes A and B if it receives unicast packets coming from A and going to B or visa versa. In other words, each node knows which spanning trees it is part of in the network. Reverse Path Broadcasting works identical to Reverse Path Forwarding with the exception that it only forwards packets on edges that are part of a spanning tree (again excluding the incoming line to N).",
        "provided_answer": "Reverse Path Forwarding :  purpose:Variation of the Spanning Tree,Each sender has its own Spanning Tree,but IS do not need to know the Spanning Trees Algorithm:Has this packet arrived at the IS entry port over which the packets for this station/source are usually also sent? Yes: -Assumption: Packet used the BEST route until now   Action: resend over all edges (not including the incoming one) No: -Assumption: Packet did NOT use this route (it is NOT the best route)     Action: discard packet (most likely duplicate)   Reverse Path Broadcast: purpose:Has packet arrived at the IS entry port over which the packets for this station/source are usually also sent? Yes: -Packet used the BEST route until now -resend over ALL edges(not including the incoming one) No: Discard Packet did NOT use this route (it is NOT the best route)   Algorithm: packet from S(ource) to D(estination) Like REVERSE PATH FORWARDING with specific selection of the outgoing links Has this packet arrived at THE IS entry over which the packets for this station/source S are usually also sent? YES: Packet used the BEST route until now?      YES: select the edge at which the packets arrived and from which they                 are then rerouted to source S (in reversed direction)       NO:  DO NOT send over all edges (without the incoming one), i.e., not as              in Reverse Path Forwarding (RPF) NO: discard packet",
        "answer_feedback": "The response correctly explains  RPF and RPB but the stated purpose is incorrect. It should be to reduce the number of duplicates and unnecessary packets in flooding/broadcasting by inspecting the optimal unicast paths.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.8
    },
    {
        "id": "8d2b4b2a7f77444aa3cadcc0c8ab192e",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "All intermediate stations periodically have to send link state packets via broadcast to all others. The link state packets contain the distance to the neighbors and additional information about the multicast group.  Each IS then calculates a multicast tree on the locally available data and determines the lines on which packets must be sent.",
        "answer_feedback": "The response does not mention the spanning-tree property that makes it appealing for broadcast and multicast. The explanation for modifying the link-state algorithm to construct a  multicast spanning tree for nodes is correct.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "ba4e8c0cd5cb4814a70128904982c783",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees are appealing to broad- and multicasting scenarios, because they allow the packets to only travel one path (except travling backwards). This removes the need for looking up specific tables as in RPF / RPB.",
        "answer_feedback": "It is true that there is a unique path between nodes but that not only does away with the need to look at routing tables in RPF/RPB but reduces duplicates by removing loops(unnecessary links). No explanation was provided for modifying the link-state algorithm to construct a  multicast spanning tree for nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "67770c3cb512453f901bffe3df325a04",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "-The property of spanning trees, that it is a subset of subnets including all routers with no loops, makes them appealing for broad- and multicasting.   -If link state routing is used and each IS/router knows the complete topology, including which hosts belong to which groups,then the spanning tree can be pruned  from bottom of each path to root, all routers are removed that do not belong to the group.",
        "answer_feedback": "The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast. The second part does not answer how the link-state is modified. It assumes hosts have already discovered which nodes belong to which group, which is not correct. In the link-state additional multicast-group information is added and send to all other nodes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "38d1152676204dbe8730599f925b8db5",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees includes all routers with no circles, so the packets are not sended (infinitely) often around.\n\nIf the frequently sended packets of the Link State Routing also contains informations on multicast groups, every IS has enough information to construct a spanning tree for multicasting.",
        "answer_feedback": "The explanation behind using a spanning tree for multicast and broadcast is partially correct because though the network is loop-free, using the tree results in the minimum number of message copies required to be forwarded and not just the prevention of forwarding loops.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "302939ab3d2c490ea65ebe22641ee8e6",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "A spanning tree has no loops, includes all routers (of the subnet) and has a root IS (intermediate system). This is appealing for broad- and multicasting, because you only need to send the data to the root IS. From there every node (or a specific set of nodes) can be reached. Link State Routing constructs a spanning tree. For multicast routing, the information, which systems belong to one group, must be provided. Therefore the link state packets are expanded to contain the information on multicast groups. These are then propagated from a predefined root IS to calculate the tree. A spanning tree has no loops. Link State Routing constructs a spanning tree, it needs to know which systems belong to a group.Therefore the link state packets are expanded to contain the information on multicast groups. These are then propagated from a predefined root IS to calculate the tree.",
        "answer_feedback": "The stated property may be correct for specific types of spanning trees but is not the general property of a spanning tree. Not all spanning trees need to have a root node. The reason why a spanning tree is used in multicast and broadcast is the lack of loops, which reduces the number of duplicates needed. The modification description of the link-state algorithm is correct, except that it does not need to be propagated from the root node.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "55cee294fa754d5ba8a2deb3b02b8ec0",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "The property is, if each router knows which of its lines belong to the spanning tree, it can copy an incoming broadcast packet onto all the spanning tree lines except the one it arrived on. There is no loop in a tree. Therefore in order to build a spanning tree by modifying Link State Routing, the loops need to be cut. Assuming that a router is a vertice and when two routers are connected, there is an edge between them. After five steps of LSR, it can be abstracted as a weighted directed graph. Below is the basic idea. Divide the vertices in the graph into two groups, S and U. S contains vertices that has already computed shortest path. U contains vertices that the shortest path is uncertain. Add following steps after regular LSR. a. Originally, S only contains source vertice v, U contains the rest of them. b. Pick up vertice k from U, which has shortest distance from v, put k into S. c. Let k be the new intermediate vertice, changing the distances from k to the rest vertices in U. d. Repeat step a and b until all vertices are in group S",
        "answer_feedback": "The response is not correct about how the link-state algorithm is modified. The link-state packet is expanded to contain multicast group information and exchange it with other nodes to calculate their multicast spanning tree. Dividing the graph into two parts and calculating the shortest distance does not help in sharing the needed multicast group information of each node.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "7928e70858a448d0904ef1e7a59f243d",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Properties of Spanning Tree for broad- and multicast:  * The spanning tree does not have any cycle  * A connected graph can have more than one spanning tree  * All possible spanning trees of the graph have the same number of edges and vertices  * The spanning tree is minimally connected, means it generates less complexity Modifying Link State Routing to construct a Spanning Tree:  * All IS sent link state packets periodically containing information on distance to neighbours, information on multicast groups, which will be broadcasted to all others  * Each IS calculating a multicast tree from the now locally available and complete state information  * Based on the information about the multicast tree IS determines the outgoing lines and on which packets have to be transmitted",
        "answer_feedback": "The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast and how the link-state algorithm is modified to construct a  multicast spanning tree.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bc0c10f769c04095a0ea8c2e366970fd",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees allows to reach all other nodes of a network with a small subset of links. A spanning tree therefore \"provides\" information for broad- and multicasting to send the packets through the \"best\" route (e.g. minimum hops).",
        "answer_feedback": "The response incorrectly describes the reason why a spanning tree is attractive for broadcast and multicast. Although the number of links is reduced, they also need to connect without loops, reducing duplicates. The modification related to the link state algorithm to construct a  multicast spanning tree is not provided.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "fb658274b57847bd8a5c2a6cc1f5113a",
        "question": "WHICH PROPERTY of spanning trees makes them appealing for broad- and multicasting? EXPLAIN how you can modify Link State Routing to construct a spanning tree for multicasting.",
        "reference_answer": "Property: There is a single unique path between every pair of nodes in the tree. Alternatively, you can say that spanning trees are subnets of a network that do not contain loops but contain all nodes. This means that no unnecessary duplicates are distributed in the network when forwarding packets using that tree structure.\n\nSpanning Tree with Link State Routing: Each intermediate system knows which multicast groups it belongs to, but initially doesn\u2019t know which other IS belong to the groups. Therefore, you can add multicast group information to the link state packet and each node can construct multicast trees once the full network topology and group information are distributed in the whole network, as each node then has the complete state information stored locally (e.g. with Prim or Kruskal)",
        "provided_answer": "Spanning trees have all nodes covered with the minimum number of edges, so there can't be any loops. That makes them appealing for Broad- and Multicasting. If we modify Link State Routing so that each IS calculates a multicast tree. All IS send link state packets periodically and from the now locally available and complete state information each IS calculates a multicast tree.",
        "answer_feedback": "The response correctly answers why a spanning-tree usage is ideal in multicast and broadcast. The provided information for modifying link state to construct a multicast spanning group is not complete as it does not state what additional information is added in each link-state packet apart from the regular information.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "c8679d86ae694bdb87d20888a5036587",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers describes a form of additional information, which can be added to the packet. If and how many extension headers are added is optional and completely up to the higher layers to decide. In the packet they are located between the header and the data (unit). The main advantage is that you are not forced to put an option-field in the header, like in IPv4, anymore. So that space is not wasted, if the option-field is not used.",
        "answer_feedback": "The response answers the description correctly. The location of the extension headers is not precise. They are located between the fixed header and payload. The stated main advantage is incorrect as the option field in the IPv4 header is already optional, so there is no added advantage.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "54124f3eb2584e1e9c35fc2669c4cbcd",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension headers in IPv6 are additional data contains in an IPv6 packet, located between the fixed header and the payload. \nThe main advantage compared to IPv4 is that it allows to add new options without changing the fixed header.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fbac64191c55418ebba611378b07d145",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional headers in IPv6 that are not limited by any size limitation as they are located in between the IP header and the data payload and therefore don't change the IP header. One header links thereby to the next header with the first header being pointed to by the IPv6 \"Next Header\" field. Because of that flexibility, new headers may be invented as the need arises.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0c28b4c85c564dcbbf05a8b4c386824a",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "- In order to keep the core packet structure simple, IPv6 removed some unnecessary header fields and provide instead a extension headers, which can be used only when really need it. \n\n-Extension headers are located between the main header and the payload.\n\n- Advantage: It will help to extend the protocol in the future without affecting other\ncore fields in the packet.( flexibility)",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "360b34ceb29a49d38c6a47059f6725c8",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers in IPv6 are a way of enlarge the header in order to put additional information. They are placed between the fixed header and the payload. The main advantage of extension layer compared to IPv4 is that they are optional while in IPv4 the options field is required. Therefore in IPv6 you can add bigger variable length optional information without changing the fixed header. So if you want to change IPv6 you are able to put in information in these extensions.",
        "answer_feedback": "The advantage given in the response is partially correct because the option field in IPv4 is optional as well. It had a variable length of 0-40 bytes.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "49ab7c32fb974fd0820114d47f0975d5",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional and located between the header and the payload. \nThey can extend the IPv6 datagram by telling a router that the payload is used for identification, is fragmented or give additional information to hops.\nThe main advantages are more flexibility of messages and a reduced headersize, because they are optional.",
        "answer_feedback": "The response is partially correct because the advantages are incorrect. The header size is not reduced based on the optionality, as the options field in IPv4 could already be 0 bits long. Additionally, the response does not explain what type of flexibility is gained.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "bf916abced404fc2829eec86bd2770d3",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension headers are placed between fixed header and payload. Extension headers have advantages compared to IPv4 because they are optional, help to overcome size limitation, and allow to append new options without changing the fixed header.",
        "answer_feedback": "The response answers all three parts of the question. There is no specific definition stated in the response but it is present within the advantage part.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d5300a0f61a84dc790d78e03af2554ac",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension Headers can hold additional options that are not possible in the simplified and fixed IPv6 header, replacing the OPTIONS from IPv4. They are optionally placed between the fixed header and the payload. They help overcome the size limitation and allow for more options without having to change the fixed header (like we have to in IPv4).",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b116f8165b164ed29313e7a277655d89",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are headers that can provice additional information for a packet. They are located between the fixed header field and the payload.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. The advantage is missing in the response.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "ee95b7fb3e0b46bfa363a62e8048489f",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are the way to put additional information in the packet and are placed between fixed header and payload. The main advantage compared to IPv4 is that they are optional and extensible, so they don't consume additional space and can be modified easily later on (should the specification change).",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional, there is no added advantage over the IPv4 option field in terms of space consumption.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "9e4b53b65249452087267d84fd06da92",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers can provide information in addition to the IPv6 header. They are placed between the fixed header and the payload. They are optional, they only have to be transferred if they are actually used. The fixed header remains small.",
        "answer_feedback": "The response answers the description and location of extension headers correctly. As even the option field in the IPv4 header is optional and can be 0 bits long, there is no added advantage over the IPv4 option field in terms of unnecessary transfers. The main header remaining smaller is not an advantage in itself.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "8cbec38131b7417986e73f74dc80cadc",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "They are placed between the fixed header and the payload and are used to add additional non-necessary information to the IP package. The main advantage is, that they are allowing to append new options and the fixed header stays the same.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "52342d6a955249d0bf12402b8146fd56",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Each IPv6 data packet consists of a header and the payload in which the user data is located. Compared to IPv4, the Internet Protocol v6 is characterized by a significantly simplified packet format. In order to facilitate the processing of IPv6 packets, a standard length of 40 bytes was specified for the header. Optional information that is only required for special cases is outsourced to so-called extension headers, which are embedded between the header data area and the payload. Therefore the main advantage is that options can be inserted without having to change the header. The IPv6 packet header now comprises only eight header fields with IPv4 thirteen fields were used.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c2e95e496d0e47b58e782471b224e08b",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension Headers are optional additions to the fixed header. They are located between the fixed header and the payload.\nAs a main advantage compared to IPv4 the optional headers allow to append new options without needing to change anything in the fixed header.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fc03bb8b84fd478680726a5b3ccd0c80",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers are optional fields in IPv6 address, placed between the header and the playload. The main advantage compared to IPv4 is that extension headers allow for extra information to be headed, overcaming the address size limitation",
        "answer_feedback": "The response is partially correct because the advantage part is somewhat ambiguous. Extension headers help to overcome the fixed header size instead of the address size of the IPv4 packet.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "8aa679351fd54714a1197455b41fdf7c",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "The extension headers are located between the regular Header and the payload, the extension headers are optional and are only used of needed.\nThe main advantage of having extension headers compared to IPv4 is, that they are optional and can reduce traffic if not needed and that they allow to append new options without having to change a fixed header.",
        "answer_feedback": "The response is partially correct because the advantage part is somewhat ambiguous. Extension headers are optional but they are not helpful for reducing traffic.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "6f98da583f4a4f418c9978e434113fcd",
        "question": "What are extension headers in IPv6 and where are they located in a packet? What is the main advantage of extension headers compared to IPv4?",
        "reference_answer": "Extension headers are used to extend the fixed IPv6 header with additional, optional network layer information. If present, they are located between the fixed header/main header and payload/upper-layer header/ transport-layer header.\n\nMain advantage: One of the following advantages are considered fully correct:\n1. It allows the appending of new options without changing the header.\n2. IPv6 packets with optional headers are typically processed faster/simpler by intermediate devices as most of the options are ignored (except \u201cHop-by-Hop Extension\u201d) while they are processed by all routers in IPv4 unless ordered otherwise.",
        "provided_answer": "Extension headers in IPv6 contain supplementary (additional) information used by network devices to decide how to direct or process an IPv6 packet and they are located between fixed header and payload. They allow to append new options without changing the fixed header.",
        "answer_feedback": "The response answers the description, location of extension headers, and advantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7ae93aeb10ea4d069bfbe498cf4ab388",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1.Support larger address space. \n\nThe number of user has increased dramatically. In order to support more addresses,  we need IPv6 with longer address (128 bits) since we are out of addresses with IPv4 (only 32 bits). \n\n2. IPv6 can perform multicasting(and anycast). \n\nIn other word, it can send to multiple persons with multiple addresses, which ipv4 can not deliver (ipv4 can only have 1 destination address for unicast).\n\n3.  It helps providing flexibility.\n\nIt provides a extension header field, which can be used for appending new field(if need it in the future) without affecting the other fixed headers. \n\n4. IPv6 can simplify the protocol processing.\n\nThe header fields in IPv6 is simpler than IPv4. Some header fields, which were rarely used in IPV4, have been removed. Therefore IPv6 can simplify the protocol processing.",
        "answer_feedback": "All the IPv6 objectives mentioned in the response are completely correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "98347b34a8384006a10407db071c6870",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "-to support more end-systems than IPv4\n-to reduce routing tables and simplify protocol processing\n-to increase security\n-to support real time data traffic",
        "answer_feedback": "The response is correct because all stated objectives of IPv 6 are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f5dfc163fb6a4f828155f069f0e04018",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "IPv6 is made for supporting more addresses allowing billions of end-systems. It also gives the possibility to increase security and to simplify protocol processing. Additional IPv6 provides multicasting beneath a few other objectives.",
        "answer_feedback": "The response contains four correct objectives of IPv6.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6434f998b0a2414f9d3ed8f7288819d1",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Four objectives of IPv6 are:\nLonger addresses to be able support more end-systems\nTo be more adaptable than IPv4 in the future by providing extension headers\nMake the header simpler to allow for faster processing in routers\nIncrease the security by including IPSec as a mandatory feature",
        "answer_feedback": "The response is correct because all stated objectives of IPv 6 are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "477c49d65416411fa9125650b68a3e21",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "more addresses\nrefactor of header, enables future modifications\nIPv4 allow broadcast and unicast, IPv6 supports anycast (e.g. nearest node)\nno checksum\nno fragmenting\nincrease security\nsimplify protocol processing",
        "answer_feedback": "The response correctly answers the four objectives of IPv6.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8dc6ce06de3e42dda595b7427c06ca2b",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "- support more end-systems by using longer addresses\n- reduce the size of the routing tables\n- simplify the protocol, to allow routers to process packets faster.\n- integrate security\n- provide multicasting\n- support real time data traffic",
        "answer_feedback": "All six IPv6 objectives are completely correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e1cba3c3e874470386a0190e5c5c7e7c",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Objectives of ipv6 are: 1.To support billions of end-systems-it has Longer addresses 2.To reduce routing tables 3.To simplify protocol processing by Simplified header 4.To increase security (integrated) 5.To support real time data traffic (quality of service) by Flow label, traffic class,etc",
        "answer_feedback": "All five IPv6 objectives in the response are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "aca20a0b5f144ca49b18706bdb4e029a",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "To support billions of addresses, since it has much longer addresses than IPv4\nTo be open to change, it does so by the support of extension headers.\nTo provide better multicasting features than IPv4\nTo increase security.",
        "answer_feedback": "The response answers all four objectives of IPv6 correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c31d7c06926745a9903e6fdf4a721d77",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1. Enlarge the available address pool:\n    By increasing the IP address length from 32 bits to 128 bits, a greater number of addresses can be assigned to end systems.\n2. Simplify protocol processing:\n    Any previous shortcomings in IPv4 can be removed and optimized in IPv6.\n3. Provide Multicasting:\n    Packets can now be sent to multiple destination addresses, which makes multicasting possible.\n4. Better Security:\n    Security means are already integrated in IPv6.",
        "answer_feedback": "The response correctly states four objectives of IPv6 with explanations.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7a846709b3b24276bc2135926eac0aa6",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "- Bigger adress room\n- Better security \n- Reduce routing tables\n- Simplify headers",
        "answer_feedback": "The response is correct because all stated objectives of IPv 6 are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "be063b4ed6244dd09eab20108556a12d",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1.to support billions of End-Systems.\n2.to reduce Routing tables.\n3.to simplify protocol processing.\n4.to increase security.\n5.to support real time data traffic.",
        "answer_feedback": "All five IPv6 objectives mentioned in the response are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6985cfa5268541a58f069e90cf8001ed",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "- Support larger number of end systems than IPv4 by using longer addresses\n- Reduce size of routing tables\n- Simplify protocol processing by simplifying header\n- Improve multicast support",
        "answer_feedback": "All four objectives of IPv6 are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "350fb8e5ac97453eb5cdbd66f772461e",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Some of the objectives are: support billion of end-systems, reduce routing tables, simplify protocol processing, increase security, support real ti,e data traffic, provide multicasting, support mobility, be open for change, coexistence with existing protocols",
        "answer_feedback": "All the IPv6 objectives mentioned in the response are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7f578adfd0274a7c887e03af2f84f48a",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "IPv6 aims to:\n- support billions of end systems, through longer adresses;\n- reduce routing tables;\n- simplify protocol processing, since headers have less fields;\n- provide multicasting;\n- be open for change in future, through extension headers.",
        "answer_feedback": "All the objectives of IPv6 mentioned in the response are completely correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fd11ecdf38d947a0ba88a48386529cf9",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Support billions of end systems due to a longer addressing scheme.\nReduce routing tables and simplify protocol processing due to simpler header structures.\nSupport real time data traffic due to traffic classes and flow labels\nProvide multicasting and support mobile end systems",
        "answer_feedback": "The response states four correct objectives of IPv6.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3b4ec11883eb48768dcc099f98074f2f",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "Support billions of end-systems \n\nsimplify routing tables and protocol processing \n\nsupport real time traffic (quality of service)\n\nto be open for change",
        "answer_feedback": "The response answers four objectives of IPv6 correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2e0228e7a0974409b491d6475e08d36a",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "- Extending the address space of IPv4 (Longer addresses), to support billions of end systems.\n- Simplify protocol processing (simplified header)\n- Increase security (security means integrated)\n- Allow Multicast",
        "answer_feedback": "The response is correct as all four IPv6 objectives are completely accurate.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e254210354a04d349c78462f1fe45608",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "- Support billions of end-systems by using longer addresses\n- Coexistence with existing protocols\n- Support roaming\n- Reduce routing tables",
        "answer_feedback": "The response is correct as it contains accurate IPv6 objectives.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e5d5852e3ad3423bbabbbe55bfe2a7c5",
        "question": "What are the objectives of IPv6? Please state at least 4 objectives. ",
        "reference_answer": "To support billions of end-systems.\nTo reduce routing tables.\nTo simplify protocol processing with Simplified header.\nTo increase security.\nTo support real time data traffic (quality of service).\nFlow label, traffic class.\nTo provide multicasting.\nTo support mobility (roaming).\nTo be open for change (future): extension headers for additional change incorporation.\nTo coexistence with existing protocols.",
        "provided_answer": "1.) Global addressing concept for end systems\n2.) Simplified address allocation\n3.) Addresses independent from\n  - type and topology of the subnetworks\n  - number and type of the subnetworks to which the end systems have been connected \n  - location of a source end systems\n4.) Increasing data security",
        "answer_feedback": "The response correctly answers four objectives of IPv6.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "5d8a457fea9c4c349f93286a7170e569",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The size of \"collision domain diameter\" is dreduced by about 10 times, e.g. from ca.3000m to ca.300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b0aa489b5d74455f9738e62973a8b853",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter decreases by the factor of 10. That means:\nnew collision diameter = old colision diameter / 10",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e3271b5f2c5040b9ad42c038aa8b0d6c",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The possible distance is reduced by factor 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8b4ae5ccaa28417ba297250d73b6c86a",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter reduces by the same factor for example for a network with 10Mb/s has a collision domain diameter of 3000m and when we increase the speed of the network by 10 to 100Mb/s, the collision domain diameter will become 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bbc72decacd3415f8466131d1e5fff30",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "Then the maximum segment length becomes 2000m, which is too long for collision recognition.",
        "answer_feedback": "The collision diameter decreases by a factor of 10 rather than becoming 2000m for a collision to be still detected.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "5a142b9a85d54691836d731cd644409d",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "Collision Domain Diameter means the maximum length of line between two nodes in the network which you can use in respect to the length of used frames, the sending speed and the fact, that you want to be able to detect any kind of data collision during transmission. If we now increase the speed of transmission with all other factors remaining the same - especially the frame length -, the time of transmission for each node decreases. So there is less time of data frames travelling on the line during which the stations could detect a collision. To keep the function of collision detection, the maximum length of line - a.k.a. the Collision Domain Diameter - in the network has to decrease to the same factor. So an increase of transmission speed from 10 Mbit/s to 100 Mbit/s - factor 10 - causes a reduction of Collision Domain Diameter to the factor of 10, too. For example, in Ethernet 802.3 this means a reduction from 3000m to 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "05b3625fdc8c40a3a0e19806c2339216",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter is decreased by the same factor, the network speed is increased by, i.e. if the collision domain diameter in a 10Mb/s network was 100m, the collision domain diameter in the same network with 100Mb/s would be 10m. \n\nThis is because the sender still must be able to recognize a collision during simultaneous sending.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "38b98440b2e847c9958f6ac42146eacf",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The diameter decreases by factor 10. The increased  Bit rate leads to a reduced bit duration but the transfer speed remains the same. The result of that is a shorter maximal distance between two stations which is allowed so that collisions can still be detected.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "10c3e3fc9b6842aaa2388773055e88ce",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision Domain Diameter will be increased to 10times compared with the distance in 10Mb/s.",
        "answer_feedback": "The diameter is decreased by a factor of 10 instead of increased.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "6176b653698d4948993db9f32ba43d07",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "With CSMA/CD, if the speed of the network increases by a factor of 10, the collision domain diameter shrinks. The distance decreases by a factor of 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "32d2706a54e9419baef58a125245c9ff",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter is shortened by factor 10. Assuming the collision domain diameter is 3000m at 10Mb/s. If the speed is increased to 100Mb/s, the collision domain diameter will be 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "550fada62f364553943bba67078eb54c",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter would be 10 times smaller which would have many collisions as a consequence, making it inadvisable to choose these dimensions.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "65d61ac26fa64080977237ce85808b9f",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "Collision domain diameter is the distance between the two furthest nodes. If I use CSMA/CD, then the distance limitation for collision domain diameter will be one tenth as large as before when I increase the speed of a network by a factor of 10.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "39e0833365ec46119b62807232ca5f22",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter, the maximal distance where collisions can be recognized decreases tenfold. If the collision domain diameter was 3000 meters before, it now is 300 meters.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8806b3fc084b468ea7b4be4a605dbc3f",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The collision domain diameter (CDD) is reduced to 1/10th of the original. In the given example CDD is reduced from approximately 3000m to about 300m.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "fd3ad1d878a945238165b56f9be3cb21",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The maximal distance between stations reduces by a factor of 10.\n\nEx. : 64 byte sent with 10 Mb/s: Max Distance of 5.12 km\n        64 byte send with 100 Mb/s: Max Distance of 112 km",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "50bd285e3ca54b17af8e0b28812d0f59",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "If the speed got ten times faster, we have to decrease the maximum collision domain diameter by 90%, that is the distance between the nodes, so that a collision can still be detected while sending. So there is basically a tradeoff between distance and efficiency.",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "389546012a3a4983b34fc2e9f599d451",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "When the speed of a network is increased by a factor of ten, while letting everything else remain the same, the collision domain diameter reduces by the same factor. So, the diameter will decrease by a factor of 10 (that is, divide by 10)",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a0bdb6a4d8564ce887ec20cdf3bfe923",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "the collision domain diameter will be increased.",
        "answer_feedback": "The response is incorrect as it states that the diameter increases. Instead, for collisions to be detected, the diameter decreases by a factor of 10.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "7014f6b6797442549d5f70e2a16c28b5",
        "question": "What happens to the \"collision domain diameter\" if you use CSMA / CD and increase the speed of a network by a factor of 10, eg from 10Mb / s to 100Mb / s (all else being equal)?",
        "reference_answer": "Diameter decreases by a factor of 10, e.g 300m to 30m.",
        "provided_answer": "The \u201ccollision domain diameter\u201d is the distance two station can have and still detect a collision while sending. It depends on the minimum frame size, the speed of the signal through the medium and the bit rate of the network. If the bit rate increases while nothing else changes in CSMA/CD a frame is transmitted in less time than before. In this time the signal travels a shorter distance thus decreasing the \u201ccollision domain diameter\u201d. If the speed of the network increases by a factor of 10 the \u201ccollision domain diameter\u201d shrinks by a factor of 1/10",
        "answer_feedback": "The response is correct as it answers the change in the diameter scale accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "417fc4927628483e8c95798669800600",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting means, that the sender can concatenate a sequence of multiple frames and transmit them in a single transmission.\nAn advantage is, that frame bursting has a better efficiency than carrier extension, because carrier extension extends the length of the frame from 512 bit to 512 byte but not the data and therefore ca. 90% of the frame is useless.\nA disadvantage is, that while with carrier extension every frame is send immediately, with frame bursting the sender must wait until he has reached the number of frames neccessary for a transmission, e.g. 10 frames. So if you want to send 1 frame now, the sender will wait until 9 other frames arrive and then transmits the whole concatenated sequence.",
        "answer_feedback": "The response correctly answers the definition of frame bursting, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a0ed266d6e24435c92d90f3a7fb24509",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting enables the sender to transmit concentrated sequence of multiple spaces in a single transmission. Therefore a buffer waits for a set of packets to send them all once together.\n An advantage of frame bursting is the high efficiency due to the amount of data that can be sent in one transmission. In the carrier extension only 9% user data is possible to send which leads to a low efficiency. \nA disadvantage of frame bursting is the long time it takes to be transmitted. If you just want to send two packets you have to put some additional data or rubbish inside because the frame only will send if the whole frame is filled with data. The higher latency is introduced through the packing/filling of the buffer, depending on the implementation, either the two packages get stuck or transmitted with a higher latency with rubbish frames to fill the buffer.  This causes a delay. Therefore the carrier extension is better to send less packages and does not lead to a delay.",
        "answer_feedback": "The response answers all three parts of the question correctly. However, 9% efficiency of the carrier extension feature is only for the worst case. The efficiency depends on the  size of actual data sent which can be between 46-1500 bytes.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "398f33db00fd481d8a682c66d825e6fb",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "When using frame bursting within a shared broadcast mode, the stations wait until they have a min. number of frames to send and then send them all at once in a single transmission as a sequence of multiple frames. It is more efficient as carrier extension, especially if the station has continiously a lot to send. Otherwise the frames have a lot of waiting time to be ready for a transmission.",
        "answer_feedback": "The response correctly explains the frame bursting concept, an advantage and a disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "36786a81ca7a42aab54055d8b5ef4603",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is used to increase the throughput of data in a network without changing the cables or connection within it.\nThe wait time is used by the clients to burst up a sequence of up to three packets, then they take their waiting period.\nIt should not be used with more than 3 clients because too much data can kill the throughput for the whole network.\n\nadvantage:\nbetter efficiency\n\ndisadvantage:\nneed frames waiting for transmission",
        "answer_feedback": "The response answers the advantage and disadvantage part correctly. However, the definition is incorrect as it contains details, such as a maximal burst of three packets and a recommendation not to use it with more than 3 clients, that do not hold for frame bursting in general.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "38f264d805e74e9789e0c49b42bc2a01",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a communication protocol, where one transmission can consist of a concatenation of frames. Compared to the carrier extension the throughput increases and has higher efficiency. One disadvantage is that the frames have to wait for their transmission. So frame bursting shouldn't\nbe used with more than 3 clients as this disadvantage can result in lower for every client.",
        "answer_feedback": "The response correctly states the frame bursting definition, advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d42d4a39f7c7484f8cb8a941889c5b6c",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "In Frame Bursting the sender can transmit several frames in one single transmission. \nThe advantage of the frame bursting is definitely a lot higher efficiency than the carrier extension has (which loses around 90% for collision detection). \nThe disadvantage of frame bursting is that the frames have to wait for their transmission.",
        "answer_feedback": "The response gives the frame bursting definition, its advantage and disadvantage correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "06b55442218c4bc5b125637cf76dedd8",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "T1.Allow sender to transmit concatenated sequence of multiple frames in single transmission .\n2.Needs frames waiting for transmission .\n3.Better efficiency .",
        "answer_feedback": "The response is correct as it answers all parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8d5b141fab5142d4aa2ab6f445831b14",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting allows sender to transmit concatenated sequence of multiple frames in single transmission\n\nAdvantage: Better efficiency\nDisadvantage: It needs frames waiting for transmission. Therefore, it has end-to-end delay problem",
        "answer_feedback": "The response correctly answers all the parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "458eafc25d934b8e83b27048e3abe3c0",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a technique where the sender waits for a certain number of frames to be concatenated into one single \u201cbig\u201d frame and then send this \u201cbig\u201d frame in one go. \n\nAdvantage: better efficiency in data transmission, while in carrier extension introduces only trash information just to extend the frame length. \n\nDisadvantage: because the sender has to wait until the concatenation buffer is full then the big frame is only ready to send. This accidentally introduces more end-to-end delay which hampers the performance and user experience of interactive applications (e.g. live stream video, live phone and video calls, etc).",
        "answer_feedback": "The response correctly answers the frame bursting definition, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "db995f3ea4aa423197ff275bf805d06d",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame Bursting is a Shared Broadcast Mode of Gigabit Ethernet which concatenates multiple frames and send them in one transmission. \nDisadvantage: Frames must wait for transmission until enough frames are queued for sending so that the minimal length is achieved\nAdvantage: better efficiency because in Carrier extension mode short frames are extended with non-sense bits to achieve the minimal transmission length",
        "answer_feedback": "The response correctly answers all the parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b97868de7d8446728f38b923edca30ee",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame Bursting allows sender to transmit CONCATENATED SEQUENCE OF MULTIPLE FRAMES in a single transmission.\n\nAdvantage as compared to carrier extension is:\n-Better efficiency\n\nDisadvantage as compared to carrier extension is:\n-it needs frames waiting for transmission",
        "answer_feedback": "The response gives the correct definition of frame bursting, including an advantage and a disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "735fff07b0504e01969b250613bb006e",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Two solutions to the problem mentioned before are carrier extension and frame bursting. The basic principle in carrier extension is to attach a new extension field containing \u201cscrap\u201d data to the frame just to make it larger so that collisions can be detected. In Frame Bursting you take a similar approach, but instead of appending otherwise useless data, you just send a concatenated sequence of multiple frames in a single transmission. This is much more efficient than the carrier extension as no bandwidth is wasted on \u201cscrap\u201d data but the drawback is that the end-to-end delay for a frame is increased since you generally have to wait until you have the specified number of frames to send as a sequence which is bad for interactive services.",
        "answer_feedback": "The response correctly answers the definition of frame bursting, its advantage and disadvantage.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "94cd0306a0d14d699264c91ebf092277",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting: allows sender to transmit concatenated sequence of multiple frames in a single transmission\n\nAdvantage: more efficient than carrier extension\n\nDisadvantage: needs frames waiting for transmission",
        "answer_feedback": "The response answers all three parts of the question accurately.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9e534a26afb6424698450cd623677687",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is the transmission of concatenated frames in a single transmission. This increases the efficiency in comparison to the carrier extension because we only send relevant data. However, we have to wait until the buffer is full in order to concatenate and send them which increases the end to end delay.",
        "answer_feedback": "The response correctly answers all three parts of the question.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "08382e721318400cba2b25b4ad364360",
        "question": "What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.",
        "reference_answer": "Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\nDisadvantage :need frames waiting for transmission or buffering and delay of frames",
        "provided_answer": "Frame bursting is a communication protocol feature for the principle of shared broadcast mode in gigabit ethernet. \nAdvantage: frame bursting has a better efficiency than carrier extension. \nDisadvantage: needs frames waiting for transmission",
        "answer_feedback": "The response is partially correct as it answers the advantage and disadvantage parts correctly, but the definition of frame bursting is too broad as it does not explain what the feature does. Additionally, frame bursting can be used in other scenarios than gigabit ethernet as well.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "37f19cc6f2c847bbad071400d736264b",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "asynchronous:\n- each character is bounded by a start bit and a stop bit\n- generally low transmission rates, often up to 200 bit/sec\n- simple and inexpensive\n\nsynchronous:\n- several characters are pooled to frames and those frames are sent\n- frames are defined by SYN or flag\n- more complex, but higher transmission rates",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3d7eea11a7ac4db9a4f4f8accbec46fd",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "In the asynchronous transmission mode each character is bounded by a start bit and a stop bit. In the synchronous transmission several characters are pooled to frames, that are defined by SYN or flag.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "357ae5ef987443aeb7d4f44e2c50ef9a",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission (byte-oriented/ block-oriented):\nData is sent in form of byte or character. The data (byte/character) is then bounded by a start bit and a stop bit. This is considered to be the simpler and less expensive way but it only supports low transmission rates.\n\nSynchronous transmission (character-oriented/ count-oriented/ bit-oriented):\nData is sent in form of frames. Therefore, several characters are bundled to frames. The Frames are defined by SYN or flag. This is considered to be the more complex approach. However, it supports higher transmission rates.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b44f0154cbc7463fb0b7dc76faf5b19c",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission mode: \n- Each bit is transmitted individually\n- Each character is bounded by a start bit and stop bit\n- Simple and inexpensive\n- Low transmission rate\n\nSynchronous transmission mode:\n- Combine multiple bits to be transmitted at the same time\n- Several characters pooled to frames -> different possibilities of frames bounding\n- Complex\n- High transmission rate",
        "answer_feedback": "The response answer is correct as it correctly explains the differences between synchronous and asynchronous mode.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "dc46d268504d456faca768ba30c821bf",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "The asynchronous transmission mode needs a start and a stop character before and after each byte. Therefore it has low transmission rates. But it is very simple and inexpensive.\nThe synchronous transmission mode has higher transmission rates. It works by pooling several characters to frames defined by SYN or flag. Therefore it is more complex than the asynchronous transmission mode.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "be35f019cc624cbca2bf4113ce3e4d3b",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "In asynchronous transmission, each character contains a start bit as prefix and a stop bit as suffix.\nIn a synchronous transmission, several characters are grouped together in a frame, which are defined by a SYN or flag.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "a0cd5a4831d54518b4caacf3a690785a",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission:\n Each character is bounded by a strat bit and a stop bit.\n+ simple and not expensive\n- low transmission rates, often up to 200 bit/sec\n\nSynchronous transmission:\n Several characters pooled to frames.\n Frames defined by SYN or flag.\n+ higher transmission rates\n- more complex",
        "answer_feedback": "The response correctly answers the differences between synchronous and asynchronous transmission mode.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d76164668ffa46bf87857c79650b6d71",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Async. Transmission: Each character of a message is bounded by a start and stop bit.This mode is both simple and inexpensive, but to the costs of low transmission rates which are mostly limited to 200 bit/sec.\n\nSync.Transmission: Several Characters of the message a \"pooled\" to frames and those frames are defined by SYN or flag. This apporach is more complex but enables higher transmission rate.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e73c345e659f4f029cd414b28e40ec34",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "In asynchronous mode each byte is bounded by start- and stop Bit and sent individually at any time. This is inefficient and has low transmission rate.\nSynchronous has higher transmission rate, but complex with regards to \u201chow to detect beginning and ending of fields within a frame?\u201d",
        "answer_feedback": "The response is correct as it answers the differences between synchronous and asynchronous transmission mode correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7849853c8aa94b6bb38be70517905c19",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "- Asynchronous: start and stop bit between every byte (bad transmission rate, but easy to realize)\n\n- Synchronous: use of flags/SYN for start and end of data frames containing multiple bytes (higher transmission rate, but more complex)",
        "answer_feedback": "The response explains the differences between asynchronous and synchronous transmission correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3c109c56a5b24e998c01fe7d2daa1cf5",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronisation between sender and receiver.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "cc472cee228247ad9a4bbd89a24b7c17",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "The asynchronous transmission puts on each byte (character) a starter bit and a stop bit. That way is cheap and easy to implement, but the transmission rate would be low.\n\nThe synchronous transmission puts several bytes (characters) to a frame. SYN or flags define these frames at the end and beginning of the frame. This mode is more complicated but has a higher transmission rate.",
        "answer_feedback": "The response correctly explains the differences between asynchronous and synchronous transmission.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b0cdf13ad14846a1a6ef94f123c01b58",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission:\n- Each character is bounded by a start bit and a stop bit\n- Simple + inexpensive, but low transmission rates, often up to 200 bit/sec\n\nSynchronous transmission:\n- Several characters pooled to frames\n- Frames defined by SYN or flag\n- More complex, but higher transmission rates",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "b54f189b594c4d4fb0b9523c466250be",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Synchronous: Characters are sent in frames. Their size defined by a SYN or flag sent at the start and the end of the transmission.\nAsynchronous: Characters begin and end are marked a start and a stop bit. Each character is sent independently.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "d68d1f223c324e9ea6ca1e98b2ddf14f",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous mode: each character is bound between a start bit and a stop bit. -> simple and inexpensive but low transmission rate. \n\nSynchronous mode: several characters form into one frame, frames defined by SYN or other flags.  -> More complex but higher transmission rate.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "65f4e7c08f6f4dd7b2a238cea8a088eb",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous Transmission: In this type of transmission, each character is bounded by a start and end bit. This is simple but offers low transmission rates upto 200bits/sec.\n\nSynchronous Transmission: In this type of transmission, several characters are pooled  into frames and these frames are added SYN or flag, this offers more transmission rates than the asynchronous.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "bb9c185b275d4a8fb38cf04bd710c6ec",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission \u2013 Characters are transmitted individually, by encapsulating them within start and stop bit. Its simple and inexpensive but has low data transmission rates\nSynchronous Transmission \u2013 Several characters are combined into a Frame and encapsulating frame within SYN or Flag symbol. Its complex but offers higher data transmission rates.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7c82ae35ed854048a9128949169fa15e",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission:\n- Byte-oriented / Block-oriented\n- Each Byte/Block (packet) is bounded by start bit and stop bit\n- low transmission rates, often up to 200 bit/s\n- simple, inexpensive\nSynchronous transmission:\n- Multiple Characters pooled to frames\n- Frames begin and end with SYN or Flag\n- higher transmission rates\n- requires additional effort / time\n- eg. Character-Oriented, Count-Oriented, Bit-Oriented Protocols",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "61201db677b54b49a7200dae2f4d4616",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous transmission: Each character (byte) is sent one-by-one at any time and is surrounded by a start and stop bit. This allows only low transmission rates, but is a simple and inexpensive form of transmission.\n\nSynchronous transmission: Here the characters are combined to frames, and headers and trailers (SYN or flag) are attached. As the frames can have different lengths, protocols (Character oriented, Count oriented, Bit oriented) are needed to be declared, defining what is part of the flag and what is part of the data. This form of transmission is more complex, however it offers higher transmission rates.",
        "answer_feedback": "The response correctly explains the differences between asynchronous and synchronous transmission.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f1faf83f6b364136b818f271a6e00596",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asynchronous: \n\u2022\tEach character is framed by a start bit and an end bit\n\u2022\tSlow transmission\n\u2022\tNo gaps between data\n\nSynchronous:  \n\u2022\tCharacters are pooled in frames, which are defined by flags or SYNs (synchronous idle characters)\n\u2022\tFast transmission\n\u2022\tGaps between data",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "9d309c81477b4bff8481025038e6286c",
        "question": "What is the difference between asynchronous and synchronous transmission mode in the Data Link Layer.",
        "reference_answer": "Asynchronous transmission: Every character a self-contained unit surrounded by a start bit and a stop bit, which is an easy and cheap pattern, but causes low transmission rates.\n\nSynchronous transmission: Several characters pooled to a continuous stream of data (frames), Frames defined by SYN or flag, higher complexity, but higher transmission rates. Requires synchronization between sender and receiver.",
        "provided_answer": "Asychronous: Each charactes is wrapped by a start and stop bit. \nSynchronous: Several characters are pooled to frames, defined by SYNs or any other type of flags.\n\nIn the asychronous transmission mode multiple characters are combined in frames. The frames can be bounded by idle times, character-oriented, count-oriented, bit-oriented or by invalid characters of the physical layer. \nThe synchronous transmission is more complex, but has a higher transmission rate than the asychronous.",
        "answer_feedback": "The response answers the differences correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "c3bc25a3192647fc934b39188f3bde70",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a network management protocol that is primarily used to automatically assign an IP address to each device/host on a network so they can communicate with other IP networks or endpoints in the network.",
        "answer_feedback": "The response is partially correct because the DHCP usage part is not specific enough. DHCP is used to assign automatic, manual, and dynamic IP addresses in a network.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "892c515d874142abad443d9e2ab8ce37",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a protocol which is used to configure network-data and allocate IP-adressen (manual or automatic) within a network:\nEvery Host can request and process IP-Configurations from a \nDHCP-Server. This simplifies the process of IP adress assignement and extends the functionality of the former used RARP.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly except it does not configure network data.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f612f1b6c05a4c7fa376e74a8f9d8240",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is the new version of ARP (Address Resolution Protocol), basically DHCP does the same : \nThe DHCP (=Dynamic Host Configuration Protocol) is a communication protocol (for network management) on internet protocols.\nA DHCP server is used for assignment : A DHCP server assigns network configurations and IP addresses to devices on a network in \na dynamic way. As a result, the devices are to work and communicate with other IP networks.\nSo, DHCP simplifies installation and configuration of end systems, it can assign manual and automatic, dynamic IP addresses to devices in a network and can provide network parameter information like netmask, DNS server, default router and so on.\nThe IP address assignment is for limited time available only.",
        "answer_feedback": "The definition part in the response is partially correct because DHCP is not a new version of ARP. DHCP uses ARP requests during the IP allocation process but they are separate protocols.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "a7ece11370d749c3a8f223dc6661ed62",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "Simplifies installation and configuration of end systems , Allows for manual and automatic IP address assignment, May provide additional configuration information(DNS server, netmask, default router, etc.)\nUsed for:  Request can be relayed by DHCP relay agent, if server on other LAN",
        "answer_feedback": "The stated usage is not a usage but the process of how it is carried out.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.5
    },
    {
        "id": "0d2f6680de20496b8249543307ba652e",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "The Dynamic Host Configuration Protocol  is a network management protocol.\n\nUsed for intranet or network service providers to automatically assign IP addresses to users.\nUsed by the intranet administrator to centrally manage all Computers.",
        "answer_feedback": "The response is partially correct because DHCP also allows the manual and dynamic allocation of IP addresses.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "89117ee2e8c545f1a1e511b782fbeb84",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a new version of RARP and BOOTP but more or less does the same thing. Most of the time its a router that can both manually or automatically assign IP addresses (and other information such as DNS Server, netmask, default router etc. ) to clients in its domain. The request from the clients is a broadcast request, the DHCP server then answers. \nA big advantage is the dynamic allocation of IP addresses. Each address is given out with a lease timer. A client has to refresh his lease before the timer runs out. If it does, and the client has not refreshed his lease, that IP address can once again be assigned to a new client. Therefore it allows to reclaim addresses of disappearing hosts.",
        "answer_feedback": "The definition/description is partially correct as it is a separate protocol or a successor protocol with extended functionality and not a new version of RARP and BOOTP.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "f40f5399004e4202a0038c6950cc2b5a",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is a newer version of RARP. \nSystems use this protocol to resolve their own IP address in a network from their hardware/MAC address. \nA specific DHCP server assigns the IP addresses and is contacted to resolve them.",
        "answer_feedback": "The response answer is partially correct because the statement regarding the DHCP server is correct. However, DHCP is more a replacement of RARP and not a new version. Also, ARP is used to resolve the IP address from the MAC address.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.25
    },
    {
        "id": "62b60b31759e421da9ef16d680871fd4",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "With the \u201cDynamic Host Configuration Protocol\u201d a host can find out about the IP address to a dedicated hardware address. It simplifies the installation and configuration of end-systems and it is used for manual and automatic IP address assignment. It may also provide additional configuration information such as DNS server or netmask information.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "df0a1cfb4efe42c5983d100ed2ac2d7a",
        "question": "What is the \u201cDynamic Host Configuration Protocol (DHCP)\u201d? What is it used for? ",
        "reference_answer": "The Dynamic Host Configuration Protocol (DHCP) is a network management protocol used in Internet Protocol (IP) networks, whereby a DHCP server dynamically assigns an IP address and other network configuration parameters to each device on the network. Further, DHCP has largely replaced RARP (and BOOTP)\n\t Uses of DHCP are: Simplifies installation and configuration of end systems. Allows for manual and automatic IP address assignment. May provide additional configuration information  (DNS server, netmask, default router, etc.)",
        "provided_answer": "DHCP is used to assign IP addresses and other configuration to (new) hosts in a network. After an initial DHCP DISCOVER packet of a client the server sends the assigned IP address back with additional information, like DNS server or netmask.",
        "answer_feedback": "The response answers the definition and the usage of DHCP correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "dbecb8c382d84d629f8cfda0d4349b4d",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "If you send data in both direcions, you can add the acknowledgment ,for earlier packages, on the next data package.",
        "answer_feedback": "The response identifies the underlying requirement duplex connection correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "2ec6726925d149e8a0f8ae8698d7c958",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The initial SeqNo needs to be known + the next SeqNo and the next ACK-SeqNO needs to be known.\nOtherwise, Piggybacking is not possible.",
        "answer_feedback": "The response is incorrect as the main requirement for piggybacking is a two-way/duplex channel. The points stated in the response are an implementation detail of the sliding window protocol.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "82911b1bdf4e4833aa4856ca1126ab9a",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The communication needs to be duplex.\nAdditionally, there should be a time period, within this time period, data link layer should wait for the next packet, and attach the acknowledgement to the outgoing data frame and then send the frame.\nWhen time expires and there is no packet to be sent, link layer sends a separate acknowledgement frame.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "f61b064a65ad4e3b976e706c26dc77c9",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Before using Piggybacking extension, there should be duplex operation. Furthermore, a new packet should arrive quickly then the acknowledgement is piggybacked onto it; otherwise, if no new packet has arrived by the end of this time period, the data link layer just sends a separate acknowledgment frame. \n\nAlso, the sliding windows protocol will utilize the bandwidth of the communication channel with piggybacking, frames may contain implicit acknowledges. For example, the intuitive SeqNo. is 0, then the next SeqNo. and the next ACK-SeqNo to be expected is given.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "8a306aed09d642b2881a73490a40a3a1",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The receiver must have data to send back to the sender so he can attach the ACK information to that data. If he has no data to send, the service can be jammed. To prevent this, a receiver timeout can be added so that after the timeout has expired and no data was sent, an ACK packet is sent independently",
        "answer_feedback": "The response does not answer the underlying requirement for piggybacking. The response states a possible situation in piggybacking and how to overcome it.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "9a9983a6ec9d40448302166cd4e956be",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "A relieable in-order delivery of packets (like data link layer 2 in OSI)\n\nAt least an acknowledged connectionless service or an Acknowledged connection-oriented service (for feedback if the packets / frames are received).",
        "answer_feedback": "The response is not directly related to the piggybacking but to sliding window protocol in general.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "796d2be28fe943b19c2bac6d36fd060e",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "-It has to be a Full-Duplex operation\n-Frames must contain implicit acknowledgments",
        "answer_feedback": "The response answers the underlying requirement correctly. Both points are correct.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3f4dfec8e6804088896b64c370cf8838",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Data and Acknowledgements are send in both directions (sender to receiver and receiver to sender). The data and acknowledgements are bundled into one package.",
        "answer_feedback": "The response is correct as it implicitly answers the requirement of duplex communication.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "3a73cb0819ee4608aaf03f438de28127",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Duplex operation",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "0feac5f89e2c448aaa965172753d53ba",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Piggybacking means the ACKs are not sent separately but are inside the header of the next package that the party who is acknowledging the last package wants to send.\nSo piggybacking only makes sense when both partners are sending and receiving data, i.e. we have a duplex transfer operating mode.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7c1d371998d24699b68739569fd93840",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Frames may contain an implicit ACKs.\nDuplex Operation. \nIt has to have an initial SeqNo. of 0",
        "answer_feedback": "Apart from the correct answer of duplex operation, the response also contains other requirements. The first point is true, but it refers to what happens in piggybacking in general. The last point is incorrect as it is specific to the example given in the slides.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "95c60c298fbd43ca9a82d4d302c37d59",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "we require atleast a (semi)duplex data transfer",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "7f3e239e989241fd9d6ddcaee5a7ef07",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "We need a duplex operation. This means that Sender and Receiver both sends and receives frames. Then ACK and data can be merged into one frame and sent together.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "070e2ed4e09044d08c1e3dbca7043683",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "sending must be possible in both directions in order to send data and acks (Two-way-communication) and the frames must be able to contain acks.",
        "answer_feedback": "The response states both the requirements correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "6b783d4c1f82440a8d3143e39653c53e",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Both parties must be able to send data and acknowledge information",
        "answer_feedback": "The response answers the underlying requirement, i.e. duplex connection, correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "e680404bd5ff4f06bedfdb58089e0ee6",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "The participant, who sends the ACK, has to have Data, which he wants to send. If there is no Data to \"biggyback\" the ACK on, the participant will wait infinitly for data to send with the ACK and therefore the piggyback extension would not work.",
        "answer_feedback": "The response is incorrect because a dedicated timer can be used on the receiver side to overcome the above problem of no data on the receiver side. After the timeout, an acknowledgment is sent independently.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "ca7d1c911c864abca82a7bf5b0a9bfaf",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Piggybacking in sliding window allows to send the acknowledgment of a received frame together with data.\n- Sender and receiver needs two sequence counter. One for its own frames and one for the acknowledgment of the received frames.\n- Benefits the most from a duplex connection\n- when a frame is send, the SeqNo of the frame and the SeqNo of the last received frame (for acknowledgment) are send together.\n- The SeqNo is initialized with 0 and is increased before a new frame is sent.",
        "answer_feedback": "The response is correct as it identifies duplex connection as one of the requirements, but the sequence number need not be initialized with 0.",
        "verification_feedback": "Partially correct",
        "max_score": 1.0,
        "normalized_score": 0.75
    },
    {
        "id": "8161ed8f4c35420d81814d73e55b1c26",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Both parties have to advertise their window sizes to each other to avoid the received data to be thrown away (so the receiving buffer was full). Therefore, they need to introduce a new field to the frames which holds this value. Alternatively, they agree upon first communication to some static window size and afterwards send their data (and hope the free buffer sizes of both remain the same). In both cases it would be recommandable not to send huge data chunks upon first communication, because both parties don't know the window sizes of the other one yet. With each frame they send a sequence number (even if no data is sent, so the receiver is able to acknowledge it), the ACK-number (even if no new data has been received, so the receiver can use this as base for their sequence number), and lastly the window size.",
        "answer_feedback": "The response is incorrect. All the stated points are correct but are related to the window sliding mechanism in general and how the initial setup occurs.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "8af80ae20a734672a7b5f030de0c7821",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "Every transmission must contain an ACK.",
        "answer_feedback": "The response does not answer the underlying requirement for piggybacking. A duplex connection is needed, so that data and acknowledgments can be sent both ways.",
        "verification_feedback": "Incorrect",
        "max_score": 1.0,
        "normalized_score": 0.0
    },
    {
        "id": "99e09920912547c2841ed020d5520cfe",
        "question": "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?",
        "reference_answer": "Piggybacking only makes sense if there is a full-duplex or semi-duplex connection between sender and receiver i.e.  two-way communication or Frames must contain additional field for acknowledgement.",
        "provided_answer": "To be able to take advantage of the piggybacking extension in the sliding window protocol, a full duplex communication channel with both parties actively sending messages is required. Otherwise, the implicit ACKs cannot be added to outgoing data frames.",
        "answer_feedback": "The response answers the underlying requirement correctly.",
        "verification_feedback": "Correct",
        "max_score": 1.0,
        "normalized_score": 1.0
    },
    {
        "id": "90ccaf87dcd440ae8602a5b2a2b73b22",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The Problem will occur at the receivers end which cannot differentiate between the correct and the duplicated data without given any additional information",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "c555871e7efe4e5fa02352b4b1888acc",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "if the receiver is not capable of differentiating between valid and duplicated packets\u00a0it may act on the same information twice.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "cf67ef58d125439eb4bd7a5cff26f5f2",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "without additional means the receiver\ncannot differentiate between correct\ndata and duplicated data.",
        "answer_feedback": "The response is correct. The response can also state what will be the consequence in such a scenario.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "4521515af2f34a6a82b2d8c0bc8bb80d",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Without additional safeguards duplicate packets could trigger an action (i.e. Bank Transfer) a second time.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "a59c5c1849e640fda2d37cf5ffa352e3",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Without additional measures in a network duplicate packets can not be differentiated from the correct packets and therefore a transaction would be re-executed by the duplicates.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "d344fa984a1f458895c9aa58011f29af",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The network protocol needs a method to prevent duplicate packets otherwise a loop could be formed where the duplicate packet is forwarded indefinitely. An IS would need to keep track of all packets it has seen recently which is not feasible.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "a046d27716434e61b787e0a2ebd30db5",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "The performance of the network goes down with too many duplicate packets and we need to find ways to differentiate between correct date and duplicate date.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "2032f14f3ef14833a27a8614b9ce44b8",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate packages are handed over to the receiver too late at an undesired time, so that the receiver can't handle it or handles it twice, which for example can result in a loss if for example money is booked twice.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "f73fb47f8a8f476787e243ad43f06df1",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "If the receiver can't detect that a certain packet is a duplicate of one that arrived before, it might be processed independently causing maybe an action to be executed twice (for example a bank transaction).",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "6c177189b5fb40dea9f8eabffd812f81",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "If you have too many duplicates the performance goes down and the network gets overloaded.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "e3b363d9bd114126a3106a82b6cac438",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicates can lead to network congestion.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "4dbcd208aeac40d3b6b9428afb8640cc",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "They congest the network by creating unnecessary traffic.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "2bddea03f90c4eb6bb79fc3cb177818d",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "If there are many duplicates in the network, the performance may decrease (-> congestion) and problems might occur (bank example from the lecture).",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "be15d5fcf63d4d2f837cc59e1bb413a6",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate packets arriving at receiver may cause a transaction to be completed multiple times.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "f7b5e8354f284f4b9736378b4524b0ba",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate data cannot be differentiated from correct data , and it would re-execute the transaction.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0db1acd6622b44eb919b3fda5f84e725",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicate k\u00f6nnen zum Problem werden, weil zum Beispiel Befehle mehrmals ankommen und auch mehrmals ausgef\u00fchrt werden.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "02fb0d14e1764a14a0e3e1d93d01c00b",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "Duplicates may look like two separate commands/messages to the receiver.",
        "answer_feedback": "The response is correct. The response can also state what will be the consequence in such a scenario.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "a4fc8dcddb9a4dc4b3a970565c90bbfd",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "They can cause an action to happen twice since the receiver may not be able to distinguish between a packet that was sent twice and one packet that arrives twice.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0198472195004fc2a3a7e8fc00dfc332",
        "question": "Why can duplicate packets be a problem in a network? Please state your answer in a single sentence.",
        "reference_answer": "if the receiver is not capable of differentiating between valid and duplicated packets it may act on the same information twice.",
        "provided_answer": "This can be a problem if the recipient of the duplicates has no way to distinguish between correct and duplicate data, such that late duplicates could be incorrectly considered as new data.",
        "answer_feedback": "The response is correct.",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "34b80cddaf004139904dae9cea358350",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0\u00a0\n127.255.255.255\nThe first adress and the last adress of each subnet are reserved",
        "answer_feedback": "The addresses have a range: 127.0.0.0 - 127.255.255.255\nMissing Loopback",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "32bce467b7764dc5acce04d213290af9",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 and 127.x.y.z",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "f53726e7af9c4765b21e0e9c9880c5c6",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "reserved for host\n0.0.0.0/8 - 0.255.255.255/8 \nreserved for loopback addresses/broadcast\n127.0.0.0/8 - 127.255.255.255/8",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0467703ff80547778fed769656beed43",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "For each x \u2208 {1,...,127} the network address x.0.0.0 and the broadcast address x.255.255.255 are reserved.\nAccording to RFC 6890addresses 10.0.0.0-10.255.255.255 are reserved for private-use networks\naddresses 100.64.0.0-100.127.255.255 are reserved for Shared Address Space\naddresses 127.0.0.0-127.255.255.255 are reserved for Loopback",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "a6e7baf8415e481183a565419c0bf35d",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 - 0.255.255.255\n\n10.0.0.0 - 10.255.2555.255\n\n100.64.0.0 - 100.127.255.255\n127.0.0.0 - 127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "dc71dc1629574dff8804ec158f93246d",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Class A:\n0.0.0.0. - 127.255.255.255",
        "answer_feedback": "Not all addresses in Class A are reserved",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "cfa2be07eac64025875dff4e65c0f752",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0/8 - dummy address\n10.0.0.0/8 - private network\n127.0.0.0/8 - loopback",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "dee1d8ee89d14b08a58dc9a79a9b20df",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0/8 - start address\n127.0.0.0/8\u00a0through\u00a0127.255.255.255/8\u00a0are reserved for\u00a0loopback addresses",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0ddb032ad52f40f28f13410e5f1501e4",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Class A:\n1.0.0.0 up to 127.255.255.255\nReserved addresses:\n(1-126).0.0.0\n(1-126).255.255.255\n127.0.0.0 to 127.255.255.255 (loopback adresses)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "e00c5179dd19478591a83272fb13b53f",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "According to slide 45 of the lecture \"Internet Protocol: IP Addressing \u2013 Interior and Exterior Gateway Protocols\", following adresses are reserved:\nreserved for current network:0.0.0.0 - 0.255.255.255\nreserved for loopback adresses to the local host:127.0.0.0 - 127.255.255.255\nMoreover, the first and last adress of every network can't be used, because they are for network and broadcast: (1-126).0.0.0 and (1-126).255.255.255.",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "c962460233de422f83be3d0b141517bf",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 and 127.255.255.255",
        "answer_feedback": "The addresses have a range: 127.0.0.0 - 127.255.255.255\nMissing Loopback",
        "verification_feedback": "Incorrect",
        "max_score": 0.5,
        "normalized_score": 0.0
    },
    {
        "id": "0ed05841b53346778442e17efaa51eb3",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "in the Host block two adresses are reserved:\n- all 1\u00b4s as host acts a a broadcast address\n- all 0\u00b4s as host acts as the network",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "379367506ec44f90904d8a61d5b31f53",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0 to 0.255.255.255 is for current network and 127.0.0.0 to 127.255.255.255 for loop-back addresses",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "e6537b3af62b469d85e797322b866d8f",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 to 10.255.255.255 private IP addresses\n\n127.0.0.0 to 127.255.255.255 reserved for loopback",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "3e105de6d26840329998610e34fbe0b0",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0\u20130.255.255.255\n127.0.0.0 - 127.255.255.255\n10.0.0.0\u201310.255.255.255 (Reserved for private Networks, not routed through the Internet)\n100.64.0.0\u2013100.127.255.25 (Reserved for private Networks, not routed through the Internet)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "363d29b2e2eb4e819965c55aa6edd715",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "According to RFC 6890 the following address blocks are reserved (https://datatracker.ietf.org/doc/html/rfc6890#section-2.2.2):\n127.0.0.0/8 with the range 127.0.0.0 - 127.255.255.255 loopback adresses\n100.64.0.0/10 with the range   100.64.0.0\u2013100.127.255.255 if carrier-grade NAT is used this is the shared address space between a service provider and its subscribers\n\n10.0.0.0/8\u00a0 with the range 10.0.0.0 - 10.255.255.255 private network adresses\n\n0.0.0.0/8 with the range 0.0.0.0 - 0.255.255.255 host adresses at this network with 0.0.0.0 being this host\n\nAdditionally these adresses are reserved:\nx.0.0.0 network adresses, where x is 0-127 (excluding 0.0.0.0 and 127.0.0.0 as reserved RFC Addresses)\nx.255.255.255 broadcast adresses, where x is 0-127 (excluding 0.255.255.255 and 127.255.255.255 as reserved RFC Addresses)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "4b96973e77894c519c0f58a3a97d2bf6",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "Reserved addresses:\n127.0.0.0 \u2013 127.255.255.255 (for Loop-Backs)",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "50602b62a9cb4dcdb3648ec7d4fcfdfb",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0\n127.0.0.0 to 127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "230c8812eab44963b03df9b3d4a59e34",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "0.0.0.0\u20130.255.255.255\n10.0.0.0\u201310.255.255.255\n100.64.0.0\u2013100.127.255.255\n127.0.0.0\u2013127.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "0bc93c32fa924ce8a726bbb94a3c91e8",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "10.0.0.0 to 10.255.255.255 are reserved",
        "answer_feedback": "Missing: Loopback",
        "verification_feedback": "Partially correct",
        "max_score": 0.5,
        "normalized_score": 0.5
    },
    {
        "id": "1bc63176fd804b428e663faa77424a98",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "reserved for loopback : 127.0.0.0 - 127.255.255.255\nClass A reserved range : 10.0.0.0 to 10.255.255.255",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    },
    {
        "id": "2343d07a6e6d4069b87b1c54564c9c7f",
        "question": "Write-down all addresses in Class A networks that are reserved.",
        "reference_answer": "126 Class A nets can be addressed in classful IP addressing (1.xx.yy.zz - 126.xx.yy.zz) 127.xx.yy.zz is reserved for loopback testing 0.xx.yy.zz can be accepted if stated accordingly (definitions defer whether this is in Class A)",
        "provided_answer": "the host itself:\n0.0.0.0 (00000000.00000000.00000000.00000000)\n\na host at the same network:\n0.x.x.x (00000000.yyyyyyyy.yyyyyyyy.yyyyyyyy)\n\nbroaodcast on another network:\nx.255.255.255 (yyyyyyyy.11111111.11111111.11111111) for x in range [0,127]\n\nloopback:\n127.x.x.x",
        "answer_feedback": "Correct",
        "verification_feedback": "Correct",
        "max_score": 0.5,
        "normalized_score": 1.0
    }
]